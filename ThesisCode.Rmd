---
title: "Lewis' Thesis Code"
author: "Lewis Hakam"
output: html_document
---

```{r setup, include=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#personal access token if needed: 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
#Data retrieval and manipulation
library(move) #download, manipulate, anlayze data from Movebank
library(sp) #spatial objects in Movebank
library(moveVis) #converting a data frame to a move object
#organizating, filtering, cleaning data
library(plyr) #batch calculating stats for subsets of data
library(dplyr) #deleted and re installed rlang
library(tidyr) #extracting character string values
library(readr) #read in CSV files
library(stats) #basic stats functions and tools
library(devtools) #deleted and re installed htmltools
library(lubridate) #ensuring date formats are correct
library(solaR)
library(solartime)
library(lutz)
find_mode <- function(x) {
  u <- unique(x)
  tab <- tabulate(match(x, u))
  u[tab == max(tab)]
} #no built in function for mode in R so I got this one from the web. 
library(data.table) #manipulating data tables
#Movement models
library(adehabitatLT) #creating trajectories and calculating characteristics. 
#mapping
library(leaflet) #interactive maps
library(htmltools) #popups for interactive maps
library(ggplot2) #graphs and maps
library(sf) #spatial objects for leaflet and ggplot
library(tmap) #package containing world country polygons
wgs84<-CRS("+proj=longlat +datum=WGS84") #GCS in WGS 1984
azim_Bierr = CRS("+proj=aeqd +lat_0=19 +lon_0=-72 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m +no_defs") #PCS in custom azimuthal equidistant projection. estimated middle point of the data visually with Google Maps
data(World) #world data with country polygons needed for mapping
world_sf<-st_transform(World, azim_Bierr) #projected world data
world_filter<-world_sf[,"name"] #reduce info for spatial join
world_filter$name<-as.character(world_filter$name) #ensure variables are in appropriate data type
plotshit<-function(y) {
  ggplot(y) +
    geom_sf(data = world_sf, fill = "#464646") +
    geom_sf(size = 1, aes(color = year)) +
    coord_sf(datum = st_crs(azim_Bierr), xlim = c(-4000000, 3000000), ylim = c(3500000, -2500000)) +
    theme(legend.position = "bottom", legend.text = element_text(size = 7)) +
    labs(color = "year") +
    ggtitle(paste0(y$trackId))
} #function for batch plotting in a for loop
plotshit2<-function(y) {
  ggplot(y) +
  geom_sf(data = world_sf, fill = "#464646") +
  geom_sf(size = 1, aes(color = DaylightUTC)) +
  coord_sf(datum = st_crs(azim_Bierr), xlim = c(-4000000, 3000000), ylim = c(3500000, -2500000))+
  ggtitle(paste0(y$trackId))
} #daylight assessment
#data analysis
library(car) # Q-Q plots for EDA
library(FactoMineR) #PCA biplots
library(hopkins) #hopkins statistic for clustering potential
library(cluster) #linkage methods 
library(stats) #PCA, HCA and dendrogram
library(NbClust)#optimal clusterss
library(dendextend) #modify dendrogram
library(ComplexHeatmap) #heatmaps
library(RColorBrewer) #color palette customization 
library(vegan) #PERMANOVA
```

## Introduction

> Hey there! If you are viewing this, you are interested in how I analyzed osprey tracking data for my final thesis, completed in Spring 2024. I will guide through my step by step process to answer three questions:

> **Do individual ospreys use the same migration strategy between years?**
> **Do individual ospreys use the same migration strategy between regions?**
> **How does varying the temporal sampling frequency affect my results?**

> I have used R Markdown, which may be new for you. This document will not go over the nuances I used to code the Markdown file. For simplicity and privacy, not all code lines are shown in the Rmarkdown output when downloading data. I have done my best to annotate my code for easy interpretation, but this is not a guide on how to use R. Some aspects of coding will not be explained.


## Packages

> Here is a list of packages and functions in the order of their first appearance that I used followed by custom functions and R objects. You can also find a list of packages and their uses in Appendix **XX**. You will likely have to use the function install.packages ("Package") before you can access packages using the library (Package) function if you are new to R. I had a few instances where I had supporting packages downloaded, but they were not up-to-date nor could I force them to update. For these packages I had to locally delete the packages that were not updating and then re-install them. Many of these packages have dependencies (they require other packages) so do not be alarmed if download takes a while. Lastly, some packages are not from CRAN, rather they are from personal repositories (repos) on github or from Bioconductor. These packages require different download code. To download packages from repos, you need to use the install_github("username/repo/package file name",) from the 'devtools' package. For Bioconductor packages you need to use the install("package name") function from the 'BioManager' package.

* CRAN: install.packages() from base R *
* BioConductor: install() from BioManager **
* Personal Repositories: install_github() from devtools ***

```{r, eval=FALSE}
library(move) # download and retrieve movement data and characteristics *
library(sp) # Create sp spatial objects *
library(moveVis) # transform dataframes into move objects ***
library(plyr) # apply a function to a list *
library(dplyr) # manipulate dataframes and filter data *
library(stats) # some statistic tools plus HCA function *
library(lutz) # Determine if there was daylight during a GPS fix *
library(solaR) # Calculate local solar time *
library(lubridate) # Manipulate time/date formats *
library(data.table) # Work with data tables *
library(migrateR) # Calculate start/end dates of migration **
library(adehabitatLT) # Find fpt and displacement *
library(ggplot2) # graphing/mapping *
library(sf) # sf spatial objects in R for mapping *
library(tmap) # world country shapefiles *
library(PerformanceAnalytics) # Correlations and EDA *
library(FactoMineR) # PCA biplots *
library(hopkins) # Hopkins statistic for clustering potential *
library(cluster) # Linkage methods *
library(NbClust)# Optimal clusterss *
library(dendextend) # Modify dendrogram *
library(ComplexHeatmap) # Heatmaps **
library(RColorBrewer) # Color palette customization *
library(vegan) # PERMANOVA *


wgs84<-CRS("+proj=longlat +datum=WGS84") # GCS
azim_orign = CRS("+proj=aeqd +lat_0=24.43 +lon_0=-82.74 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m +no_defs") # Azimuthal Equidistant PCS
data(World) # Country Shapefiles
world_sf<-st_transform(World, azim_orign) #Country Shapefiles projected with custom PCS
find_mode <- function(x) {
  u <- unique(x)
  tab <- tabulate(match(x, u))
  u[tab == max(tab)]
} # Mode function
plotshit<-function(y) {
  ggplot(y) +
    geom_sf(data = world_sf, fill = "white") +
    geom_sf(size = 1, aes(color = year)) +
    coord_sf(datum = st_crs(azim_orign), xlim = c(-4000000, 3000000), ylim = c(3500000, -2500000)) +
    theme(legend.position = "bottom", legend.text = element_text(size = 7)) +
    labs(color = "year") +
    ggtitle(paste0(y$trackId))
} # Batch Map raw data by individual bird and year function
```



## Downloading and Filtering Tracking Data

#### Setup

> First, If you do not have a Movebank account, you must make a free account before you can access data. Even after you make an account, not all data are available for download. There are three levels of privacy on Movebank: downloadable (orange), visible fixes with permission required to download (blue), and no visible fixes with permission required to download (grey). More information about navigating Movebank can be found on the Movebank website: <https://www.movebank.org/cms/movebank-main>

> After reviewing accessible data sets, I determined that I could only use is "Osprey Bierregaard North and South America". I chose this data set since it provided me with the most individuals with 1-hour or finer sampling frequencies and allowed me to control for possible regional differences among worldwide osprey populations. The dataset consists of ospreys from Newfoundland to South Carolina tracked from 1995-2019. Not all of these individuals are tracked using GPS or GPS/GSM devices with a 1-hour or finer sampling frequency. 

> To start, you will consult the reference data and see if you can reduce the dataset before downloading all fixes. This dataset is large, and the more data you pull, the longer it will take. Reducing the number of points before downloading will significantly increase processing speeds. To pull reference data or datasets from Movebank you will use the 'move' package. Before you can pull data with the move package you will need to create an object called loginStored which contains your login info. The code below is an example, since I do not want to give out my actual password. 

>Your code will look something like this:

```{r, eval=FALSE}
loginStored <- movebankLogin(username = "UserName", password = "Password")
```

```{r, echo=FALSE}
loginStored <- movebankLogin(username = "lhaka2", password = "!ShamaJ2016!")
```

#### Downloading Movebank Summary Table

> Next you will download the Movebank study synopsis and summary tables. It is possible to search for studies within R, but you will save yourself some time using the Movebank website's interactive map. 
* The synopsis can provide details on how many individuals were tracked, the time frame tracking occurred, the type of device(s) used, demographic information (sex, age, etc.), and permission/privacy restrictions. 
* The summary table will provide a summary of each individual tracked per device. This means that some individuals may have multiple entries in a summary table if they were fitted with more than one transmitter. You can use the summary table info to partially fill out demographic information and determine how many individuals were tracked using GPS or GPS/GSM devices. I kept a running document of individuals with their demographic and geographic information that will come in handy later on. 

```{r, warning=FALSE}
#download Bierregaard data Synopsis
reference<-getMovebankStudy(study ="Osprey Bierregaard North and South America", login = loginStored)
reference$study_objective
#download the summary table
Bierregaard_Study_summary<-getMovebankReferenceTable(study ="Osprey Bierregaard North and South America", login = loginStored, allAttributes = FALSE)
#Notice that each individual has up to 3 entries. Some individuals only have one entry with sensor_type_id equalling 82789. These individuals were tracked with devices that only collected doppler shift locations. These individuals were mostly tracked before 2007. Some of these individuals may have two entries, but you can always look at tag comments which provide info on whether locations  were estimated via doppler shift or GPS. 
#Explore sensor attributes and associated codes. We only want a summary of individuals with GPS or GPS/GSM tracks. We also do not need accessory or engineering data. 
attributes<-getMovebankSensorsAttributes(study="Osprey Bierregaard North and South America",login=loginStored) #sensor_type_id for GPS locations that we will need is 653. 7842954 is the GPS engineering data and 82798	is doppler shift data. Filter the summary table to keep only entries with the sensor_type_id of 653.
Bierregaard_Osprey_summary<-filter(Bierregaard_Study_summary,
                                   Bierregaard_Study_summary$sensor_type_id == 653)
#Fill in attributes of individuals on a spreadsheet. 
```

> The summary tables give me an idea of the total number of individuals in each study and the sex and age of each individual. However, it does not tell me the number of fall migrations for each individual, whether those migrations were complete or not, nor if the individual even started migration. Age and sex data may also be missing. Sometimes sex is unknown, especially for juveniles, which are not sexually dimorphic. 

#### Download Movebank Dataset

> The data sets are really large, so I filtered the data to make it easier to work with and plot. 

> Remove:
* individuals with sampling frequencies courser than 1-hour (using mode and median)
* GPS fixes outside of July-December (7-12) when osprey are "stationary" or on spring migrations
* erroneous points that don't meet a defined biological/practical threshold.

```{r}
#Download data from Movebank for GPS tracked individuals.This takes some time. When downloading Movebank data, I can specify which individuals I want in my dataset using animalName = (list of animal local identifiers)
Bierregaard_Osprey<-getMovebankData(study ="Osprey Bierregaard North and South America",
                                    animalName = c(Bierregaard_Osprey_summary$animal_local_identifier),
                                    login = loginStored, removeDuplicatedTimestamps=TRUE) 
citations(Bierregaard_Osprey) #no citation recorded
#Get the time between consecutive locations using the move package
#Start by calculating the time lag betwen points for each individual. The output is a list. 
list_Bierr<-timeLag(Bierregaard_Osprey, units = "hours")
#lapply function applies a function to a list. Each individual within the data set has a median or mode timelag of 1 hour or finer. I used median/mode instead of average since its most studies only have trackers turned on during daylight hours, which means some lag values will be 10 hours or larger. This results in a mean sampling frequency greater than one. Osprey are diurnal and typically sleep at night with a few exceptions. It is often more advantageous to turn off trackers at night to conserve battery power than to continue recording locations for birds after daylight. 
sapply(list_Bierr, mean) #You can see thant most study averages are larger than 1 point per hour .
sapply(list_Bierr, find_mode) # birds with a finer than 1-hour sampling frequency were tracked using GPS/GSM transmitters. 
sapply(list_Bierr, median) #results are similar to mode. 

#make into a data frame so I can easily filter, plot, and manipulate the data.
Bierregaard_Osprey_DF<-as.data.frame(Bierregaard_Osprey)
#Filter out non GPS location entries
Bierregaard_Osprey_DF_GPS<-Bierregaard_Osprey_DF %>%
  filter(sensor_type_id == 653) #reduces dataset by around 50,000 points. 
#Extract month data for filtering
Bierregaard_Osprey_DF_GPS$month<-month(ymd_hms(Bierregaard_Osprey_DF_GPS$timestamp))
#Extract year data for plotting later
Bierregaard_Osprey_DF_GPS$year<-year(ymd_hms(Bierregaard_Osprey_DF_GPS$timestamp))
length(unique(Bierregaard_Osprey_DF_GPS$trackId)) #86 individuals
#remove outliers
#filter quantitative erroneous points: Erroneous points are those with unrealistically high speed and turning angle (course). For course (often referred to and confused with heading) we will look at the 90th percentile of points as suggested by Gupte et al., 2021. Course is measured as the angle from magnetic north (clockwise) in which the tracker is moving. For speed I used Kerlinger 1989 to determine that the maximum recorded flight speed of an osprey was 33.4 meters/second.
quant<-quantile(Bierregaard_Osprey_DF_GPS$heading, probs = 0.9)
Bierregaard_Osprey_DF_Filter<-filter(Bierregaard_Osprey_DF_GPS, ground_speed <= 33.4 & heading < quant) #about 25,000 fixes were removed. 
#Filter data further by removing timestamps from January through June (keep months 7-12)
Bierregaard_Osprey_Data <- Bierregaard_Osprey_DF_Filter %>%
  filter(month %in% c(7:12))
#Create migration event IDs. Later I will create segment IDs. 
Bierregaard_Osprey_Data$migrationEvent<-paste(Bierregaard_Osprey_Data$trackId, Bierregaard_Osprey_Data$year)
length(unique(Bierregaard_Osprey_Data$migrationEvent)) #86 individuals with 145 migration events between them.
#Check that the IDs: unique(Bierregaard_Osprey_Data$migrationEvent)
#Format the migration event names so that they don't have any spaces or special characters
Bierregaard_Osprey_Data$migrationEvent<-gsub("\\.", "_", Bierregaard_Osprey_Data$migrationEvent) #remove periods
Bierregaard_Osprey_Data$migrationEvent<-gsub(" ", "_", Bierregaard_Osprey_Data$migrationEvent) #remove spaces
```

#### Mapping and Filtering Data

> First, I will quickly map the data for each individual by year to determine if I should remove any individuals that obviously did not complete at least two migration segments between all tracking years. Ospreys typically have three migration segments defined as follows:
* travel from and across the US mainland. Exceptions are for those ospreys that started by travelling over the Atlantic. The ending point of the first segment is the point closest to the border between the first and second segment.
* Across the Caribbean starting from the ending point of the first second to the point closest tothe border between the second and third segemnt. 
* Across central/South America until the final migration point. 
There are a few exceptions to the three segments as some osprey wintered in Florida or Cuba. 

> I will use 'ggplot' and 'leaflet', which both rely on 'sf' spatial objects for mapping purposes. I will map each individual bird, with tracks color-coded by year so I know if the bird has more than one fall migration that I can use. This requires the custom 'plotshit' function and a for loop. Data on the background world countries is from the 'tmap'. Data is projected using a custom aziumthal equidistant projection. The actual projection used to map data is not all that important at the moment since we are not measuring anything, however the projection will be important later on. The projection was defined by using google maps to estimate the center of all of my data. 

```{r, fig.show="hold", out.width="10%"}
#Spatially project the Bierregaard data
Bierregaard_Osprey_Data_sf<-st_as_sf(x=Bierregaard_Osprey_Data, 
                                     coords = c("location_long.1", "location_lat.1"), crs = wgs84)
Bierregaard_Osprey_Data_sf<-st_transform(Bierregaard_Osprey_Data_sf, azim_Bierr)
#to color code year, ggplot needs the year column to be a factor data type. 
Bierregaard_Osprey_Data_sf$year<-as.factor(Bierregaard_Osprey_Data_sf$year) 
#batch map the GPS points for each individual using ggplot
names<-unique(Bierregaard_Osprey_Data_sf$trackId)
for (i in names) {
  y = filter(Bierregaard_Osprey_Data_sf, trackId == i)
  pws<-plotshit(y)
  print(pws)
}
```

> From the plots, I can get an idea of how many fall migration events occurred for each bird. I can also tell if some individuals were juveniles during their first migration as juveniles spend 18 months wintering and return to breeding areas at three-years old. Juvenile wintering periods during the expected fall migration period were removed. Finally, I removed events where birds did not complete at least one segment of their migration or where the tracks were suspiciously thin for birds tracked in 2007 or earlier. Some birds never left their breeding/natal grounds, which means they cannot be used in my analysis.   

> Next, I will further filter events and individuals by taking a close look at each event using an interactive map. I will also determine the start/end dates of migration and the start/end latitude and longitude of each segment. 

```{r}
#project data using a GCS
Bierregaard_Osprey_Data_sf<-st_as_sf(x=Bierregaard_Osprey_Data, coords = c("location_long", "location_lat"), crs = wgs84)

#Create and interactive map where you can click on points for each migration event and visually determien start/end of migration. I did this for each event separately so I left an example of a plot here. 
leaflet(filter(Bierregaard_Osprey_Data_sf, migrationEvent == "Art_2012")) %>% 
  addTiles() %>%
  addCircleMarkers(radius = 2, color = "red", opacity = 100) %>%
  addPopups(popup = ~htmlEscape(timestamp))
```

```{r}
#create a list of migration events to remove. 
remove<-c("Woody_2015", "Weber_2014", "Virginia_2016", "Uncas_2014", "Trepassey_2016", "Tommy_2017", "Tilton_2014", "Thatcher_2011", "Snowy_2012", "Sanford_2011", "Saco_2011", "Ron_2015", "Roger_Tory_2014", "Rodney_2014", "Rock_2017", "Rammie_2013", "Rafael_2009", "Penelope_2009", "Peirce_2013", "Pearl_2013", "North_Fork_Bob_2015", "Neale_2010", "Mackenzie_2013", "Luke_2007", "Lizzie_2015", "Little_Ricky_2008", "Layla_2017", "Katy_2009", "Katbird_2011", "Jocelyn_2016", "Hix_Jr_2009", "Gunny_2010", "Gerry_2014", "Flow_2015", "Flow_2017", "Felix_2007", "Daphne_2017", "Claws_2007", "Claws_2008", "Chip_2012", "Chester_2014", "Captain_Liz_2013", "Caleb_2013", "Buck_2010", "Bridget_2014", "Bridger_2012", "Borealis_2017", "Borealis_2018", "Blackie_2014", "Belle_2011", "Aster_2017", "Artoo_2014", "Art_2013")

Bierregaard_Osprey_Data$migrationEvent<-as.character(Bierregaard_Osprey_Data$migrationEvent)
Bierregaard_Osprey_Data<-Bierregaard_Osprey_Data[!(Bierregaard_Osprey_Data$migrationEvent %in% remove), ]
length(unique(Bierregaard_Osprey_Data$migrationEvent))
length(unique(Bierregaard_Osprey_Data$trackId)) #92 migration events for 51 individuals
```

```{r, eval=FALSE}
#While using leaflet works, this method will take a long grueling time to complete. If you want to expedite this process you can export the data points to CSV files and find start/end dates in ArcGIS or QGIS. The code below will allow you to do this. 
Bierr_Event<-unique(Bierregaard_Osprey_Data$migrationEvent)
Bierregaard_Osprey_Data$migrationEvent<-gsub(" ", "_", Bierregaard_Osprey_Data$migrationEvent)
for (i in Bierr_Event) {
  ID2 <- subset(Bierregaard_Osprey_Data, migrationEvent == i)
  write.csv(ID2, file = paste0("Z://Personal_Folders/Lewis/Thesis/ROutput/",i, ".csv"))
} #make sure you change the file path so data is saved to your own folder.  
```

> Next you will remove all fixes for each migration event that fall outside of the start and end date of migration. You will have to load in the metadata file with the migration event start/end dates.

```{r, warning=FALSE}
IndividualMetadata <- read_csv("IndividualMetadata.csv")
#make sure this is a dataframe
IndividualMetadata<-as.data.frame(IndividualMetadata)
#Format start/end timestamps so they are date/time format. I keep having to go back to the CSV file and adjust these values to yyyy-mm-dd hh:mm:ss
IndividualMetadata$Seg_End<-ymd_hms(IndividualMetadata$Seg_End)
IndividualMetadata$Seg_Start<-ymd_hms(IndividualMetadata$Seg_Start)
IndividualMetadata$Mig_Start<-ymd_hms(IndividualMetadata$Mig_Start)
IndividualMetadata$Mig_End<-ymd_hms(IndividualMetadata$Mig_End)
#I suggest checking the column class with the class() to ensure this worked properly.
#Now filter out non-migratory fixes for each migration event.
events<-unique(Bierregaard_Osprey_Data$migrationEvent)
Filtered<-data.frame()
for (i in events) {
  y = filter(Bierregaard_Osprey_Data, migrationEvent == i)
  x = filter(IndividualMetadata, migrationEvent == i)
  n = filter(y, timestamp <= x$Mig_End & timestamp >= x$Mig_Start)
  Filtered = rbind(Filtered, n)
}
#map to check that filtering worked. 
Filtered_sf<-st_as_sf(x=Filtered, coords = c("location_long.1", "location_lat.1"), 
                      crs = wgs84)
Filtered_sf<-st_transform(Filtered_sf, azim_Bierr)
Filtered_sf$year<-as.factor(Filtered_sf$year)
Filter_names<-unique(Filtered_sf$trackId)
for (i in Filter_names) {
  y = filter(Filtered_sf, trackId == i)
  pws<-plotshit(y)
  print(pws)
}
```



## Create Data Subsets
#### Regularize the Trajectories

Before we break down data into subsets, I need to regularized the data. Data was not collected at perfect 1-hour intervals for a 24 hour period. This means that when I calculate trajectory characteristics, points from the end of one day may connect to points at the start of the next day. If the osprey was sleeping during this time period, speed values would be very low and add inaccuracy to my dataset. When osprey roost they do not move. Regularizing will help me pinpoint times when osprey roosted so those points can be taken out later for 1-hour and 3-hour sampling frequencies. There are of course exceptions when osprey cross over open water. In these cases, speed should be high even after dark. 
```{r}
#Convert to a move object. To do this I had to use the moveVis package since the move function does not work well with actual dataframes. The move package has a number of very convenient functions that allow me to calculate a number of different movement characteristics and prep the data from analysis.
OSPR<-df2move(Filtered,
        proj = wgs84, 
        x = "location_long", y = "location_lat", 
        time = "timestamp", track_id = "migrationEvent")
#check to see that there are still 92 migration events. Each event in the movestack is named with a unique trackID. 
length(unique(Filtered$migrationEvent))
length(unique(OSPR@trackId))
#regularize the time series through the move package. To do this we will have to apply the regularization to each move object within the Movestack (annoying, I know). The interpolate time function fills in positional points along a standard time series. We will use 1-hour to do this. 
Stacked_1hour<-list() #empty list for output

for (i in 1:92) {
  x<-interpolateTime(OSPR[[i]], time=as.difftime(1, units="hours"), spaceMethod='rhumbline')
  plot(x, col="red",pch=20, main="By time interval")
  points(OSPR[[i]], col = "black", pch = 20)
  print(legend("bottomleft", c("True locations", "Interpolated locations"), 
               col=c("black", "red"), pch=c(19,20)))
  Stacked_1hour[[i]] = x
}
#convert output list of move objects into a move stack.
OSPR_1hour<-moveStack(Stacked_1hour) 
length(unique(OSPR_1hour@trackId)) #check that all migration events are accounted for

list_1hour<-timeLag(OSPR_1hour, units = "hours")
sapply(list_1hour, median) 
```

#### Create Subsets 1 & 2
> I will first determine which migration events/individuals should belong to each subset.
* Subset 1: individuals with at least two migration events that have at least one complete segment each
* Subset 2: individuals with at least two complete segments (keep all events). 

```{r}
OSPR_1hour_DF<-as.data.frame(OSPR_1hour)
subset1_keep<-c("Artoo_2013", "Artoo_2015", "Belle_2010", "Belle_2012", "Belle_2013", "Belle_2014", "Belle_2015", "Belle_2016", "Buck_2009", "Buck_2011", "Charlie_2014", "Charlie_2015", "DJ_2013", "DJ_2014", "Donovan_2013", "Donovan_2014", "Donovan_2015", "Edwin_2013", "Ediwin_2014", "Edwin_2015", "Flow_2014", "Flow_2016", "Holly_2016", "Holly_2017", "Holly_2018", "Holly_2019", "Mr__Hannah_2009", "Mr__Hannah_2010", "Nick_2013", "Nick_2014", "Nick_2015", "Nick_2016", "North_Fork_Bob_2010", "North_Fork_Bob_2011", "North_Fork_Bob_2012", "North_Fork_Bob_2013", "North_Fork_Bob_2014", "Penelope_2008", "Penelope_2010", "Quin_2013", "Quin_2014", "Quin_2015", "Ron_2013", "Ron_2014", "Shanawdithit_2016", "Shanawdithit_2017", "Snowy_2011", "Snowy_2013", "Snowy_2014", "Snowy_2015", "Sr__Bones_2010", "Sr__Bones_2011", "Sr__Bones_2012", "Sr__Bones_2013", "Staddler_2016", "Staddler_2015", "Staddler_2017", "Thatcher_2010", "Thatcher_2012", "Wausau_2015", "Wausau_2016", "Woody_2013", "Woody_2014")
OSPR_1hour_DF$trackId<-as.character(OSPR_1hour_DF$trackId)
Subset1_1hour<-OSPR_1hour_DF[(OSPR_1hour_DF$trackId %in% subset1_keep), ]
unique(Subset1_1hour$trackId) #62 tracks from 22 individuals

#subset 2 is easy
Subset2_1hour<-OSPR_1hour_DF
```

#### Segment Subset 2

> I will segment subset 2 now so I don't have segment every thinned dataset. 

```{r, warning=FALSE}
#Separate data into segments based on the start/end date of each segment and combine into a singular data frame
segment_events<-unique(Subset2_1hour$trackId)
segs<-data.frame()
for (i in segment_events) {
  y = filter(Subset2_1hour, trackId == i)
  x = filter(IndividualMetadata, migrationEvent == i)
  for (j in 1:nrow(x)) {
    z = x[j,] #you have to specify the row the row
    n = filter(y, timestamps <= z$Seg_End & timestamps >= z$Seg_Start)
    b = merge(n, z, by.x = "trackId", by.y ="migrationEvent")
    segs = rbind(segs, b)
  }
}

#check to make the number of segments matches those in the metadata.
length(unique(IndividualMetadata$SegmentID))
length(unique(segs$SegmentID))

#filter out segments with a duration less than 3 days. When thinning segments, I will lose those segments with such a short timespan. Duration was calculated in the metadata spreadsheet
segments<-filter(segs, Duration >= 3)
length(unique(segments$SegmentID)) #25 segments removed
#look to see if any individuals no longer fit the criteria of having at least two complete segments.
table(segments$Segment, segments$Indvidual)
#remove individuals
segment_remove<-c("Henrietta", "Isabel", "Moffet", "Ozzie", "Quin")
segments_final<-segments[!(segments$Indvidual %in% segment_remove), ]
length(unique(segments_final$Indvidual))
length(unique(segments_final$SegmentID)) #46 individuals with 198 migration segments between them.
```

#### Thin Trajectories

> Next I will thin my trajectories to 3-hour, 1-day, and 3-day sampling frequencies for both subsets.

```{r}
#First create a move object
Subset1_1hour_move<-df2move(Subset1_1hour,
        proj = wgs84, 
        x = "coords.x1", y = "coords.x2", 
        time = "timestamps", track_id = "trackId")
#3-hour sampling frequency
Subset1_3hr<-list()
for (i in 1:62) {
  x<-thinTrackTime(Subset1_1hour_move[[i]], interval = as.difftime(3, units="hours"),
                          tolerance = as.difftime(15, units="mins"), criteria = "closest")
  split<-move::split(x)
  Subset1_3hr[[i]] = split$selected
}
Subset1_3hr_move<-moveStack(Subset1_3hr) 
str(Subset1_3hr_move@trackId)
#1-day sampling frequency
Subset1_1day<-list()
for (i in 1:62) {
  x<-thinTrackTime(Subset1_1hour_move[[i]], interval = as.difftime(1, units="days"),
                          tolerance = as.difftime(2, units="hours"), criteria = "closest")
  split<-move::split(x)
  Subset1_1day[[i]] = split$selected
}
Subset1_1day_move<-moveStack(Subset1_1day) 
str(Subset1_1day_move@trackId)

#3-day sampling frequency
Subset1_3day<-list()
for (i in 1:62) {
  x<-thinTrackTime(Subset1_1hour_move[[i]], interval = as.difftime(3, units="days"),
                          tolerance = as.difftime(2, units="hours"), criteria = "closest")
  split<-move::split(x)
  Subset1_3day[[i]] = split$selected
}
Subset1_3day_move<-moveStack(Subset1_3day) 
str(Subset1_3day_move@trackId)

#Check to see if thinning looks right. 
plot(Subset1_1hour_move[[1]])
plot(Subset1_3hr_move[[1]])
plot(Subset1_1day_move[[1]])
plot(Subset1_3day_move[[1]])
```

```{r}
#First create a move object
Subset2_1hour_move<-df2move(segments_final,
        proj = wgs84, 
        x = "coords.x1", y = "coords.x2", 
        time = "timestamps", track_id = "SegmentID")
unique(Subset2_1hour_move@trackId)
#3-hour sampling frequency
Subset2_3hr<-list()
for (i in 1:198) {
  x<-thinTrackTime(Subset2_1hour_move[[i]], interval = as.difftime(3, units="hours"),
                          tolerance = as.difftime(15, units="mins"), criteria = "closest")
  split<-move::split(x)
  Subset2_3hr[[i]] = split$selected
}
Subset2_3hr_move<-moveStack(Subset2_3hr) 
str(Subset2_3hr_move@trackId)
#1-day sampling frequency
Subset2_1day<-list()
for (i in 1:198) {
  x<-thinTrackTime(Subset2_1hour_move[[i]], interval = as.difftime(1, units="days"),
                          tolerance = as.difftime(2, units="hours"), criteria = "closest")
  split<-move::split(x)
  Subset2_1day[[i]] = split$selected
}
Subset2_1day_move<-moveStack(Subset2_1day) 
str(Subset2_1day_move@trackId)

#3-day sampling frequency
Subset2_3day<-list()
for (i in 1:198) {
  x<-thinTrackTime(Subset2_1hour_move[[i]], interval = as.difftime(3, units="days"),
                          tolerance = as.difftime(2, units="hours"), criteria = "closest")
  split<-move::split(x)
  Subset2_3day[[i]] = split$selected
}
Subset2_3day_move<-moveStack(Subset2_3day) 
str(Subset2_3day_move@trackId)

#Check to see if thinning looks right. 
plot(Subset2_1hour_move[[1]])
plot(Subset2_3hr_move[[1]])
plot(Subset2_1day_move[[1]])
plot(Subset2_3day_move[[1]])
```

## Calculate Trajectory Characteristics and Summarize Data
Now we have all four data sets. The final step before analysis is to calculate trajectory characteristics.

#### Calculate Characteristics with 'move'

> When calculating distance measurements, Keep in mind that data must be projected using a projection that preserves distance (best projection to use is an azimuthal equidstant projection), while angle measurements need to be calculated using a mercator projection. 

```{r}
#Project data with custom azimuthal equidistant projection
Subset1_1hour_move_azim <- spTransform(Subset1_1hour_move, CRSobj=azim_Bierr)
Subset1_3hr_move_azim <- spTransform(Subset1_3hr_move, CRSobj=azim_Bierr)
Subset1_1day_move_azim <- spTransform(Subset1_1day_move, CRSobj=azim_Bierr)
Subset1_3day_move_azim <- spTransform(Subset1_3day_move, CRSobj=azim_Bierr)

Subset2_1hour_move_azim <- spTransform(Subset2_1hour_move, CRSobj=azim_Bierr)
Subset2_3hr_move_azim <- spTransform(Subset2_3hr_move, CRSobj=azim_Bierr)
Subset2_1day_move_azim <- spTransform(Subset2_1day_move, CRSobj=azim_Bierr)
Subset2_3day_move_azim <- spTransform(Subset2_3day_move, CRSobj=azim_Bierr)

#calculate distance between consecutive GPS points (meters)
Subset1_1hour_move_azim$distance<-unlist(lapply(distance(Subset1_1hour_move_azim), c, NA))
Subset1_3hr_move_azim$distance<-unlist(lapply(distance(Subset1_3hr_move_azim), c, NA))
Subset1_1day_move_azim$distance<-unlist(lapply(distance(Subset1_1day_move_azim), c, NA))
Subset1_3day_move_azim$distance<-unlist(lapply(distance(Subset1_3day_move_azim), c, NA))

Subset2_1hour_move_azim$distance<-unlist(lapply(distance(Subset2_1hour_move_azim), c, NA))
Subset2_3hr_move_azim$distance<-unlist(lapply(distance(Subset2_3hr_move_azim), c, NA))
Subset2_1day_move_azim$distance<-unlist(lapply(distance(Subset2_1day_move_azim), c, NA))
Subset2_3day_move_azim$distance<-unlist(lapply(distance(Subset2_3day_move_azim), c, NA))

#calculate speed/velocity between consecutive GPS points (m/s)
Subset1_1hour_move_azim$speed<-unlist(lapply(speed(Subset1_1hour_move_azim), c, NA))
Subset1_3hr_move_azim$speed<-unlist(lapply(speed(Subset1_3hr_move_azim), c, NA))
Subset1_1day_move_azim$speed<-unlist(lapply(speed(Subset1_1day_move_azim), c, NA))
Subset1_3day_move_azim$speed<-unlist(lapply(speed(Subset1_3day_move_azim), c, NA))

Subset2_1hour_move_azim$speed<-unlist(lapply(speed(Subset2_1hour_move_azim), c, NA))
Subset2_3hr_move_azim$speed<-unlist(lapply(speed(Subset2_3hr_move_azim), c, NA))
Subset2_1day_move_azim$speed<-unlist(lapply(speed(Subset2_1day_move_azim), c, NA))
Subset2_3day_move_azim$speed<-unlist(lapply(speed(Subset2_3day_move_azim), c, NA))

#calculating lag between fixes.
Subset1_1hour_move_azim$lag<-unlist(lapply(timeLag(Subset1_1hour_move_azim,units = "hours"), c, NA))
Subset1_3hr_move_azim$lag<-unlist(lapply(timeLag(Subset1_3hr_move_azim,units = "hours"), c, NA))
Subset1_1day_move_azim$lag<-unlist(lapply(timeLag(Subset1_1day_move_azim,units = "hours"), c, NA))
Subset1_3day_move_azim$lag<-unlist(lapply(timeLag(Subset1_3day_move_azim,units = "hours"), c, NA))

Subset2_1hour_move_azim$lag<-unlist(lapply(timeLag(Subset2_1hour_move_azim,units = "hours"), c, NA))
Subset2_3hr_move_azim$lag<-unlist(lapply(timeLag(Subset2_3hr_move_azim,units = "hours"), c, NA))
Subset2_1day_move_azim$lag<-unlist(lapply(timeLag(Subset2_1day_move_azim,units = "hours"), c, NA))
Subset2_3day_move_azim$lag<-unlist(lapply(timeLag(Subset2_3day_move_azim,units = "hours"), c, NA))

#Project data with conformal projection: the Mercator projection is a conformal cylindrical projection
mercator = CRS("+init=epsg:4326") #this projection covers the world so no need to custom define projection attributes
Subset1_1hour_move_mercator <- spTransform(Subset1_1hour_move, CRSobj=mercator)
Subset1_3hr_move_mercator <- spTransform(Subset1_3hr_move, CRSobj=mercator)
Subset1_1day_move_mercator <- spTransform(Subset1_1day_move, CRSobj=mercator)
Subset1_3day_move_mercator <- spTransform(Subset1_3day_move, CRSobj=mercator)

Subset2_1hour_move_mercator <- spTransform(Subset2_1hour_move, CRSobj=mercator)
Subset2_3hr_move_mercator <- spTransform(Subset2_3hr_move, CRSobj=mercator)
Subset2_1day_move_mercator <- spTransform(Subset2_1day_move, CRSobj=mercator)
Subset2_3day_move_mercator <- spTransform(Subset2_3day_move, CRSobj=mercator)

#calculating absolute angle
Subset1_1hour_move_mercator$abs_angle<-unlist(lapply(angle(Subset1_1hour_move_mercator), c, NA))
Subset1_3hr_move_mercator$abs_angle<-unlist(lapply(angle(Subset1_3hr_move_mercator), c, NA))
Subset1_1day_move_mercator$abs_angle<-unlist(lapply(angle(Subset1_1day_move_mercator), c, NA))
Subset1_3day_move_mercator$abs_angle<-unlist(lapply(angle(Subset1_3day_move_mercator), c, NA))

Subset2_1hour_move_mercator$abs_angle<-unlist(lapply(angle(Subset2_1hour_move_mercator), c, NA))
Subset2_3hr_move_mercator$abs_angle<-unlist(lapply(angle(Subset2_3hr_move_mercator), c, NA))
Subset2_1day_move_mercator$abs_angle<-unlist(lapply(angle(Subset2_1day_move_mercator), c, NA))
Subset2_3day_move_mercator$abs_angle<-unlist(lapply(angle(Subset2_3day_move_mercator), c, NA))

#calculate Relative Angle
Subset1_1hour_move_mercator$rel_angle<-unlist(lapply(turnAngleGc(Subset1_1hour_move_mercator),function(x) c(NA, x, NA)))
Subset1_3hr_move_mercator$rel_angle<-unlist(lapply(turnAngleGc(Subset1_3hr_move_mercator),function(x) c(NA, x, NA)))
Subset1_1day_move_mercator$rel_angle<-unlist(lapply(turnAngleGc(Subset1_1day_move_mercator),function(x) c(NA, x, NA)))
Subset1_3day_move_mercator$rel_angle<-unlist(lapply(turnAngleGc(Subset1_3day_move_mercator),function(x) c(NA, x, NA)))

Subset2_1hour_move_mercator$rel_angle<-unlist(lapply(turnAngleGc(Subset2_1hour_move_mercator),function(x) c(NA, x, NA)))
Subset2_3hr_move_mercator$rel_angle<-unlist(lapply(turnAngleGc(Subset2_3hr_move_mercator),function(x) c(NA, x, NA)))
Subset2_1day_move_mercator$rel_angle<-unlist(lapply(turnAngleGc(Subset2_1day_move_mercator),function(x) c(NA, x, NA)))
Subset2_3day_move_mercator$rel_angle<-unlist(lapply(turnAngleGc(Subset2_3day_move_mercator),function(x) c(NA, x, NA)))
```

#### Calculate Characteristics with Base R and 'adehabitatLT'

>Next I will transform my move objects into data frames and extract the characteristics I calculated previously.

```{r}
#1-hour
Subset1_1hour_move_azim_DF<-as.data.frame(Subset1_1hour_move_azim)
Subset1_1hour_move_mercator_DF<-as.data.frame(Subset1_1hour_move_mercator)
Subset1_1hour_move_azim_DF$rel_angle<-Subset1_1hour_move_mercator_DF$rel_angle
Subset1_1hour_move_azim_DF$abs_angle<-Subset1_1hour_move_mercator_DF$abs_angle
Subset1_1hour_move_azim_DF$turn_speed<-Subset1_1hour_move_azim_DF$speed * cos(Subset1_1hour_move_azim_DF$rel_angle)

Subset2_1hour_move_azim_DF<-as.data.frame(Subset2_1hour_move_azim)
Subset2_1hour_move_mercator_DF<-as.data.frame(Subset2_1hour_move_mercator)
Subset2_1hour_move_azim_DF$rel_angle<-Subset2_1hour_move_mercator_DF$rel_angle
Subset2_1hour_move_azim_DF$abs_angle<-Subset2_1hour_move_mercator_DF$abs_angle
Subset2_1hour_move_azim_DF$turn_speed<-Subset2_1hour_move_azim_DF$speed * cos(Subset2_1hour_move_azim_DF$rel_angle)

#3-hour
Subset1_3hr_move_azim_DF<-as.data.frame(Subset1_3hr_move_azim)
Subset1_3hr_move_mercator_DF<-as.data.frame(Subset1_3hr_move_mercator)
Subset1_3hr_move_azim_DF$rel_angle<-Subset1_3hr_move_mercator_DF$rel_angle
Subset1_3hr_move_azim_DF$abs_angle<-Subset1_3hr_move_mercator_DF$abs_angle
Subset1_3hr_move_azim_DF$turn_speed<-Subset1_3hr_move_azim_DF$speed * cos(Subset1_3hr_move_azim_DF$rel_angle)

Subset2_3hr_move_azim_DF<-as.data.frame(Subset2_3hr_move_azim)
Subset2_3hr_move_mercator_DF<-as.data.frame(Subset2_3hr_move_mercator)
Subset2_3hr_move_azim_DF$rel_angle<-Subset2_3hr_move_mercator_DF$rel_angle
Subset2_3hr_move_azim_DF$abs_angle<-Subset2_3hr_move_mercator_DF$abs_angle
Subset2_3hr_move_azim_DF$turn_speed<-Subset2_3hr_move_azim_DF$speed * cos(Subset2_3hr_move_azim_DF$rel_angle)

#1-day
Subset1_1day_move_azim_DF<-as.data.frame(Subset1_1day_move_azim)
Subset1_1day_move_mercator_DF<-as.data.frame(Subset1_1day_move_mercator)
Subset1_1day_move_azim_DF$rel_angle<-Subset1_1day_move_mercator_DF$rel_angle
Subset1_1day_move_azim_DF$abs_angle<-Subset1_1day_move_mercator_DF$abs_angle
Subset1_1day_move_azim_DF$turn_speed<-Subset1_1day_move_azim_DF$speed * cos(Subset1_1day_move_azim_DF$rel_angle)

Subset2_1day_move_azim_DF<-as.data.frame(Subset2_1day_move_azim)
Subset2_1day_move_mercator_DF<-as.data.frame(Subset2_1day_move_mercator)
Subset2_1day_move_azim_DF$rel_angle<-Subset2_1day_move_mercator_DF$rel_angle
Subset2_1day_move_azim_DF$abs_angle<-Subset2_1day_move_mercator_DF$abs_angle
Subset2_1day_move_azim_DF$turn_speed<-Subset2_1day_move_azim_DF$speed * cos(Subset2_1day_move_azim_DF$rel_angle)

#3-day
Subset1_3day_move_azim_DF<-as.data.frame(Subset1_3day_move_azim)
Subset1_3day_move_mercator_DF<-as.data.frame(Subset1_3day_move_mercator)
Subset1_3day_move_azim_DF$rel_angle<-Subset1_3day_move_mercator_DF$rel_angle
Subset1_3day_move_azim_DF$abs_angle<-Subset1_3day_move_mercator_DF$abs_angle
Subset1_3day_move_azim_DF$turn_speed<-Subset1_3day_move_azim_DF$speed * cos(Subset1_3day_move_azim_DF$rel_angle)

Subset2_3day_move_azim_DF<-as.data.frame(Subset2_3day_move_azim)
Subset2_3day_move_mercator_DF<-as.data.frame(Subset2_3day_move_mercator)
Subset2_3day_move_azim_DF$rel_angle<-Subset2_3day_move_mercator_DF$rel_angle
Subset2_3day_move_azim_DF$abs_angle<-Subset2_3day_move_mercator_DF$abs_angle
Subset2_3day_move_azim_DF$turn_speed<-Subset2_3day_move_azim_DF$speed * cos(Subset2_3day_move_azim_DF$rel_angle)
```

> Finally, I will transform my dataframe into an ltraj object with the azimuthal equidistant projection so I can calculate FPT

```{r}
#transform datasets into ltraj objects
Subset1_1hour_traj<-Subset1_1hour_move_azim_DF
coordinates(Subset1_1hour_traj) <- c("coords.x1", "coords.x2")
proj4string(Subset1_1hour_traj) <- azim_Bierr
Subset1_1hour_traj <- as.ltraj(coordinates(Subset1_1hour_traj),
                          date=Subset1_1hour_traj$timestamps,
                          id=Subset1_1hour_traj$trackId, typeII=TRUE)
Subset1_3hr_traj<-Subset1_3hr_move_azim_DF
coordinates(Subset1_3hr_traj) <- c("coords.x1", "coords.x2")
proj4string(Subset1_3hr_traj) <- azim_Bierr
Subset1_3hr_traj <- as.ltraj(coordinates(Subset1_3hr_traj),
                          date=Subset1_3hr_traj$timestamps,
                          id=Subset1_3hr_traj$trackId, typeII=TRUE)
Subset1_1day_traj<-Subset1_1day_move_azim_DF
coordinates(Subset1_1day_traj) <- c("coords.x1", "coords.x2")
proj4string(Subset1_1day_traj) <- azim_Bierr
Subset1_1day_traj <- as.ltraj(coordinates(Subset1_1day_traj),
                          date=Subset1_1day_traj$timestamps,
                          id=Subset1_1day_traj$trackId, typeII=TRUE)
Subset1_3day_traj<-Subset1_3day_move_azim_DF
coordinates(Subset1_3day_traj) <- c("coords.x1", "coords.x2")
proj4string(Subset1_3day_traj) <- azim_Bierr
Subset1_3day_traj <- as.ltraj(coordinates(Subset1_3day_traj),
                          date=Subset1_3day_traj$timestamps,
                          id=Subset1_3day_traj$trackId, typeII=TRUE)

Subset2_1hour_traj<-Subset2_1hour_move_azim_DF
coordinates(Subset2_1hour_traj) <- c("coords.x1", "coords.x2")
proj4string(Subset2_1hour_traj) <- azim_Bierr
Subset2_1hour_traj <- as.ltraj(coordinates(Subset2_1hour_traj),
                          date=Subset2_1hour_traj$timestamps,
                          id=Subset2_1hour_traj$trackId, typeII=TRUE)
Subset2_3hr_traj<-Subset2_3hr_move_azim_DF
coordinates(Subset2_3hr_traj) <- c("coords.x1", "coords.x2")
proj4string(Subset2_3hr_traj) <- azim_Bierr
Subset2_3hr_traj <- as.ltraj(coordinates(Subset2_3hr_traj),
                          date=Subset2_3hr_traj$timestamps,
                          id=Subset2_3hr_traj$trackId, typeII=TRUE)
Subset2_1day_traj<-Subset2_1day_move_azim_DF
coordinates(Subset2_1day_traj) <- c("coords.x1", "coords.x2")
proj4string(Subset2_1day_traj) <- azim_Bierr
Subset2_1day_traj <- as.ltraj(coordinates(Subset2_1day_traj),
                          date=Subset2_1day_traj$timestamps,
                          id=Subset2_1day_traj$trackId, typeII=TRUE)
Subset2_3day_traj<-Subset2_3day_move_azim_DF
coordinates(Subset2_3day_traj) <- c("coords.x1", "coords.x2")
proj4string(Subset2_3day_traj) <- azim_Bierr
Subset2_3day_traj <- as.ltraj(coordinates(Subset2_3day_traj),
                          date=Subset2_3day_traj$timestamps,
                          id=Subset2_3day_traj$trackId, typeII=TRUE)
```

```{r}
#before I can calculate FPT I need to calculate the average maximum spatial continuity within all eight datasets. I want the spatial continuity to be the same for all datasets so I can compare FPT between individuals if I desire.
#1-hour fpt
fpt_max_subset1_1hour<-data.frame()
for (i in 1:62) {
  x<-fpt(Subset1_1hour_traj[i], radii=0:100, units="hours") #calculate fpt values
  y<- varlogfpt(x, graph=TRUE) #variogram of fpt
  z<-as.numeric(y[1,])
  fpt<-max(z, na.rm = TRUE) #maximum of the variogram
  fpt_max_subset1_1hour<-rbind(fpt_max_subset1_1hour, fpt)
}
mean(fpt_max_subset1_1hour[,1]) #11 km
fpt_max_subset1_3hour<-data.frame()
for (i in 1:62) {
  x<-fpt(Subset1_3hr_traj[i], radii=0:100, units="hours") 
  y<- varlogfpt(x, graph=TRUE)
  z<-as.numeric(y[1,])
  fpt<-max(z, na.rm = TRUE)
  fpt_max_subset1_3hour<-rbind(fpt_max_subset1_3hour, fpt)
}
mean(fpt_max_subset1_3hour[,1]) #10 km
fpt_max_subset1_1day<-data.frame()
for (i in 1:62) {
  x<-fpt(Subset1_1day_traj[i], radii=0:100, units="hours")
  y<- varlogfpt(x, graph=TRUE)
  z<-as.numeric(y[1,])
  fpt<-max(z, na.rm = TRUE)
  fpt_max_subset1_1day<-rbind(fpt_max_subset1_1day, fpt)
}
mean(fpt_max_subset1_1day[,1]) #5 km
fpt_max_subset1_3day<-data.frame()
for (i in 1:62) {
  x<-fpt(Subset1_3day_traj[i], radii=0:100, units="hours")
  y<- varlogfpt(x, graph=TRUE)
  z<-as.numeric(y[1,])
  fpt<-max(z, na.rm = TRUE)
  fpt_max_subset1_3day<-rbind(fpt_max_subset1_3day, fpt)
}
mean(fpt_max_subset1_3day[,1]) #5 km

fpt_max_subset2_1hour<-data.frame()
for (i in 1:198) {
  x<-fpt(Subset2_1hour_traj[i], radii=0:100, units="hours") #calculate fpt
  y<- varlogfpt(x, graph=TRUE) #variogram of fpt
  z<-as.numeric(y[1,])
  fpt<-max(z, na.rm = TRUE) #maximum of the variogram
  fpt_max_subset2_1hour<-rbind(fpt_max_subset2_1hour, fpt)
}
mean(fpt_max_subset2_1hour[,1]) #11 km

fpt_max_subset2_3hour<-data.frame()
for (i in 1:198) {
  x<-fpt(Subset2_3hr_traj[i], radii=0:100, units="hours") #calculate fpt
  y<- varlogfpt(x, graph=TRUE) #variogram of fpt
  z<-as.numeric(y[1,])
  fpt<-max(z, na.rm = TRUE) #maximum of the variogram
  fpt_max_subset2_3hour<-rbind(fpt_max_subset2_3hour, fpt)
}
mean(fpt_max_subset2_3hour[,1]) #10 km

fpt_max_subset2_1day<-data.frame()
for (i in 1:198) {
  x<-fpt(Subset2_1day_traj[i], radii=0:100, units="hours") #calculate fpt
  y<- varlogfpt(x, graph=TRUE) #variogram of fpt
  z<-as.numeric(y[1,])
  fpt<-max(z, na.rm = TRUE) #maximum of the variogram
  fpt_max_subset2_1day<-rbind(fpt_max_subset2_1day, fpt)
}
mean(fpt_max_subset2_1day[,1]) #3 km

fpt_max_subset2_3day<-data.frame()
for (i in 1:198) {
  x<-fpt(Subset2_3day_traj[i], radii=0:100, units="hours") #calculate fpt
  y<- varlogfpt(x, graph=TRUE) #variogram of fpt
  z<-as.numeric(y[1,])
  fpt<-max(z, na.rm = TRUE) #maximum of the variogram
  fpt_max_subset2_3day<-rbind(fpt_max_subset2_3day, fpt)
}
mean(fpt_max_subset2_3day[,1]) #3 km

# I got errors trying to calculate spatial continuity values for 3-day frequencies and warnings for 1-day frequencies. It is interesting that average continuity goes down as the frequency gets courser. I will go ahead and use the 11 km spatial continuity from the 1-hour datasets for consistancy.  

#calculate FPT
#1-hour fpt
fpt_subset1_1hour<-data.frame()
for (i in 1:62) {
  x<-fpt(Subset1_1hour_traj[i], radii=0:100, units="hours")
  y<-x[[1]]$r11
  r<-as.data.frame(y)
  id<-id(Subset1_1hour_traj[i])
  r$migrationEvent<-paste(id)
  fpt_subset1_1hour<-rbind(fpt_subset1_1hour, r)
}
colnames(fpt_subset1_1hour)<- c("FPT", "migrationEvent")
Subset1_1hour_fin<-cbind(Subset1_1hour_move_azim_DF, fpt_subset1_1hour)

fpt_subset2_1hour<-data.frame()
for (i in 1:198) {
  x<-fpt(Subset2_1hour_traj[i], radii=0:100, units="hours")
  y<-x[[1]]$r11
  r<-as.data.frame(y)
  id<-id(Subset2_1hour_traj[i])
  r$migrationEvent<-paste(id)
  fpt_subset2_1hour<-rbind(fpt_subset2_1hour, r)
}
colnames(fpt_subset2_1hour)<- c("FPT", "migrationEvent")
Subset2_1hour_fin<-cbind(Subset2_1hour_move_azim_DF, fpt_subset2_1hour)

#3-hour fpt
fpt_subset1_3hr<-data.frame()
for (i in 1:62) {
  x<-fpt(Subset1_3hr_traj[i], radii=0:100, units="hours")
  y<-x[[1]]$r11
  r<-as.data.frame(y)
  id<-id(Subset1_3hr_traj[i])
  r$migrationEvent<-paste(id)
  fpt_subset1_3hr<-rbind(fpt_subset1_3hr, r)
}
colnames(fpt_subset1_3hr)<- c("FPT", "migrationEvent")
Subset1_3hr_fin<-cbind(Subset1_3hr_move_azim_DF, fpt_subset1_3hr)

fpt_subset2_3hr<-data.frame()
for (i in 1:198) {
  x<-fpt(Subset2_3hr_traj[i], radii=0:100, units="hours")
  y<-x[[1]]$r11
  r<-as.data.frame(y)
  id<-id(Subset2_3hr_traj[i])
  r$migrationEvent<-paste(id)
  fpt_subset2_3hr<-rbind(fpt_subset2_3hr, r)
}
colnames(fpt_subset2_3hr)<- c("FPT", "migrationEvent")
Subset2_3hr_fin<-cbind(Subset2_3hr_move_azim_DF, fpt_subset2_3hr)

#1-day
fpt_subset1_1day<-data.frame()
for (i in 1:62) {
  x<-fpt(Subset1_1day_traj[i], radii=0:100, units="hours")
  y<-x[[1]]$r11
  r<-as.data.frame(y)
  id<-id(Subset1_1day_traj[i])
  r$migrationEvent<-paste(id)
  fpt_subset1_1day<-rbind(fpt_subset1_1day, r)
}
colnames(fpt_subset1_1day)<- c("FPT", "migrationEvent")
Subset1_1day_fin<-cbind(Subset1_1day_move_azim_DF, fpt_subset1_1day)

fpt_subset2_1day<-data.frame()
for (i in 1:198) {
  x<-fpt(Subset2_1day_traj[i], radii=0:100, units="hours")
  y<-x[[1]]$r11
  r<-as.data.frame(y)
  id<-id(Subset2_1day_traj[i])
  r$migrationEvent<-paste(id)
  fpt_subset2_1day<-rbind(fpt_subset2_1day, r)
}
colnames(fpt_subset2_1day)<- c("FPT", "migrationEvent")
Subset2_1day_fin<-cbind(Subset2_1day_move_azim_DF, fpt_subset2_1day)

#3-day
fpt_subset1_3day<-data.frame()
for (i in 1:62) {
  x<-fpt(Subset1_3day_traj[i], radii=0:100, units="hours")
  y<-x[[1]]$r11
  r<-as.data.frame(y)
  id<-id(Subset1_3day_traj[i])
  r$migrationEvent<-paste(id)
  fpt_subset1_3day<-rbind(fpt_subset1_3day, r)
}
colnames(fpt_subset1_3day)<- c("FPT", "migrationEvent")
Subset1_3day_fin<-cbind(Subset1_3day_move_azim_DF, fpt_subset1_3day)

fpt_subset2_3day<-data.frame()
for (i in 1:198) {
  x<-fpt(Subset2_3day_traj[i], radii=0:100, units="hours")
  y<-x[[1]]$r11
  r<-as.data.frame(y)
  id<-id(Subset2_3day_traj[i])
  r$migrationEvent<-paste(id)
  fpt_subset2_3day<-rbind(fpt_subset2_3day, r)
}
colnames(fpt_subset2_3day)<- c("FPT", "migrationEvent")
Subset2_3day_fin<-cbind(Subset2_3day_move_azim_DF, fpt_subset2_3day)
```

> I am left with the following datasets:
* Subset1_1hour_fin
* Subset1_3hr_fin
* Subset1_1day_fin
* Subset1_3day_fin
* Subset2_1hour_fin
* Subset2_3hr_fin
* Subset2_1day_fin
* Subset2_3day_fin

#### Remove Nightime Fixes

> Lastly, I will remove fixes calculated during hours of darkness where the bird was travelling overland. while osprey may migrate at night on occasion, this typically only occurs over water or deserts. The osprey in this study do not encounter deserts, so I will remove any points during darkness that intersect with the World country polygons. 

```{r}
#calculate daylight. I used a 30 minute daylight buffer to account for possible movement at dusk. 
Subset1_1hour_fin$DaylightUTC<-computeIsDayByLocation(Subset1_1hour_fin$timestamps, 
                                                      latDeg = Subset1_1hour_fin$y, 
                                                      longDeg = Subset1_1hour_fin$x, 
                                                      timeZone = getHoursAheadOfUTC(Subset1_1hour_fin$timestamps), 
                                                      duskOffset = 0.5) 
Subset1_3hr_fin$DaylightUTC<-computeIsDayByLocation(Subset1_3hr_fin$timestamps, 
                                                      latDeg = Subset1_3hr_fin$y, 
                                                      longDeg = Subset1_3hr_fin$x, 
                                                      timeZone = getHoursAheadOfUTC(Subset1_3hr_fin$timestamps), 
                                                      duskOffset = 0.5)

Subset2_1hour_fin$DaylightUTC<-computeIsDayByLocation(Subset2_1hour_fin$timestamps, 
                                                      latDeg = Subset2_1hour_fin$y, 
                                                      longDeg = Subset2_1hour_fin$x, 
                                                      timeZone = getHoursAheadOfUTC(Subset2_1hour_fin$timestamps), 
                                                      duskOffset = 0.5) 
Subset2_3hr_fin$DaylightUTC<-computeIsDayByLocation(Subset2_3hr_fin$timestamps, 
                                                      latDeg = Subset2_3hr_fin$y, 
                                                      longDeg = Subset2_3hr_fin$x, 
                                                      timeZone = getHoursAheadOfUTC(Subset2_3hr_fin$timestamps), 
                                                      duskOffset = 0.5) 

#create sf objects. 
Subset1_1hour_fin_sf<-st_as_sf(x=Subset1_1hour_fin, coords = c("x", "y"), crs = wgs84)
Subset1_1hour_fin_sf<-st_transform(Subset1_1hour_fin_sf, azim_Bierr)
Subset1_3hr_fin_sf<-st_as_sf(x=Subset1_3hr_fin, coords = c("x", "y"), crs = wgs84)
Subset1_3hr_fin_sf<-st_transform(Subset1_3hr_fin_sf, azim_Bierr)

Subset2_1hour_fin_sf<-st_as_sf(x=Subset2_1hour_fin, coords = c("x", "y"), crs = wgs84)
Subset2_1hour_fin_sf<-st_transform(Subset2_1hour_fin_sf, azim_Bierr)
Subset2_3hr_fin_sf<-st_as_sf(x=Subset2_3hr_fin, coords = c("x", "y"), crs = wgs84)
Subset2_3hr_fin_sf<-st_transform(Subset2_3hr_fin_sf, azim_Bierr)

#filter out points during dark hours where birds where not over the ocean. 
Subset1_1hour_fin_sf_filter<-st_join(Subset1_1hour_fin_sf, world_filter, join = st_within)
Subset1_1hour_fin_sf_filter$name[is.na(Subset1_1hour_fin_sf_filter$name)] <- "Ocean" #need to replace NA values with a character for filtering
Subset1_1hour_fin_sf_filter$DaylightUTC<-as.character(Subset1_1hour_fin_sf_filter$DaylightUTC)
Subset1_1hour_fin_sf_filter<-Subset1_1hour_fin_sf_filter[!(Subset1_1hour_fin_sf_filter$name != "Ocean" & 
                              Subset1_1hour_fin_sf_filter$DaylightUTC == "FALSE"),]
Subset1_1hour_fin_filter<-st_drop_geometry(Subset1_1hour_fin_sf_filter)

Subset1_3hr_fin_sf_filter<-st_join(Subset1_3hr_fin_sf, world_filter, join = st_within)
Subset1_3hr_fin_sf_filter$name[is.na(Subset1_3hr_fin_sf_filter$name)] <- "Ocean" 
Subset1_3hr_fin_sf_filter$DaylightUTC<-as.character(Subset1_3hr_fin_sf_filter$DaylightUTC)
Subset1_3hr_fin_sf_filter<-Subset1_3hr_fin_sf_filter[!(Subset1_3hr_fin_sf_filter$name != "Ocean" & 
                              Subset1_3hr_fin_sf_filter$DaylightUTC == "FALSE"),]
Subset1_3hr_fin_filter<-st_drop_geometry(Subset1_3hr_fin_sf_filter)

Subset2_1hour_fin_sf_filter<-st_join(Subset2_1hour_fin_sf, world_filter, join = st_within)
Subset2_1hour_fin_sf_filter$name[is.na(Subset2_1hour_fin_sf_filter$name)] <- "Ocean"
Subset2_1hour_fin_sf_filter$DaylightUTC<-as.character(Subset2_1hour_fin_sf_filter$DaylightUTC)
Subset2_1hour_fin_sf_filter<-Subset2_1hour_fin_sf_filter[!(Subset2_1hour_fin_sf_filter$name != "Ocean" & 
                              Subset2_1hour_fin_sf_filter$DaylightUTC == "FALSE"),]
Subset2_1hour_fin_filter<-st_drop_geometry(Subset2_1hour_fin_sf_filter)

Subset2_3hr_fin_sf_filter<-st_join(Subset2_3hr_fin_sf, world_filter, join = st_within)
Subset2_3hr_fin_sf_filter$name[is.na(Subset2_3hr_fin_sf_filter$name)] <- "Ocean" 
Subset2_3hr_fin_sf_filter$DaylightUTC<-as.character(Subset2_3hr_fin_sf_filter$DaylightUTC)
Subset2_3hr_fin_sf_filter<-Subset2_3hr_fin_sf_filter[!(Subset2_3hr_fin_sf_filter$name != "Ocean" & 
                              Subset2_3hr_fin_sf_filter$DaylightUTC == "FALSE"),]
Subset2_3hr_fin_filter<-st_drop_geometry(Subset2_3hr_fin_sf_filter)

#optional mapping to visualize daylight and after daylight points..
names2<-unique(Subset1_3hr_fin_sf_filter$trackId)
for (i in names2) {
  y = filter(Subset1_1hour_fin_sf_filter, trackId == i)
  pws2<-plotshit2(y)
  print(pws2)
}

#Remove NA values
Subset1_1hour_fin_filter<-na.omit(Subset1_1hour_fin_filter[,c("time", "distance", "speed", "lag","migrationEvent", "rel_angle", "abs_angle", "turn_speed", "FPT" )])
Subset1_3hr_fin_filter<-na.omit(Subset1_3hr_fin_filter[,c("time", "distance", "speed", "lag","migrationEvent", "rel_angle", "abs_angle", "turn_speed", "FPT" )])
Subset1_1day_fin<-na.omit(Subset1_1day_fin[,c("time", "distance", "speed", "lag","migrationEvent", "rel_angle", "abs_angle", "turn_speed", "FPT" )])
Subset1_3day_fin<-na.omit(Subset1_3day_fin[,c("time", "distance", "speed", "lag","migrationEvent", "rel_angle", "abs_angle", "turn_speed", "FPT" )])

Subset2_1hour_fin_filter<-na.omit(Subset2_1hour_fin_filter[,c("time", "distance", "speed", "lag","migrationEvent", "rel_angle", "abs_angle", "turn_speed", "FPT" )])
Subset2_3hr_fin_filter<-na.omit(Subset2_3hr_fin_filter[,c("time", "distance", "speed", "lag","migrationEvent", "rel_angle", "abs_angle", "turn_speed", "FPT" )])
Subset2_1day_fin<-na.omit(Subset2_1day_fin[,c("time", "distance", "speed", "lag","migrationEvent", "rel_angle", "abs_angle", "turn_speed", "FPT" )])
Subset2_3day_fin<-na.omit(Subset2_3day_fin[,c("time", "distance", "speed", "lag","migrationEvent", "rel_angle", "abs_angle", "turn_speed", "FPT" )])
#Add in Individual IDs.
Subset1_1hour_fin_filter<-Subset1_1hour_fin_filter %>% 
    separate(migrationEvent,c("ID", "Year"), sep = "_(?=[^_]+$)", remove = FALSE)
Subset1_3hr_fin_filter<-Subset1_3hr_fin_filter %>% 
    separate(migrationEvent,c("ID", "Year"), sep = "_(?=[^_]+$)", remove = FALSE)
Subset1_1day_fin<-Subset1_1day_fin %>% 
    separate(migrationEvent,c("ID", "Year"), sep = "_(?=[^_]+$)", remove = FALSE)
Subset1_3day_fin<-Subset1_3day_fin %>% 
    separate(migrationEvent,c("ID", "Year"), sep = "_(?=[^_]+$)", remove = FALSE)

Subset2_1hour_fin_filter<-Subset2_1hour_fin_filter %>% 
    separate(migrationEvent,c("ID", "Region"), sep = "_(?=[^_]+$)", remove = FALSE)
Subset2_1hour_fin_filter$Year<-format(as.Date(Subset2_1hour_fin_filter$time, format="%d/%m/%Y"),"%Y")
Subset2_1hour_fin_filter<-merge(Subset2_1hour_fin_filter, IndividualMetadata, by.x = "migrationEvent", by.y = "SegmentID")
Subset2_1hour_fin_filter<-Subset2_1hour_fin_filter[,c(1:5,7:13)]

Subset2_3hr_fin_filter<-Subset2_3hr_fin_filter %>% 
    separate(migrationEvent,c("ID", "Region"), sep = "_(?=[^_]+$)", remove = FALSE)
Subset2_3hr_fin_filter$Year<-format(as.Date(Subset2_3hr_fin_filter$time, format="%d/%m/%Y"),"%Y")
Subset2_3hr_fin_filter<-merge(Subset2_3hr_fin_filter, IndividualMetadata, by.x = "migrationEvent", by.y = "SegmentID")
Subset2_3hr_fin_filter<-Subset2_3hr_fin_filter[,c(1:5,7:13)]

Subset2_1day_fin<-Subset2_1day_fin %>% 
    separate(migrationEvent,c("ID", "Region"), sep = "_(?=[^_]+$)", remove = FALSE)
Subset2_1day_fin$Year<-format(as.Date(Subset2_1day_fin$time, format="%d/%m/%Y"),"%Y")
Subset2_1day_fin<-merge(Subset2_1day_fin, IndividualMetadata, by.x = "migrationEvent", by.y = "SegmentID")
Subset2_1day_fin<-Subset2_1day_fin[,c(1:5,7:13)]

Subset2_3day_fin<-Subset2_3day_fin %>% 
    separate(migrationEvent,c("ID", "Region"), sep = "_(?=[^_]+$)", remove = FALSE)
Subset2_3day_fin$Year<-format(as.Date(Subset2_3day_fin$time, format="%d/%m/%Y"),"%Y")
Subset2_3day_fin<-merge(Subset2_3day_fin, IndividualMetadata, by.x = "migrationEvent", by.y = "SegmentID")
Subset2_3day_fin<-Subset2_3day_fin[,c(1:5,7:13)]

#Normalize
Subset1_1hour_norm<-as.data.frame(lapply(Subset1_1hour_fin_filter[, c(3,8:11)], scale))
Subset1_1hour_norm<-cbind(Subset1_1hour_norm, Subset1_1hour_fin_filter[,c(1:2, 4:7)])
Subset1_3hour_norm<-as.data.frame(lapply(Subset1_3hr_fin_filter[, c(3,8:11)], scale))
Subset1_3hour_norm<-cbind(Subset1_3hour_norm, Subset1_3hr_fin_filter[,c(1:2, 4:7)])
Subset1_1day_norm<-as.data.frame(lapply(Subset1_1day_fin[, c(3,8:11)], scale))
Subset1_1day_norm<-cbind(Subset1_1day_norm, Subset1_1day_fin[,c(1:2, 4:7)])
Subset1_3day_norm<-as.data.frame(lapply(Subset1_3day_fin[, c(3,8:11)], scale))
Subset1_3day_norm<-cbind(Subset1_3day_norm, Subset1_3day_fin[,c(1:2, 4:7)])

Subset2_1hour_norm<-as.data.frame(lapply(Sustrbset2_1hour_fin_filter[, c(4,8:10)], scale))
Subset2_1hour_norm<-cbind(Subset2_1hour_norm, Subset2_1hour_fin_filter[,c(1:3, 5:7, 11:12)])
Subset2_3hour_norm<-as.data.frame(lapply(Subset2_3hr_fin_filter[, c(4,8:10)], scale))
Subset2_3hour_norm<-cbind(Subset2_3hour_norm, Subset2_3hr_fin_filter[,c(1:3, 5:7, 11:12)])
Subset2_1day_norm<-as.data.frame(lapply(Subset2_1day_fin[, c(4,8:10)], scale))
Subset2_1day_norm<-cbind(Subset2_1day_norm, Subset2_1day_fin[,c(1:3, 5:7, 11:12)])
Subset2_3day_norm<-as.data.frame(lapply(Subset2_3day_fin[, c(4,8:10)], scale))
Subset2_3day_norm<-cbind(Subset2_3day_norm, Subset2_3day_fin[,c(1:3, 5:7, 11:12)])
```

## Data Analysis of Subset 1

#### Exploratory data analysis

> First I will look at the distribution of variables/characteristics for each individual for each sampling frequency using Q-Q plots. The distribution of my data for each individual will determine the kinds of tests I employ later on. 

```{r}
#1-hour
migrationEvents<-unique(Subset1_1hour_norm$ID)
for (i in migrationEvents) {
  x<-filter(Subset1_1hour_norm, ID == i)
  y<-x[,c(1:5)]
  for (j in 1:ncol(y)) {
    qqPlot(y[, j], main = names(y)[j])
  }
}

#3-hour
for (i in migrationEvents) {
  x<-filter(Subset1_3hour_norm, migrationEvent == i)
  y<-x[,c(1:5)]
  for (j in 1:ncol(y)) {
    qqPlot(y[, j], main = names(y)[j])
  }
}

#1-day
for (i in migrationEvents) {
  x<-filter(Subset1_1day_norm, migrationEvent == i)
  y<-x[,c(1:5)]
  for (j in 1:ncol(y)) {
    qqPlot(y[, j], main = names(y)[j])
  }
}

#3-day
for (i in migrationEvents) {
  x<-filter(Subset1_3day_norm, migrationEvent == i)
  y<-x[,c(1:5)]
  for (j in 1:ncol(y)) {
    qqPlot(y[, j], main = names(y)[j])
  }
}
```

#### PERMANOVA

> Many variables do not seem normally distributed. To avoid issues with normal distribution assumptions, I will use a non-parametric test. Since I am comparing groups that each have multiple variables, I will use a PERMANOVA. To start, you will first need to see if you data meets the PERMANOVA dispersion assumptions. Interpretation of p-values will be heavily influenced based on whether the assumptions are met. <https://www.youtube.com/watch?v=OyB_w4XNQ58>

```{r}
#test assumptions for PERMANOVA for each individual at each sampling frequency
x<-filter(Subset1_3hour_norm, ID == "Holly")
#determine if the test will be unbalanced. If there are hundreds of entries, an unbalanced design should not affect the results.  
table(x$migrationEvent) 
#Create a distance matrix
dis <- vegdist(x[,c(1, 3:5)], method = "euclidean")
#create a dispersion object
mod <- betadisper(dis, x$migrationEvent, type = "centroid")
#Test the homogeneity of dispersion between groups. If the p-value is > 0.05, then the assumptions of PERMANOVA are met: the null hypothesis is that overall dispersion between groups is not different.
permutest(mod) #test if dispersion from centroids drives differences. 
adonis2(dis ~ migrationEvent, data = x, method = "euclidean") #Alternate hypothesis - the groups are different from each other. 
```

```{r}
#add cluster labels to data 
cut_avg_3hour <- cutree(OSPR_3hour_clust, k = 2)
OSPR_3hour_norm<-mutate(OSPR_3hour_norm, cluster = cut_avg_3hour)

#test assumptions for PERMANOVA
dis_3hour <- vegdist(OSPR_3hour_norm[,1:7], method = "euclidean")
mod <- betadisper(dis_3hour, OSPR_3hour_norm$cluster, type = "centroid")
anova(mod) #Meets the assumptions
plot(mod) #largest group has the most dispersion = PERMANOVA test will be overly conservative
adonis2(dis_3hour ~ OSPR_3hour_norm$cluster, method = "euclidean") #The groups are not different
```

```{r}
#add cluster labels to data 
cut_avg_1day <- cutree(OSPR_1day_clust, k = 2)
OSPR_1day_norm<-mutate(OSPR_1day_norm, cluster = cut_avg_1day)

#test assumptions for PERMANOVA
dis_1day <- vegdist(OSPR_1day_norm[,1:7], method = "euclidean")
mod <- betadisper(dis_1day, OSPR_1day_norm$cluster, type = "centroid")
anova(mod) #Meets the assumptions
plot(mod) #largest group has the most dispersion = PERMANOVA test will be overly conservative
adonis2(dis_1day ~ OSPR_1day_norm$cluster, method = "euclidean") #The groups are not different
```

```{r}
#add cluster labels to data 
cut_avg_3day <- cutree(OSPR_3day_clust, k = 6)
OSPR_3day_norm<-mutate(OSPR_3day_norm, cluster = cut_avg_3day)

#test assumptions for PERMANOVA
dis_3day <- vegdist(OSPR_3day_norm[,1:7], method = "euclidean")
mod <- betadisper(dis_3day, OSPR_3day_norm$cluster, type = "centroid")
anova(mod) #Does not meet assumptions
plot(mod) #largest group has the most dispersion = PERMANOVA test will be overly conservative
adonis2(dis_3day ~ OSPR_3day_norm$cluster, method = "euclidean") #The groups are not different
```

## Data Analysis of Subset 2

## References

* Help using the move package to download data from Movebank: <https://cran.r-project.org/web/packages/move/vignettes/browseMovebank.html> 
* Navigating and using data from the move package: <https://cran.r-project.org/web/packages/move/vignettes/move.html>
* Download a CSV file from github: <https://lokraj.me/post/download-github-data/>
* Github and RStudio interface: <https://rfortherestofus.com/2021/02/how-to-use-git-github-with-r/>
* Finding mode in R: <https://www.statology.org/mode-in-r/>
* migrateR Vignette: <https://github.com/dbspitz/migrateR/blob/master/migrateR%20vignette.pdf>
* Projections in R: <https://www.nceas.ucsb.edu/sites/default/files/2020-04/OverviewCoordinateReferenceSystems.pdf>
* adehabitatLT use (not the vignette): <http://www.r-gators.com/2018/01/31/wildlife-tracking-data-in-r/>
* Hopkins stat: <https://sushildeore99.medium.com/really-what-is-hopkins-statistic-bad1265df4b>, <https://stats.stackexchange.com/questions/332651/validating-cluster-tendency-using-hopkins-statistic>
* Basic HCA and Tanglegrams: <https://uc-r.github.io/hc_clustering>
* Dendrogram modification: <https://rpubs.com/JTK/hclust-color>




