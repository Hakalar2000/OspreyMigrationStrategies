---
title: "Lewis' Thesis Code"
author: "Lewis Hakam"
output: html_document
---

```{r setup, include=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(move) #Download, manipulate, anlayze data from Movebank
library(sp) #Spatial objects in Movebank
library(moveVis) #Converting a data frame to a move object
library(plyr) #Batch calculating stats for subsets of data
library(dplyr) #Deleted and re installed rlang
library(tidyr) #Extracting character string values
library(readr) #Read in CSV files
library(stats) #Basic stats functions and tools
library(devtools) #Download non-cran packages
library(lubridate) #Ensuring date formats are correct
library(solartime) #Calculate daylight hours
find_mode <- function(x) {
  u <- unique(x)
  tab <- tabulate(match(x, u))
  u[tab == max(tab)]
} #No built in function for mode in R so I got this one from the web. 
library(data.table) #Manipulating data tables
library(adehabitatLT) #Creating trajectories and calculating characteristics. 
library(leaflet) #Interactive maps
library(htmltools) #Popups for interactive maps
library(ggplot2) #Graphs and maps
library(sf) #Spatial objects for leaflet and ggplot
library(tmap) #Package containing world country polygons
wgs84<-CRS("+proj=longlat +datum=WGS84") #GCS in WGS 1984
azim_Bierr = CRS("+proj=aeqd +lat_0=19 +lon_0=-72 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m +no_defs") #PCS in custom azimuthal equidistant projection. 
data(World) #World data with country polygons needed for mapping
world_sf<-st_transform(World, azim_Bierr) #Projected world data
world_filter<-world_sf[,"name"] #Reduce info for spatial join
world_filter$name<-as.character(world_filter$name) #Ensure world polygon data is an appropriate data type
plotshit<-function(y) {
  ggplot(y) +
    geom_sf(data = world_sf, fill = "#464646") +
    geom_sf(size = 1, aes(color = year)) +
    coord_sf(datum = st_crs(azim_Bierr), xlim = c(-4000000, 3000000), ylim = c(3500000, -2500000)) +
    theme(legend.position = "bottom", legend.text = element_text(size = 7)) +
    labs(color = "year") +
    ggtitle(paste0(y$trackId))
} #Function for batch plotting in a for loop
plotshit2<-function(y) {
  ggplot(y) +
  geom_sf(data = world_sf, fill = "#464646") +
  geom_sf(size = 1, aes(color = DaylightUTC)) +
  coord_sf(datum = st_crs(azim_Bierr), xlim = c(-4000000, 3000000), ylim = c(3500000, -2500000))+
  ggtitle(paste0(y$trackId))
} #Daylight assessment
library(car) # Q-Q plots for EDA
library(rstatix) #Shapiro-Wilk Normality test
library(vegan) #PERMANOVA
library(pairwiseAdonis) # PERMANOVA post-hoc multilevel comparison test
library(MicEco) # Omega-squared values
library(stringr) #Extracting string info from dataframes. Useful for manipulating text column values.
library(factoextra) #Customizing PCA biplots
library(FactoMineR) #PCA
library(RColorBrewer) #Creating custom color palette
library(ComplexHeatmap) #Correlation plots in R
library(stats) #TukeyHSD - package comes with base R
library(rootdetectR) #Making results of TukeyHSD into a matrix
```

## Introduction

> Hey there! If you are viewing this, you are interested in how I analyzed osprey tracking data for my final thesis, completed in Spring 2024. I will guide through my step by step process to answer three questions:

> **Do individual ospreys use the same migration strategy between years or each other?**
> **Do ospreys use the same migration strategy between regions?**
> **How does varying the temporal sampling frequency affect my results?**

> I have used R Markdown, which may be new for you. This document will not go over the nuances I used to code the Markdown file. For simplicity and privacy, not all code lines are shown in the Rmarkdown output when downloading data. I have done my best to annotate my code for easy interpretation, but this is not a guide on how to use R. Some aspects of coding will not be explained.


## A Note on Installing Packages

> You will likely have to use the function install.packages ("Package") before you can access packages using the library (Package) function if you are new to R. I had a few instances where I had supporting packages downloaded, but they were not up-to-date nor could I force them to update. For these packages I had to locally delete the packages that were not updating and then re-install them. Many of these packages have dependencies (they require other packages) so do not be alarmed if download takes a while. Lastly, some packages are not from CRAN, rather they are from personal repositories (repos) on github or from Bioconductor. These packages require different download code. To download packages from repos, you need to use the install_github("username/repo/package file name",) from the 'devtools' package. For Bioconductor packages you need to use the install("package name") function from the 'BioManager' package.

* CRAN: install.packages() from base R 
* BioConductor: install() from BioManager 
* Personal Repositories: install_github() from devtools

## Downloading and Filtering Tracking Data

#### Setup

> First, If you do not have a Movebank account, you must make a free account before you can access data. Even after you make an account, not all data are available for download. There are three levels of privacy on Movebank: downloadable (orange), visible fixes with permission required to download (blue), and no visible fixes with permission required to download (grey). More information about navigating Movebank can be found on the Movebank website: <https://www.movebank.org/cms/movebank-main>

> After reviewing accessible data sets, I determined that I could only use is "Osprey Bierregaard North and South America". I chose this data set since it provided me with the most individuals with 1-hour or finer sampling frequencies and allowed me to control for possible regional differences among worldwide osprey populations. The dataset consists of ospreys from Newfoundland to South Carolina tracked from 1995-2019. Not all of these individuals are tracked using GPS or GPS/GSM devices with a 1-hour or finer sampling frequency. 

> To start, you will consult the reference data and see if you can reduce the dataset before downloading all fixes. This dataset is large, and the more data you pull, the longer it will take. Reducing the number of points before downloading will significantly increase processing speeds. To pull reference data or datasets from Movebank you will use the 'move' package. Before you can pull data with the move package you will need to create an object called loginStored which contains your login info. The code below is an example, since I do not want to give out my actual password. 

>Your code will look something like this:

```{r, eval=FALSE}
loginStored <- movebankLogin(username = "UserName", password = "Password")
```


#### Downloading Movebank Summary Table

> Next you will download the Movebank study synopsis and summary tables. It is possible to search for studies within R, but you will save yourself some time using the Movebank website's interactive map. 
* The synopsis can provide details on how many individuals were tracked, the time frame tracking occurred, the type of device(s) used, demographic information (sex, age, etc.), and permission/privacy restrictions. 
* The summary table will provide a summary of each individual tracked per device. This means that some individuals may have multiple entries in a summary table if they were fitted with more than one transmitter. You can use the summary table info to partially fill out demographic information and determine how many individuals were tracked using GPS or GPS/GSM devices. I kept a running document of individuals with their demographic and geographic information that will come in handy later on. 

```{r, warning=FALSE}
#download Bierregaard data Synopsis
reference<-getMovebankStudy(study ="Osprey Bierregaard North and South America", login = loginStored)
reference$study_objective
#download the summary table
Bierregaard_Study_summary<-getMovebankReferenceTable(study ="Osprey Bierregaard North and South America", login = loginStored, allAttributes = FALSE)
#Notice that each individual has up to 3 entries. Some individuals only have one entry with sensor_type_id equalling 82789. These individuals were tracked with devices that only collected doppler shift locations. These individuals were mostly tracked before 2007. Some of these individuals may have two entries, but you can always look at tag comments which provide info on whether locations  were estimated via doppler shift or GPS. 
#Explore sensor attributes and associated codes. We only want a summary of individuals with GPS or GPS/GSM tracks. We also do not need accessory or engineering data. 
attributes<-getMovebankSensorsAttributes(study="Osprey Bierregaard North and South America",login=loginStored) #sensor_type_id for GPS locations that we will need is 653. 7842954 is the GPS engineering data and 82798	is doppler shift data. Filter the summary table to keep only entries with the sensor_type_id of 653.
Bierregaard_Osprey_summary<-filter(Bierregaard_Study_summary,
                                   Bierregaard_Study_summary$sensor_type_id == 653)
#Fill in attributes of individuals on a spreadsheet. 
```

> The summary tables give me an idea of the total number of individuals in each study and the sex and age of each individual. However, it does not tell me the number of fall migrations for each individual, whether those migrations were complete or not, nor if the individual even started migration. Age and sex data may also be missing. Sometimes sex is unknown, especially for juveniles, which are not sexually dimorphic. 

#### Download Movebank Dataset

> The data sets are really large, so I filtered the data to make it easier to work with and plot. 

> Remove:
* individuals with sampling frequencies courser than 1-hour (using mode and median)
* GPS fixes outside of July-December (7-12) when osprey are "stationary" or on spring migrations
* erroneous points that don't meet a defined biological/practical threshold.

```{r}
#Download data from Movebank for GPS tracked individuals.This takes some time. When downloading Movebank data, I can specify which individuals I want in my dataset using animalName = (list of animal local identifiers)
Bierregaard_Osprey<-getMovebankData(study ="Osprey Bierregaard North and South America",
                                    animalName = c(Bierregaard_Osprey_summary$animal_local_identifier),
                                    login = loginStored, removeDuplicatedTimestamps=TRUE) 
citations(Bierregaard_Osprey) #no citation recorded
#Get the time between consecutive locations using the move package
#Start by calculating the time lag betwen points for each individual. The output is a list. 
list_Bierr<-timeLag(Bierregaard_Osprey, units = "hours")
#lapply function applies a function to a list. Each individual within the data set has a median or mode timelag of 1 hour or finer. I used median/mode instead of average since its most studies only have trackers turned on during daylight hours, which means some lag values will be 10 hours or larger. This results in a mean sampling frequency greater than one. Osprey are diurnal and typically sleep at night with a few exceptions. It is often more advantageous to turn off trackers at night to conserve battery power than to continue recording locations for birds after daylight. 
sapply(list_Bierr, mean) #You can see thant most study averages are larger than 1 point per hour .
sapply(list_Bierr, find_mode) # birds with a finer than 1-hour sampling frequency were tracked using GPS/GSM transmitters. 
sapply(list_Bierr, median) #results are similar to mode. 

#make into a data frame so I can easily filter, plot, and manipulate the data.
Bierregaard_Osprey_DF<-as.data.frame(Bierregaard_Osprey)
#Filter out non GPS location entries
Bierregaard_Osprey_DF_GPS<-Bierregaard_Osprey_DF %>%
  filter(sensor_type_id == 653) #reduces dataset by around 50,000 points. 
#Extract month data for filtering
Bierregaard_Osprey_DF_GPS$month<-month(ymd_hms(Bierregaard_Osprey_DF_GPS$timestamp))
#Extract year data for plotting later
Bierregaard_Osprey_DF_GPS$year<-year(ymd_hms(Bierregaard_Osprey_DF_GPS$timestamp))
length(unique(Bierregaard_Osprey_DF_GPS$trackId)) #86 individuals
#remove outliers
#filter quantitative erroneous points: Erroneous points are those with unrealistically high speed and turning angle (course). For course (often referred to and confused with heading) we will look at the 90th percentile of points as suggested by Gupte et al., 2021. Course is measured as the angle from magnetic north (clockwise) in which the tracker is moving. For speed I used Kerlinger 1989 to determine that the maximum recorded flight speed of an osprey was 33.4 meters/second.
quant<-quantile(Bierregaard_Osprey_DF_GPS$heading, probs = 0.9)
Bierregaard_Osprey_DF_Filter<-filter(Bierregaard_Osprey_DF_GPS, ground_speed <= 33.4 & heading < quant) #about 25,000 fixes were removed. 
#Filter data further by removing timestamps from January through June (keep months 7-12)
Bierregaard_Osprey_Data <- Bierregaard_Osprey_DF_Filter %>%
  filter(month %in% c(7:12))
#Create migration event IDs. Later I will create segment IDs. 
Bierregaard_Osprey_Data$migrationEvent<-paste(Bierregaard_Osprey_Data$trackId, Bierregaard_Osprey_Data$year)
length(unique(Bierregaard_Osprey_Data$migrationEvent)) #86 individuals with 145 migration events between them.
#Check that the IDs: unique(Bierregaard_Osprey_Data$migrationEvent)
#Format the migration event names so that they don't have any spaces or special characters
Bierregaard_Osprey_Data$migrationEvent<-gsub("\\.", "_", Bierregaard_Osprey_Data$migrationEvent) #remove periods
Bierregaard_Osprey_Data$migrationEvent<-gsub(" ", "_", Bierregaard_Osprey_Data$migrationEvent) #remove spaces
```

#### Mapping and Filtering Data

> First, I will quickly map the data for each individual by year to determine if I should remove any individuals that obviously did not complete at least two migration segments between all tracking years. Ospreys typically have three migration segments defined as follows:
* travel from and across the US mainland. Exceptions are for those ospreys that started by travelling over the Atlantic. The ending point of the first segment is the point closest to the border between the first and second segment.
* Across the Caribbean starting from the ending point of the first second to the point closest tothe border between the second and third segemnt. 
* Across central/South America until the final migration point. 
There are a few exceptions to the three segments as some osprey wintered in Florida or Cuba. 

> I will use 'ggplot' and 'leaflet', which both rely on 'sf' spatial objects for mapping purposes. I will map each individual bird, with tracks color-coded by year so I know if the bird has more than one fall migration that I can use. This requires the custom 'plotshit' function and a for loop. Data on the background world countries is from the 'tmap'. Data is projected using a custom aziumthal equidistant projection. The actual projection used to map data is not all that important at the moment since we are not measuring anything, however the projection will be important later on. The projection was defined by using google maps to estimate the center of all of my data. 

```{r, fig.show="hold", out.width="10%"}
#Spatially project the Bierregaard data
Bierregaard_Osprey_Data_sf<-st_as_sf(x=Bierregaard_Osprey_Data, 
                                     coords = c("location_long.1", "location_lat.1"), crs = wgs84)
Bierregaard_Osprey_Data_sf<-st_transform(Bierregaard_Osprey_Data_sf, azim_Bierr)
#to color code year, ggplot needs the year column to be a factor data type. 
Bierregaard_Osprey_Data_sf$year<-as.factor(Bierregaard_Osprey_Data_sf$year) 
#batch map the GPS points for each individual using ggplot
names<-unique(Bierregaard_Osprey_Data_sf$trackId)
for (i in names) {
  y = filter(Bierregaard_Osprey_Data_sf, trackId == i)
  pws<-plotshit(y)
  print(pws)
}
```

> From the plots, I can get an idea of how many fall migration events occurred for each bird. I can also tell if some individuals were juveniles during their first migration as juveniles spend 18 months wintering and return to breeding areas at three-years old. Juvenile wintering periods during the expected fall migration period were removed. Finally, I removed events where birds did not complete at least one segment of their migration or where the tracks were suspiciously thin for birds tracked in 2007 or earlier. Some birds never left their breeding/natal grounds, which means they cannot be used in my analysis.   

> Next, I will further filter events and individuals by taking a close look at each event using an interactive map. I will also determine the start/end dates of migration and the start/end latitude and longitude of each segment. 

```{r}
#project data using a GCS
Bierregaard_Osprey_Data_sf<-st_as_sf(x=Bierregaard_Osprey_Data, coords = c("location_long", "location_lat"), crs = wgs84)

#Create and interactive map where you can click on points for each migration event and visually determien start/end of migration. I did this for each event separately so I left an example of a plot here. 
leaflet(filter(Bierregaard_Osprey_Data_sf, migrationEvent == "Art_2012")) %>% 
  addTiles() %>%
  addCircleMarkers(radius = 2, color = "red", opacity = 100) %>%
  addPopups(popup = ~htmlEscape(timestamp))
```

>Remove events that do not fit criteria. 

```{r}
#create a list of migration events to remove. 
remove<-c("Woody_2015", "Weber_2014", "Virginia_2016", "Uncas_2014", "Trepassey_2016", "Tommy_2017", "Tilton_2014", "Thatcher_2011", "Snowy_2012", "Sanford_2011", "Saco_2011", "Ron_2015", "Roger_Tory_2014", "Rodney_2014", "Rock_2017", "Rammie_2013", "Rafael_2009", "Penelope_2009", "Peirce_2013", "Pearl_2013", "North_Fork_Bob_2015", "Neale_2010", "Mackenzie_2013", "Luke_2007", "Lizzie_2015", "Little_Ricky_2008", "Layla_2017", "Katy_2009", "Katbird_2011", "Jocelyn_2016", "Hix_Jr_2009", "Gunny_2010", "Gerry_2014", "Flow_2015", "Flow_2017", "Felix_2007", "Daphne_2017", "Claws_2007", "Claws_2008", "Chip_2012", "Chester_2014", "Captain_Liz_2013", "Caleb_2013", "Buck_2010", "Bridget_2014", "Bridger_2012", "Borealis_2017", "Borealis_2018", "Blackie_2014", "Belle_2011", "Aster_2017", "Artoo_2014", "Art_2013")

Bierregaard_Osprey_Data$migrationEvent<-as.character(Bierregaard_Osprey_Data$migrationEvent)
Bierregaard_Osprey_Data<-Bierregaard_Osprey_Data[!(Bierregaard_Osprey_Data$migrationEvent %in% remove), ]
length(unique(Bierregaard_Osprey_Data$migrationEvent))
length(unique(Bierregaard_Osprey_Data$trackId)) #92 migration events for 51 individuals
```

>#While using leaflet works, this method will take a long grueling time to complete. If you want to expedite this process you can export the data points to CSV files and find start/end dates in ArcGIS or QGIS. The code below will allow you to do this. 

```{r, eval=FALSE}
Bierr_Event<-unique(Bierregaard_Osprey_Data$migrationEvent)
Bierregaard_Osprey_Data$migrationEvent<-gsub(" ", "_", Bierregaard_Osprey_Data$migrationEvent)
for (i in Bierr_Event) {
  ID2 <- subset(Bierregaard_Osprey_Data, migrationEvent == i)
  write.csv(ID2, file = paste0("Z://Personal_Folders/Lewis/Thesis/ROutput/",i, ".csv"))
} #make sure you change the file path so data is saved to your own folder.  
```

> Next you will remove all fixes for each migration event that fall outside of the start and end date of migration. You will have to load in the metadata file with the migration event start/end dates.

```{r, warning=FALSE}
IndividualMetadata <- read_csv("IndividualMetadata.csv")
#make sure this is a dataframe
IndividualMetadata<-as.data.frame(IndividualMetadata)
#Format start/end timestamps so they are date/time format. I keep having to go back to the CSV file and adjust these values to yyyy-mm-dd hh:mm:ss
IndividualMetadata$Seg_End<-ymd_hms(IndividualMetadata$Seg_End)
IndividualMetadata$Seg_Start<-ymd_hms(IndividualMetadata$Seg_Start)
IndividualMetadata$Mig_Start<-ymd_hms(IndividualMetadata$Mig_Start)
IndividualMetadata$Mig_End<-ymd_hms(IndividualMetadata$Mig_End)
#I suggest checking the column class with the class() to ensure this worked properly.
#Now filter out non-migratory fixes for each migration event.
events<-unique(Bierregaard_Osprey_Data$migrationEvent)
Filtered<-data.frame()
for (i in events) {
  y = filter(Bierregaard_Osprey_Data, migrationEvent == i)
  x = filter(IndividualMetadata, migrationEvent == i)
  n = filter(y, timestamp <= x$Mig_End & timestamp >= x$Mig_Start)
  Filtered = rbind(Filtered, n)
}
```


## Create Data Subsets
#### Regularize the Trajectories

> Before we break down data into subsets, I need to regularized the data. Data was not collected at perfect 1-hour intervals for a 24 hour period. This means that when I calculate trajectory characteristics, points from the end of one day may connect to points at the start of the next day. If the osprey was sleeping during this time period, speed values would be very low and add inaccuracy to my dataset since I am only concerned with active migration. Regularizing will help me pinpoint times when osprey roosted so those points can be taken out later for 1-hour and 3-hour sampling frequencies after calculating trajectory characteristics. There are of course exceptions when osprey cross over open water. In these cases, speed should be high even after dark. 

```{r}
#Convert to a move object. To do this I had to use the moveVis package since the move function does not work well with actual dataframes. The move package has a number of very convenient functions that allow me to calculate a number of different movement characteristics and prep the data from analysis.
OSPR<-df2move(Filtered,
        proj = wgs84, 
        x = "location_long", y = "location_lat", 
        time = "timestamp", track_id = "migrationEvent")
#check to see that there are still 92 migration events. Each event in the movestack is named with a unique trackID. 
length(unique(Filtered$migrationEvent))
length(unique(OSPR@trackId))
#regularize the time series through the move package. To do this we will have to apply the regularization to each move object within the Movestack (annoying, I know). The interpolate time function fills in positional points along a standard time series. We will use 1-hour to do this. 
Stacked_1hour<-list() #empty list for output

for (i in 1:92) {
  x<-interpolateTime(OSPR[[i]], time=as.difftime(1, units="hours"), spaceMethod='rhumbline')
  plot(x, col="red",pch=20, main="By time interval")
  points(OSPR[[i]], col = "black", pch = 20)
  print(legend("bottomleft", c("True locations", "Interpolated locations"), 
               col=c("black", "red"), pch=c(19,20)))
  Stacked_1hour[[i]] = x
}
#convert output list of move objects into a move stack.
OSPR_1hour<-moveStack(Stacked_1hour) 
length(unique(OSPR_1hour@trackId)) #check that all migration events are accounted for

list_1hour<-timeLag(OSPR_1hour, units = "hours")
sapply(list_1hour, median) 
```

#### Create Subsets 1
> I will first determine which migration events/individuals should belong to each subset.
* Subset 1: individuals with at least two migration events that have at least one complete segment each
* Subset 2: individuals with at least two complete segments (keep all events). 

```{r}
OSPR_1hour_DF<-as.data.frame(OSPR_1hour)
subset1_keep<-c("Artoo_2013", "Artoo_2015", "Belle_2010", "Belle_2012", "Belle_2013", "Belle_2014", "Belle_2015", "Belle_2016", "Buck_2009", "Buck_2011", "Charlie_2014", "Charlie_2015", "DJ_2013", "DJ_2014", "Donovan_2013", "Donovan_2014", "Donovan_2015", "Edwin_2013", "Edwin_2014", "Edwin_2015", "Flow_2014", "Flow_2016", "Holly_2016", "Holly_2017", "Holly_2018", "Holly_2019", "Mr__Hannah_2009", "Mr__Hannah_2010", "Nick_2013", "Nick_2014", "Nick_2015", "Nick_2016", "North_Fork_Bob_2010", "North_Fork_Bob_2011", "North_Fork_Bob_2012", "North_Fork_Bob_2013", "North_Fork_Bob_2014", "Penelope_2008", "Penelope_2010", "Quin_2013", "Quin_2014", "Quin_2015", "Ron_2013", "Ron_2014", "Shanawdithit_2016", "Shanawdithit_2017", "Snowy_2011", "Snowy_2013", "Snowy_2014", "Snowy_2015", "Sr__Bones_2010", "Sr__Bones_2011", "Sr__Bones_2012", "Sr__Bones_2013", "Staddler_2016", "Staddler_2015", "Staddler_2017", "Thatcher_2010", "Thatcher_2012", "Wausau_2015", "Wausau_2016", "Woody_2013", "Woody_2014")
OSPR_1hour_DF$trackId<-as.character(OSPR_1hour_DF$trackId)
Subset1_1hour<-OSPR_1hour_DF[(OSPR_1hour_DF$trackId %in% subset1_keep), ]
unique(Subset1_1hour$trackId) #63 tracks from 22 individuals

#subset 2 is easy
Subset2_1hour<-OSPR_1hour_DF
```

#### Segment Subset 2

> I will segment subset 2 now so I don't have to segment every thinned dataset. 

```{r, warning=FALSE}
#Separate data into segments based on the start/end date of each segment and combine into a singular data frame
segment_events<-unique(Subset2_1hour$trackId)
segs<-data.frame()
for (i in segment_events) {
  y = filter(Subset2_1hour, trackId == i)
  x = filter(IndividualMetadata, migrationEvent == i)
  for (j in 1:nrow(x)) {
    z = x[j,] #you have to specify the row the row
    n = filter(y, timestamps <= z$Seg_End & timestamps >= z$Seg_Start)
    b = merge(n, z, by.x = "trackId", by.y ="migrationEvent")
    segs = rbind(segs, b)
  }
}

#check to make the number of segments matches those in the metadata.
length(unique(IndividualMetadata$SegmentID))
length(unique(segs$SegmentID))

#filter out segments with a duration less than 3 days. When thinning segments, I will lose those segments with such a short timespan. Duration was calculated in the metadata spreadsheet
segments<-filter(segs, Duration >= 3)
length(unique(segments$SegmentID)) #25 segments removed
#look to see if any individuals no longer fit the criteria of having at least two complete segments.
table(segments$Segment, segments$Indvidual)
#remove individuals
segment_remove<-c("Henrietta", "Isabel", "Moffet", "Ozzie", "Quin")
segments_final<-segments[!(segments$Indvidual %in% segment_remove), ]
length(unique(segments_final$Indvidual))
length(unique(segments_final$SegmentID)) #46 individuals with 198 migration segments between them.
```

#### Thin Trajectories

> Next I will thin my trajectories to 3-hour, 1-day, and 3-day sampling frequencies for both subsets.

```{r}
#subset 1
#First create a move object
Subset1_1hour_move<-df2move(Subset1_1hour,
        proj = wgs84, 
        x = "coords.x1", y = "coords.x2", 
        time = "timestamps", track_id = "trackId")
#Regularizing thinned/resampled data to 1-hour so no need to thin to 1 hour again. 

#3-hour sampling frequency
Subset1_3hr<-list()
for (i in 1:63) {
  x<-thinTrackTime(Subset1_1hour_move[[i]], interval = as.difftime(3, units="hours"),
                          tolerance = as.difftime(15, units="mins"), criteria = "closest")
  split<-move::split(x)
  Subset1_3hr[[i]] = split$selected
}
Subset1_3hr_move<-moveStack(Subset1_3hr) 
str(Subset1_3hr_move@trackId)
#1-day sampling frequency
Subset1_1day<-list()
for (i in 1:63) {
  x<-thinTrackTime(Subset1_1hour_move[[i]], interval = as.difftime(1, units="days"),
                          tolerance = as.difftime(2, units="hours"), criteria = "closest")
  split<-move::split(x)
  Subset1_1day[[i]] = split$selected
}
Subset1_1day_move<-moveStack(Subset1_1day) 
str(Subset1_1day_move@trackId)

#Check to see if thinning looks right. 
plot(Subset1_1hour_move[[1]])
plot(Subset1_3hr_move[[1]])
plot(Subset1_1day_move[[1]])
```

```{r}
#Subset2

#First create a move object
Subset2_1hour_move<-df2move(segments_final,
        proj = wgs84, 
        x = "coords.x1", y = "coords.x2", 
        time = "timestamps", track_id = "SegmentID")
#3-hour sampling frequency
Subset2_3hr<-list()
for (i in 1:198) {
  x<-thinTrackTime(Subset2_1hour_move[[i]], interval = as.difftime(3, units="hours"),
                          tolerance = as.difftime(15, units="mins"), criteria = "closest")
  split<-move::split(x)
  Subset2_3hr[[i]] = split$selected
}
Subset2_3hr_move<-moveStack(Subset2_3hr) 
str(Subset2_3hr_move@trackId)
#1-day sampling frequency
Subset2_1day<-list()
for (i in 1:198) {
  x<-thinTrackTime(Subset2_1hour_move[[i]], interval = as.difftime(1, units="days"),
                          tolerance = as.difftime(2, units="hours"), criteria = "closest")
  split<-move::split(x)
  Subset2_1day[[i]] = split$selected
}
Subset2_1day_move<-moveStack(Subset2_1day) 
str(Subset2_1day_move@trackId)

#Check to see if thinning looks right. 
plot(Subset2_1hour_move[[1]])
plot(Subset2_3hr_move[[1]])
plot(Subset2_1day_move[[1]])
```

## Calculate Trajectory Characteristics and Summarize Data
Now we have all four data sets. The final step before analysis is to calculate trajectory characteristics.

#### Calculate Characteristics with 'move'

> When calculating distance measurements, Keep in mind that data must be projected using a projection that preserves distance (best projection to use is an azimuthal equidstant projection), while angle measurements need to be calculated using a mercator projection. 

```{r}
#Project data with custom azimuthal equidistant projection
Subset1_1hour_move_azim <- spTransform(Subset1_1hour_move, CRSobj=azim_Bierr)
Subset1_3hr_move_azim <- spTransform(Subset1_3hr_move, CRSobj=azim_Bierr)
Subset1_1day_move_azim <- spTransform(Subset1_1day_move, CRSobj=azim_Bierr)

Subset2_1hour_move_azim <- spTransform(Subset2_1hour_move, CRSobj=azim_Bierr)
Subset2_3hr_move_azim <- spTransform(Subset2_3hr_move, CRSobj=azim_Bierr)
Subset2_1day_move_azim <- spTransform(Subset2_1day_move, CRSobj=azim_Bierr)

#calculate distance between consecutive GPS points (meters)
Subset1_1hour_move_azim$distance<-unlist(lapply(distance(Subset1_1hour_move_azim), c, NA))
Subset1_3hr_move_azim$distance<-unlist(lapply(distance(Subset1_3hr_move_azim), c, NA))
Subset1_1day_move_azim$distance<-unlist(lapply(distance(Subset1_1day_move_azim), c, NA))

Subset2_1hour_move_azim$distance<-unlist(lapply(distance(Subset2_1hour_move_azim), c, NA))
Subset2_3hr_move_azim$distance<-unlist(lapply(distance(Subset2_3hr_move_azim), c, NA))
Subset2_1day_move_azim$distance<-unlist(lapply(distance(Subset2_1day_move_azim), c, NA))

#calculate speed/velocity between consecutive GPS points (m/s)
Subset1_1hour_move_azim$speed<-unlist(lapply(speed(Subset1_1hour_move_azim), c, NA))
Subset1_3hr_move_azim$speed<-unlist(lapply(speed(Subset1_3hr_move_azim), c, NA))
Subset1_1day_move_azim$speed<-unlist(lapply(speed(Subset1_1day_move_azim), c, NA))

Subset2_1hour_move_azim$speed<-unlist(lapply(speed(Subset2_1hour_move_azim), c, NA))
Subset2_3hr_move_azim$speed<-unlist(lapply(speed(Subset2_3hr_move_azim), c, NA))
Subset2_1day_move_azim$speed<-unlist(lapply(speed(Subset2_1day_move_azim), c, NA))

#calculating lag between fixes.
Subset1_1hour_move_azim$lag<-unlist(lapply(timeLag(Subset1_1hour_move_azim,units = "hours"), c, NA))
Subset1_3hr_move_azim$lag<-unlist(lapply(timeLag(Subset1_3hr_move_azim,units = "hours"), c, NA))
Subset1_1day_move_azim$lag<-unlist(lapply(timeLag(Subset1_1day_move_azim,units = "hours"), c, NA))

Subset2_1hour_move_azim$lag<-unlist(lapply(timeLag(Subset2_1hour_move_azim,units = "hours"), c, NA))
Subset2_3hr_move_azim$lag<-unlist(lapply(timeLag(Subset2_3hr_move_azim,units = "hours"), c, NA))
Subset2_1day_move_azim$lag<-unlist(lapply(timeLag(Subset2_1day_move_azim,units = "hours"), c, NA))

#Project data with conformal projection: the Mercator projection is a conformal cylindrical projection
mercator = CRS("+init=epsg:4326") #this projection covers the world so no need to custom define projection attributes
Subset1_1hour_move_mercator <- spTransform(Subset1_1hour_move, CRSobj=mercator)
Subset1_3hr_move_mercator <- spTransform(Subset1_3hr_move, CRSobj=mercator)
Subset1_1day_move_mercator <- spTransform(Subset1_1day_move, CRSobj=mercator)

Subset2_1hour_move_mercator <- spTransform(Subset2_1hour_move, CRSobj=mercator)
Subset2_3hr_move_mercator <- spTransform(Subset2_3hr_move, CRSobj=mercator)
Subset2_1day_move_mercator <- spTransform(Subset2_1day_move, CRSobj=mercator)

#calculating absolute angle
Subset1_1hour_move_mercator$abs_angle<-unlist(lapply(angle(Subset1_1hour_move_mercator), c, NA))
Subset1_3hr_move_mercator$abs_angle<-unlist(lapply(angle(Subset1_3hr_move_mercator), c, NA))
Subset1_1day_move_mercator$abs_angle<-unlist(lapply(angle(Subset1_1day_move_mercator), c, NA))

Subset2_1hour_move_mercator$abs_angle<-unlist(lapply(angle(Subset2_1hour_move_mercator), c, NA))
Subset2_3hr_move_mercator$abs_angle<-unlist(lapply(angle(Subset2_3hr_move_mercator), c, NA))
Subset2_1day_move_mercator$abs_angle<-unlist(lapply(angle(Subset2_1day_move_mercator), c, NA))

#calculate Relative Angle
Subset1_1hour_move_mercator$rel_angle<-unlist(lapply(turnAngleGc(Subset1_1hour_move_mercator),function(x) c(NA, x, NA)))
Subset1_3hr_move_mercator$rel_angle<-unlist(lapply(turnAngleGc(Subset1_3hr_move_mercator),function(x) c(NA, x, NA)))
Subset1_1day_move_mercator$rel_angle<-unlist(lapply(turnAngleGc(Subset1_1day_move_mercator),function(x) c(NA, x, NA)))

Subset2_1hour_move_mercator$rel_angle<-unlist(lapply(turnAngleGc(Subset2_1hour_move_mercator),function(x) c(NA, x, NA)))
Subset2_3hr_move_mercator$rel_angle<-unlist(lapply(turnAngleGc(Subset2_3hr_move_mercator),function(x) c(NA, x, NA)))
Subset2_1day_move_mercator$rel_angle<-unlist(lapply(turnAngleGc(Subset2_1day_move_mercator),function(x) c(NA, x, NA)))
```

#### Calculate Characteristics with Base R and 'adehabitatLT'

>Next I will transform my move objects into data frames and extract the characteristics I calculated previously.

```{r}
#1-hour
Subset1_1hour_move_azim_DF<-as.data.frame(Subset1_1hour_move_azim)
Subset1_1hour_move_mercator_DF<-as.data.frame(Subset1_1hour_move_mercator)
Subset1_1hour_move_azim_DF$rel_angle<-Subset1_1hour_move_mercator_DF$rel_angle
Subset1_1hour_move_azim_DF$abs_angle<-Subset1_1hour_move_mercator_DF$abs_angle
Subset1_1hour_move_azim_DF$turn_speed<-Subset1_1hour_move_azim_DF$speed * cos(Subset1_1hour_move_azim_DF$rel_angle)

Subset2_1hour_move_azim_DF<-as.data.frame(Subset2_1hour_move_azim)
Subset2_1hour_move_mercator_DF<-as.data.frame(Subset2_1hour_move_mercator)
Subset2_1hour_move_azim_DF$rel_angle<-Subset2_1hour_move_mercator_DF$rel_angle
Subset2_1hour_move_azim_DF$abs_angle<-Subset2_1hour_move_mercator_DF$abs_angle
Subset2_1hour_move_azim_DF$turn_speed<-Subset2_1hour_move_azim_DF$speed * cos(Subset2_1hour_move_azim_DF$rel_angle)

#3-hour
Subset1_3hr_move_azim_DF<-as.data.frame(Subset1_3hr_move_azim)
Subset1_3hr_move_mercator_DF<-as.data.frame(Subset1_3hr_move_mercator)
Subset1_3hr_move_azim_DF$rel_angle<-Subset1_3hr_move_mercator_DF$rel_angle
Subset1_3hr_move_azim_DF$abs_angle<-Subset1_3hr_move_mercator_DF$abs_angle
Subset1_3hr_move_azim_DF$turn_speed<-Subset1_3hr_move_azim_DF$speed * cos(Subset1_3hr_move_azim_DF$rel_angle)

Subset2_3hr_move_azim_DF<-as.data.frame(Subset2_3hr_move_azim)
Subset2_3hr_move_mercator_DF<-as.data.frame(Subset2_3hr_move_mercator)
Subset2_3hr_move_azim_DF$rel_angle<-Subset2_3hr_move_mercator_DF$rel_angle
Subset2_3hr_move_azim_DF$abs_angle<-Subset2_3hr_move_mercator_DF$abs_angle
Subset2_3hr_move_azim_DF$turn_speed<-Subset2_3hr_move_azim_DF$speed * cos(Subset2_3hr_move_azim_DF$rel_angle)

#1-day
Subset1_1day_move_azim_DF<-as.data.frame(Subset1_1day_move_azim)
Subset1_1day_move_mercator_DF<-as.data.frame(Subset1_1day_move_mercator)
Subset1_1day_move_azim_DF$rel_angle<-Subset1_1day_move_mercator_DF$rel_angle
Subset1_1day_move_azim_DF$abs_angle<-Subset1_1day_move_mercator_DF$abs_angle
Subset1_1day_move_azim_DF$turn_speed<-Subset1_1day_move_azim_DF$speed * cos(Subset1_1day_move_azim_DF$rel_angle)

Subset2_1day_move_azim_DF<-as.data.frame(Subset2_1day_move_azim)
Subset2_1day_move_mercator_DF<-as.data.frame(Subset2_1day_move_mercator)
Subset2_1day_move_azim_DF$rel_angle<-Subset2_1day_move_mercator_DF$rel_angle
Subset2_1day_move_azim_DF$abs_angle<-Subset2_1day_move_mercator_DF$abs_angle
Subset2_1day_move_azim_DF$turn_speed<-Subset2_1day_move_azim_DF$speed * cos(Subset2_1day_move_azim_DF$rel_angle)
```

> Finally, I will transform my dataframe into an ltraj object with the azimuthal equidistant projection so I can calculate FPT

```{r}
#transform datasets into ltraj objects
Subset1_1hour_traj<-Subset1_1hour_move_azim_DF
coordinates(Subset1_1hour_traj) <- c("coords.x1", "coords.x2")
proj4string(Subset1_1hour_traj) <- azim_Bierr
Subset1_1hour_traj <- as.ltraj(coordinates(Subset1_1hour_traj),
                          date=Subset1_1hour_traj$timestamps,
                          id=Subset1_1hour_traj$trackId, typeII=TRUE)
Subset1_3hr_traj<-Subset1_3hr_move_azim_DF
coordinates(Subset1_3hr_traj) <- c("coords.x1", "coords.x2")
proj4string(Subset1_3hr_traj) <- azim_Bierr
Subset1_3hr_traj <- as.ltraj(coordinates(Subset1_3hr_traj),
                          date=Subset1_3hr_traj$timestamps,
                          id=Subset1_3hr_traj$trackId, typeII=TRUE)
Subset1_1day_traj<-Subset1_1day_move_azim_DF
coordinates(Subset1_1day_traj) <- c("coords.x1", "coords.x2")
proj4string(Subset1_1day_traj) <- azim_Bierr
Subset1_1day_traj <- as.ltraj(coordinates(Subset1_1day_traj),
                          date=Subset1_1day_traj$timestamps,
                          id=Subset1_1day_traj$trackId, typeII=TRUE)

Subset2_1hour_traj<-Subset2_1hour_move_azim_DF
coordinates(Subset2_1hour_traj) <- c("coords.x1", "coords.x2")
proj4string(Subset2_1hour_traj) <- azim_Bierr
Subset2_1hour_traj <- as.ltraj(coordinates(Subset2_1hour_traj),
                          date=Subset2_1hour_traj$timestamps,
                          id=Subset2_1hour_traj$trackId, typeII=TRUE)
Subset2_3hr_traj<-Subset2_3hr_move_azim_DF
coordinates(Subset2_3hr_traj) <- c("coords.x1", "coords.x2")
proj4string(Subset2_3hr_traj) <- azim_Bierr
Subset2_3hr_traj <- as.ltraj(coordinates(Subset2_3hr_traj),
                          date=Subset2_3hr_traj$timestamps,
                          id=Subset2_3hr_traj$trackId, typeII=TRUE)
Subset2_1day_traj<-Subset2_1day_move_azim_DF
coordinates(Subset2_1day_traj) <- c("coords.x1", "coords.x2")
proj4string(Subset2_1day_traj) <- azim_Bierr
Subset2_1day_traj <- as.ltraj(coordinates(Subset2_1day_traj),
                          date=Subset2_1day_traj$timestamps,
                          id=Subset2_1day_traj$trackId, typeII=TRUE)
```

>Determine spatial continuity

```{r}
#before I can calculate FPT I need to calculate the average maximum spatial continuity within all eight datasets. I want the spatial continuity to be the same for all datasets so I can compare FPT between individuals if I desire.
#1-hour fpt
fpt_max_subset1_1hour<-data.frame()
for (i in 1:62) {
  x<-fpt(Subset1_1hour_traj[i], radii=0:100, units="hours") #calculate fpt values
  y<- varlogfpt(x, graph=TRUE) #variogram of fpt
  z<-as.numeric(y[1,])
  fpt<-max(z, na.rm = TRUE) #maximum of the variogram
  fpt_max_subset1_1hour<-rbind(fpt_max_subset1_1hour, fpt)
}
mean(fpt_max_subset1_1hour[,1]) #11 km

fpt_max_subset1_3hour<-data.frame()
for (i in 1:62) {
  x<-fpt(Subset1_3hr_traj[i], radii=0:100, units="hours") 
  y<- varlogfpt(x, graph=TRUE)
  z<-as.numeric(y[1,])
  fpt<-max(z, na.rm = TRUE)
  fpt_max_subset1_3hour<-rbind(fpt_max_subset1_3hour, fpt)
}
mean(fpt_max_subset1_3hour[,1]) #10 km

fpt_max_subset1_1day<-data.frame()
for (i in 1:62) {
  x<-fpt(Subset1_1day_traj[i], radii=0:100, units="hours")
  y<- varlogfpt(x, graph=TRUE)
  z<-as.numeric(y[1,])
  fpt<-max(z, na.rm = TRUE)
  fpt_max_subset1_1day<-rbind(fpt_max_subset1_1day, fpt)
}
mean(fpt_max_subset1_1day[,1]) #5 km

fpt_max_subset2_1hour<-data.frame()
for (i in 1:198) {
  x<-fpt(Subset2_1hour_traj[i], radii=0:100, units="hours") #calculate fpt
  y<- varlogfpt(x, graph=TRUE) #variogram of fpt
  z<-as.numeric(y[1,])
  fpt<-max(z, na.rm = TRUE) #maximum of the variogram
  fpt_max_subset2_1hour<-rbind(fpt_max_subset2_1hour, fpt)
}
mean(fpt_max_subset2_1hour[,1]) #11 km

fpt_max_subset2_3hour<-data.frame()
for (i in 1:198) {
  x<-fpt(Subset2_3hr_traj[i], radii=0:100, units="hours") #calculate fpt
  y<- varlogfpt(x, graph=TRUE) #variogram of fpt
  z<-as.numeric(y[1,])
  fpt<-max(z, na.rm = TRUE) #maximum of the variogram
  fpt_max_subset2_3hour<-rbind(fpt_max_subset2_3hour, fpt)
}
mean(fpt_max_subset2_3hour[,1]) #10 km

fpt_max_subset2_1day<-data.frame()
for (i in 1:198) {
  x<-fpt(Subset2_1day_traj[i], radii=0:100, units="hours") #calculate fpt
  y<- varlogfpt(x, graph=TRUE) #variogram of fpt
  z<-as.numeric(y[1,])
  fpt<-max(z, na.rm = TRUE) #maximum of the variogram
  fpt_max_subset2_1day<-rbind(fpt_max_subset2_1day, fpt)
}
mean(fpt_max_subset2_1day[,1]) #3 km
```

>Calculate FPT

```{r}
#calculate FPT
#1-hour fpt
fpt_subset1_1hour<-data.frame()
for (i in 1:63) {
  x<-fpt(Subset1_1hour_traj[i], radii=0:100, units="hours")
  y<-x[[1]]$r11
  r<-as.data.frame(y)
  id<-id(Subset1_1hour_traj[i])
  r$migrationEvent<-paste(id)
  fpt_subset1_1hour<-rbind(fpt_subset1_1hour, r)
}
colnames(fpt_subset1_1hour)<- c("FPT", "migrationEvent")
Subset1_1hour_fin<-cbind(Subset1_1hour_move_azim_DF, fpt_subset1_1hour)

fpt_subset2_1hour<-data.frame()
for (i in 1:198) {
  x<-fpt(Subset2_1hour_traj[i], radii=0:100, units="hours")
  y<-x[[1]]$r11
  r<-as.data.frame(y)
  id<-id(Subset2_1hour_traj[i])
  r$migrationEvent<-paste(id)
  fpt_subset2_1hour<-rbind(fpt_subset2_1hour, r)
}
colnames(fpt_subset2_1hour)<- c("FPT", "migrationEvent")
Subset2_1hour_fin<-cbind(Subset2_1hour_move_azim_DF, fpt_subset2_1hour)

#3-hour fpt
fpt_subset1_3hr<-data.frame()
for (i in 1:63) {
  x<-fpt(Subset1_3hr_traj[i], radii=0:100, units="hours")
  y<-x[[1]]$r11
  r<-as.data.frame(y)
  id<-id(Subset1_3hr_traj[i])
  r$migrationEvent<-paste(id)
  fpt_subset1_3hr<-rbind(fpt_subset1_3hr, r)
}
colnames(fpt_subset1_3hr)<- c("FPT", "migrationEvent")
Subset1_3hr_fin<-cbind(Subset1_3hr_move_azim_DF, fpt_subset1_3hr)

fpt_subset2_3hr<-data.frame()
for (i in 1:198) {
  x<-fpt(Subset2_3hr_traj[i], radii=0:100, units="hours")
  y<-x[[1]]$r11
  r<-as.data.frame(y)
  id<-id(Subset2_3hr_traj[i])
  r$migrationEvent<-paste(id)
  fpt_subset2_3hr<-rbind(fpt_subset2_3hr, r)
}
colnames(fpt_subset2_3hr)<- c("FPT", "migrationEvent")
Subset2_3hr_fin<-cbind(Subset2_3hr_move_azim_DF, fpt_subset2_3hr)

#1-day
fpt_subset1_1day<-data.frame()
for (i in 1:63) {
  x<-fpt(Subset1_1day_traj[i], radii=0:100, units="hours")
  y<-x[[1]]$r11
  r<-as.data.frame(y)
  id<-id(Subset1_1day_traj[i])
  r$migrationEvent<-paste(id)
  fpt_subset1_1day<-rbind(fpt_subset1_1day, r)
}
colnames(fpt_subset1_1day)<- c("FPT", "migrationEvent")
Subset1_1day_fin<-cbind(Subset1_1day_move_azim_DF, fpt_subset1_1day)

fpt_subset2_1day<-data.frame()
for (i in 1:198) {
  x<-fpt(Subset2_1day_traj[i], radii=0:100, units="hours")
  y<-x[[1]]$r11
  r<-as.data.frame(y)
  id<-id(Subset2_1day_traj[i])
  r$migrationEvent<-paste(id)
  fpt_subset2_1day<-rbind(fpt_subset2_1day, r)
}
colnames(fpt_subset2_1day)<- c("FPT", "migrationEvent")
Subset2_1day_fin<-cbind(Subset2_1day_move_azim_DF, fpt_subset2_1day)
```

> I am left with the following datasets:
* Subset1_1hour_fin
* Subset1_3hr_fin
* Subset1_1day_fin
* Subset1_3day_fin
* Subset2_1hour_fin
* Subset2_3hr_fin
* Subset2_1day_fin
* Subset2_3day_fin

#### Remove Nightime Fixes and prepare data for analysis

> Lastly, I will remove fixes calculated during hours of darkness where the bird was travelling overland. while osprey may migrate at night on occasion, this typically only occurs over water or deserts. The osprey in this study do not encounter deserts, so I will remove any points during darkness that intersect with the World country polygons. 

```{r}
#calculate daylight. I used a 30 minute daylight buffer to account for possible movement at dusk. 
Subset1_1hour_fin$DaylightUTC<-computeIsDayByLocation(Subset1_1hour_fin$timestamps, 
                                                      latDeg = Subset1_1hour_fin$y, 
                                                      longDeg = Subset1_1hour_fin$x, 
                                                      timeZone = getHoursAheadOfUTC(Subset1_1hour_fin$timestamps), 
                                                      duskOffset = 0.5) 
Subset1_3hr_fin$DaylightUTC<-computeIsDayByLocation(Subset1_3hr_fin$timestamps, 
                                                      latDeg = Subset1_3hr_fin$y, 
                                                      longDeg = Subset1_3hr_fin$x, 
                                                      timeZone = getHoursAheadOfUTC(Subset1_3hr_fin$timestamps), 
                                                      duskOffset = 0.5)

Subset2_1hour_fin$DaylightUTC<-computeIsDayByLocation(Subset2_1hour_fin$timestamps, 
                                                      latDeg = Subset2_1hour_fin$y, 
                                                      longDeg = Subset2_1hour_fin$x, 
                                                      timeZone = getHoursAheadOfUTC(Subset2_1hour_fin$timestamps), 
                                                      duskOffset = 0.5) 
Subset2_3hr_fin$DaylightUTC<-computeIsDayByLocation(Subset2_3hr_fin$timestamps, 
                                                      latDeg = Subset2_3hr_fin$y, 
                                                      longDeg = Subset2_3hr_fin$x, 
                                                      timeZone = getHoursAheadOfUTC(Subset2_3hr_fin$timestamps), 
                                                      duskOffset = 0.5) 

#create sf objects. 
Subset1_1hour_fin_sf<-st_as_sf(x=Subset1_1hour_fin, coords = c("x", "y"), crs = wgs84)
Subset1_1hour_fin_sf<-st_transform(Subset1_1hour_fin_sf, azim_Bierr)
Subset1_3hr_fin_sf<-st_as_sf(x=Subset1_3hr_fin, coords = c("x", "y"), crs = wgs84)
Subset1_3hr_fin_sf<-st_transform(Subset1_3hr_fin_sf, azim_Bierr)

Subset2_1hour_fin_sf<-st_as_sf(x=Subset2_1hour_fin, coords = c("x", "y"), crs = wgs84)
Subset2_1hour_fin_sf<-st_transform(Subset2_1hour_fin_sf, azim_Bierr)
Subset2_3hr_fin_sf<-st_as_sf(x=Subset2_3hr_fin, coords = c("x", "y"), crs = wgs84)
Subset2_3hr_fin_sf<-st_transform(Subset2_3hr_fin_sf, azim_Bierr)

#filter out points during dark hours where birds where not over the ocean. 
Subset1_1hour_fin_sf_filter<-st_join(Subset1_1hour_fin_sf, world_filter, join = st_within)
Subset1_1hour_fin_sf_filter$name[is.na(Subset1_1hour_fin_sf_filter$name)] <- "Ocean" #need to replace NA values with a character for filtering
Subset1_1hour_fin_sf_filter$DaylightUTC<-as.character(Subset1_1hour_fin_sf_filter$DaylightUTC)
Subset1_1hour_fin_sf_filter<-Subset1_1hour_fin_sf_filter[!(Subset1_1hour_fin_sf_filter$name != "Ocean" & 
                              Subset1_1hour_fin_sf_filter$DaylightUTC == "FALSE"),]
Subset1_1hour_fin_filter<-st_drop_geometry(Subset1_1hour_fin_sf_filter)

Subset1_3hr_fin_sf_filter<-st_join(Subset1_3hr_fin_sf, world_filter, join = st_within)
Subset1_3hr_fin_sf_filter$name[is.na(Subset1_3hr_fin_sf_filter$name)] <- "Ocean" 
Subset1_3hr_fin_sf_filter$DaylightUTC<-as.character(Subset1_3hr_fin_sf_filter$DaylightUTC)
Subset1_3hr_fin_sf_filter<-Subset1_3hr_fin_sf_filter[!(Subset1_3hr_fin_sf_filter$name != "Ocean" & 
                              Subset1_3hr_fin_sf_filter$DaylightUTC == "FALSE"),]
Subset1_3hr_fin_filter<-st_drop_geometry(Subset1_3hr_fin_sf_filter)

Subset2_1hour_fin_sf_filter<-st_join(Subset2_1hour_fin_sf, world_filter, join = st_within)
Subset2_1hour_fin_sf_filter$name[is.na(Subset2_1hour_fin_sf_filter$name)] <- "Ocean"
Subset2_1hour_fin_sf_filter$DaylightUTC<-as.character(Subset2_1hour_fin_sf_filter$DaylightUTC)
Subset2_1hour_fin_sf_filter<-Subset2_1hour_fin_sf_filter[!(Subset2_1hour_fin_sf_filter$name != "Ocean" & 
                              Subset2_1hour_fin_sf_filter$DaylightUTC == "FALSE"),]
Subset2_1hour_fin_filter<-st_drop_geometry(Subset2_1hour_fin_sf_filter)

Subset2_3hr_fin_sf_filter<-st_join(Subset2_3hr_fin_sf, world_filter, join = st_within)
Subset2_3hr_fin_sf_filter$name[is.na(Subset2_3hr_fin_sf_filter$name)] <- "Ocean" 
Subset2_3hr_fin_sf_filter$DaylightUTC<-as.character(Subset2_3hr_fin_sf_filter$DaylightUTC)
Subset2_3hr_fin_sf_filter<-Subset2_3hr_fin_sf_filter[!(Subset2_3hr_fin_sf_filter$name != "Ocean" & 
                              Subset2_3hr_fin_sf_filter$DaylightUTC == "FALSE"),]
Subset2_3hr_fin_filter<-st_drop_geometry(Subset2_3hr_fin_sf_filter)
```

> Remove NA values and clean up data columns

```{r}
#Remove NA values
Subset1_1hour_fin_filter<-na.omit(Subset1_1hour_fin_filter[,c("time", "distance", "speed", "lag","migrationEvent", "rel_angle", "abs_angle", "turn_speed", "FPT" )])
Subset1_3hr_fin_filter<-na.omit(Subset1_3hr_fin_filter[,c("time", "distance", "speed", "lag","migrationEvent", "rel_angle", "abs_angle", "turn_speed", "FPT" )])
Subset1_1day_fin<-na.omit(Subset1_1day_fin[,c("time", "distance", "speed", "lag","migrationEvent", "rel_angle", "abs_angle", "turn_speed", "FPT" )])

Subset2_1hour_fin_filter<-na.omit(Subset2_1hour_fin_filter[,c("time", "distance", "speed", "lag","migrationEvent", "rel_angle", "abs_angle", "turn_speed", "FPT" )])
Subset2_3hr_fin_filter<-na.omit(Subset2_3hr_fin_filter[,c("time", "distance", "speed", "lag","migrationEvent", "rel_angle", "abs_angle", "turn_speed", "FPT" )])
Subset2_1day_fin<-na.omit(Subset2_1day_fin[,c("time", "distance", "speed", "lag","migrationEvent", "rel_angle", "abs_angle", "turn_speed", "FPT" )])

#Add in Individual IDs.
Subset1_1hour_fin_filter<-Subset1_1hour_fin_filter %>% 
    separate(migrationEvent,c("ID", "Year"), sep = "_(?=[^_]+$)", remove = FALSE)
Subset1_3hr_fin_filter<-Subset1_3hr_fin_filter %>% 
    separate(migrationEvent,c("ID", "Year"), sep = "_(?=[^_]+$)", remove = FALSE)
Subset1_1day_fin<-Subset1_1day_fin %>% 
    separate(migrationEvent,c("ID", "Year"), sep = "_(?=[^_]+$)", remove = FALSE)

Subset2_1hour_fin_filter<-Subset2_1hour_fin_filter %>% 
    separate(migrationEvent,c("ID", "Region"), sep = "_(?=[^_]+$)", remove = FALSE)
Subset2_1hour_fin_filter$Year<-format(as.Date(Subset2_1hour_fin_filter$time, format="%d/%m/%Y"),"%Y")
Subset2_1hour_fin_filter<-merge(Subset2_1hour_fin_filter, IndividualMetadata, by.x = "migrationEvent", by.y = "SegmentID")
Subset2_1hour_fin_filter<-Subset2_1hour_fin_filter[,c(1:5,7:13)]

Subset2_3hr_fin_filter<-Subset2_3hr_fin_filter %>% 
    separate(migrationEvent,c("ID", "Region"), sep = "_(?=[^_]+$)", remove = FALSE)
Subset2_3hr_fin_filter$Year<-format(as.Date(Subset2_3hr_fin_filter$time, format="%d/%m/%Y"),"%Y")
Subset2_3hr_fin_filter<-merge(Subset2_3hr_fin_filter, IndividualMetadata, by.x = "migrationEvent", by.y = "SegmentID")
Subset2_3hr_fin_filter<-Subset2_3hr_fin_filter[,c(1:5,7:13)]

Subset2_1day_fin<-Subset2_1day_fin %>% 
    separate(migrationEvent,c("ID", "Region"), sep = "_(?=[^_]+$)", remove = FALSE)
Subset2_1day_fin$Year<-format(as.Date(Subset2_1day_fin$time, format="%d/%m/%Y"),"%Y")
Subset2_1day_fin<-merge(Subset2_1day_fin, IndividualMetadata, by.x = "migrationEvent", by.y = "SegmentID")
Subset2_1day_fin<-Subset2_1day_fin[,c(1:5,7:13)]

Subset11hour<-data.frame(Subset1_1hour_fin_filter$ID, Subset1_1hour_fin_filter$migrationEvent, Subset1_1hour_fin_filter$speed, Subset1_1hour_fin_filter$abs_angle, Subset1_1hour_fin_filter$turn_speed, Subset1_1hour_fin_filter$FPT)
colnames(Subset11hour)<- c("ID", "migrationEvent", "speed", "abs_angle", "turn_speed", "FPT")

Subset13hour<-data.frame(Subset1_3hr_fin_filter$ID, Subset1_3hr_fin_filter$migrationEvent, Subset1_3hr_fin_filter$speed, Subset1_3hr_fin_filter$abs_angle, Subset1_3hr_fin_filter$turn_speed, Subset1_3hr_fin_filter$FPT)
colnames(Subset13hour)<- c("ID", "migrationEvent", "speed", "abs_angle", "turn_speed", "FPT")

Subset11day<-data.frame(Subset1_1day_fin$ID, Subset1_1day_fin$migrationEvent, Subset1_1day_fin$speed, Subset1_1day_fin$abs_angle, Subset1_1day_fin$turn_speed, Subset1_1day_fin$FPT)
colnames(Subset11day)<- c("ID", "migrationEvent", "speed", "abs_angle", "turn_speed", "FPT")

#Take the absolute value of absolute angle
Subset11hour$abs_angle<-abs(Subset11hour$abs_angle)
Subset13hour$abs_angle<-abs(Subset13hour$abs_angle)
Subset11day$abs_angle<-abs(Subset11day$abs_angle)

Subset2_1hour_fin_filter$abs_angle<-abs(Subset2_1hour_fin_filter$abs_angle)
Subset2_3hr_fin_filter$abs_angle<-abs(Subset2_3hr_fin_filter$abs_angle)
Subset2_1day_fin$abs_angle<-abs(Subset2_1day_fin$abs_angle)
```

> Take random samples

```{r}
set.seed(1998)

min_Subset11hour<-Subset11hour %>%
  group_by(migrationEvent) %>%
  summarise(N = n()) %>%
  as.data.frame()
min(min_Subset11hour$N) #69 random samples needed
Samp_Subset11hour <- Subset11hour %>% 
  group_by(migrationEvent) %>% 
  sample_n(69) 

min_Subset13hour<-Subset13hour %>%
  group_by(migrationEvent) %>%
  summarise(N = n()) %>%
  as.data.frame()
min(min_Subset13hour$N) #21 random samples
Samp_Subset13hour <- Subset13hour %>% 
  group_by(migrationEvent) %>% 
  sample_n(21) 

min_Subset11day<-Subset11day %>%
  group_by(migrationEvent) %>%
  summarise(N = n()) %>%
  as.data.frame()
min(min_Subset11day$N) #4 random samples
Samp_Subset11day <- Subset11day %>% 
  group_by(migrationEvent) %>% 
  sample_n(4) 

#take random samples from subset 2. Start by taking randam samples for each migration segment so that each segment has equal chance of being represented when we randomly sample for regions. 
min_Subset21hour<-Subset2_1hour_fin_filter %>%
  group_by(migrationEvent) %>%
  summarise(N = n()) %>%
  as.data.frame()
min(min_Subset21hour$N) #36 random samples
Samp_Subset21hour <- Subset2_1hour_fin_filter %>% 
  group_by(migrationEvent) %>% 
  sample_n(36) 

min_Subset23hour<-Subset2_3hr_fin_filter %>%
  group_by(migrationEvent) %>%
  summarise(N = n()) %>%
  as.data.frame()
min(min_Subset23hour$N) #11 random samples
Samp_Subset23hour <- Subset2_3hr_fin_filter %>% 
  group_by(migrationEvent) %>% 
  sample_n(11) 

min_Subset21day<-Subset2_1day_fin %>%
  group_by(migrationEvent) %>%
  summarise(N = n()) %>%
  as.data.frame()
min(min_Subset21day$N) #2 random samples
Samp_Subset21day <- Subset2_1day_fin %>%
  group_by(migrationEvent) %>% 
  sample_n(2) 

#Now take random samples for each region so that we have the same number of samples for each region.
table(Samp_Subset21hour$Region) #1692, SCA has the lowest count. 
Samp_Subset21hour <- Samp_Subset21hour %>% 
  group_by(Region) %>% 
  sample_n(1692)

table(Samp_Subset23hour$Region) # 517 
Samp_Subset23hour <- Samp_Subset23hour %>% 
  group_by(Region) %>% 
  sample_n(517) 

table(Samp_Subset21day$Region)
Samp_Subset21day <- Samp_Subset21day %>% 
  group_by(Region) %>% 
  sample_n(94) 
```


## Data Analysis of Subset 1

#### Exploratory data analysis

> Scale/normalize data for PERMANOVA. I scaled data since distance matrixs are heavily influenced by variable measurement sizes. Measurements at a larger scale (such as between 50 to 200) will overshadow measurements at a lower scale (such as 0 - 1). 

```{r}
#subset 1
Norm_1hour<- as.data.frame(scale(Samp_Subset11hour[3:6]))
Norm_1hour<-cbind(Norm_1hour, Samp_Subset11hour$ID, Samp_Subset11hour$migrationEvent)
colnames(Norm_1hour)<-c("speed", "abs_angle", "turn_speed", "FPT", "ID", "migrationEvent")

Norm_3hour<- as.data.frame(scale(Samp_Subset13hour[3:6]))
Norm_3hour<-cbind(Norm_3hour, Samp_Subset13hour$ID, Samp_Subset13hour$migrationEvent)
colnames(Norm_3hour)<-c("speed", "abs_angle", "turn_speed", "FPT", "ID", "migrationEvent")

Norm_1day<- as.data.frame(scale(Samp_Subset11day[3:6]))
Norm_1day<-cbind(Norm_1day, Samp_Subset11day$ID, Samp_Subset11day$migrationEvent)
colnames(Norm_1day)<-c("speed", "abs_angle", "turn_speed", "FPT", "ID", "migrationEvent")

#Subset 2
Norm2_1hour<- as.data.frame(scale(Samp_Subset21hour[c(4, 8:10)]))
Norm2_1hour<-cbind(Norm2_1hour, Samp_Subset21hour$Indvidual, Samp_Subset21hour$migrationEvent, Samp_Subset21hour$Region)
colnames(Norm2_1hour)<-c("speed", "abs_angle", "turn_speed", "FPT", "ID", "migrationEvent", "Region")
Norm2_1hour$migrationEvent<-str_sub(Norm2_1hour$migrationEvent, 0, -5)

Norm2_3hour<- as.data.frame(scale(Samp_Subset23hour[c(4, 8:10)]))
Norm2_3hour<-cbind(Norm2_3hour, Samp_Subset23hour$Indvidual, Samp_Subset23hour$migrationEvent, Samp_Subset23hour$Region)
colnames(Norm2_3hour)<-c("speed", "abs_angle", "turn_speed", "FPT", "ID", "migrationEvent", "Region")
Norm2_3hour$migrationEvent<-str_sub(Norm2_3hour$migrationEvent, 0, -5)

Norm2_1day<- as.data.frame(scale(Samp_Subset21day[c(4, 8:10)]))
Norm2_1day<-cbind(Norm2_1day, Samp_Subset21day$Indvidual, Samp_Subset21day$migrationEvent, Samp_Subset21day$Region)
colnames(Norm2_1day)<-c("speed", "abs_angle", "turn_speed", "FPT", "ID", "migrationEvent", "Region")
Norm2_1day$migrationEvent<-str_sub(Norm2_1day$migrationEvent, 0, -5)
```

> First I will look at the distribution of variables/characteristics for each individual for each sampling frequency using Q-Q plots. The distribution of my data for each individual will determine the kinds of tests I employ later on. 

```{r}
#1-hour
#Histograms...I'm not putting the entire code for all histograms. I ran histograms on normalized data
hist(Norm_1hour$speed)
hist(Norm_1hour$abs_angle)
hist(Norm_1hour$turn_speed)
hist(Norm_1hour$FPT)

hist(Norm2_1hour$speed)
hist(Norm2_1hour$abs_angle)
hist(Norm2_1hour$turn_speed)
hist(Norm2_1hour$FPT)

#More Q-Q plots...Again not putting all this code in cause running all this code may crash R and take up way to much of your time. When you look at the distribution of dataset 2, change  migrationEvent to Region.

migrationEvents<-unique(Norm_1hour$migrationEvent)
for (i in migrationEvents) {
  x<-filter(Norm_1hour, migrationEvent == i)
  y<-x[,c(1:4)]
  for (j in 1:ncol(y)) {
    qqPlot(y[, j], main = names(y)[j])
  }
}
mshapiro<-data.frame()
for (i in migrationEvents) {
  x<-filter(Norm_1hour, migrationEvent == i)
  y<-mshapiro_test(x[1:4])
  mshapiro<-rbind(mshapiro, y)
} #very not normal...


#3-hour
hist(Norm_3hour$speed)
hist(Norm_3hour$abs_angle)
hist(Norm_3hour$turn_speed)
hist(Norm_3hour$FPT)
for (i in migrationEvents) {
  x<-filter(Norm_3hour, migrationEvent == i)
  y<-x[,c(1:4)]
  for (j in 1:ncol(y)) {
    qqPlot(y[, j], main = names(y)[j])
  }
}
Norm_3hour %>%
  group_by(migrationEvent) %>%
  shapiro_test(speed, abs_angle, turn_speed, FPT) %>%
  arrange(variable) #mostly non-normally distributed
mshapiro2<-data.frame()
for (i in migrationEvents) {
  x<-filter(Norm_3hour, migrationEvent == i)
  y<-mshapiro_test(x[1:4])
  mshapiro2<-rbind(mshapiro2, y)
}#still very not normal...

#1-day
hist(Norm_1day$speed)
hist(Norm_1day$abs_angle)
hist(Norm_1day$turn_speed)
hist(Norm_1day$FPT)
for (i in migrationEvents) {
  x<-filter(Norm_1day, migrationEvent == i)
  y<-x[,c(1:4)]
  for (j in 1:ncol(y)) {
    qqPlot(y[, j], main = names(y)[j])
  }
}
Norm_1day %>%
  group_by(migrationEvent) %>%
  shapiro_test(speed, abs_angle, turn_speed, FPT) %>%
  arrange(variable) #a lot of variables are normal...but like sample size of 4...
mshapiro3<-data.frame()
#can't use mshapiro cause I don't have enough data...
```

>PCA - subset 1

```{r}
#1-hour
colors<-colorRampPalette(c("pink", "red", "orange", "green","light blue", "blue", "purple", "tan", "grey50"))(63)

EventsPCA_1hour<-PCA(Norm_1hour[,1:4], scale = F, graph = F)
EventsPCA_1hour$var
fviz_screeplot(EventsPCA_1hour, addlabels = TRUE, title = "1-hour") +
  theme(legend.position = "none", 
        axis.title.x = element_text(margin = margin(t = 30), size = 15),
        axis.title.y = element_text(margin = margin(r = 30), size = 15),
        axis.text = element_text(size = 15))
#Loadings plot, size = 938 x 697
fviz_pca_var(EventsPCA_1hour,
             col.var =  "cos2", arrowsize = 1.5, repel = T,
             title = " ") +
  theme(legend.position = "none", 
        axis.title.x = element_text(margin = margin(t = 30), size = 15),
        axis.title.y = element_text(margin = margin(r = 30), size = 15),
        axis.text = element_text(size = 15)) +
  scale_color_gradient2(low = "red", mid = "blue", high = "green", 
                        midpoint = 0.5, limits = c(0,1)) +
  xlim(c(-1,1)) +
  ylim(c(-1,1))
#Contribution of variables to each PC
contrib_1hour<-get_pca_var(EventsPCA_1hour)
colnames(contrib_1hour$contrib)<- c("PC1", "PC2", "PC3", "PC4")
rownames(contrib_1hour$contrib)<- c("Speed", "Abs. Angle", "Turn Speed", "FPT")
corrplot(contrib_1hour$contrib, method = 'color', 
         is.corr = F,
         addgrid.col = "grey",
         col = COL1('YlGn'),
         addCoef.col = 'black',
         tl.col = "black",
         tl.srt = 0,
         tl.offset = 0.5,
         mar = c(1.5, 1.5, 1.5, 1.5),
         cl.pos = 'n',
         cl.length = 5,
         col.lim=c(0,100),
         tl.cex = 3,
         number.cex = 3,
         number.digits = 0
         )
#Score Plot
fviz_pca_ind(EventsPCA_1hour,
             geom.ind = "none",
             palette = colors,
             col.ind = as.factor(Norm_1hour$migrationEvent),
             addEllipses = T, 
             mean.point = T,
             ellipse.alpha = 0,
             mean.point.size = 2,
             scaleshape = "none",
             title = " "
             ) +
  theme(legend.position = "none", 
        axis.title.x = element_text(margin = margin(t = 30), size = 15),
        axis.title.y = element_text(margin = margin(r = 30), size = 15),
        axis.text = element_text(size = 15)) +
  scale_shape_identity() +
  scale_shape_manual(values = c(0,0,1,1,1,1,1,1,2,2,3,3,4,4,5,5,5,6,6,6,8,8,9,9,9,9, 10,10,11,11,11,11,12,12,12,12,12,13,13,14,14,14,15,15,16,16,17,17,17,17,64,64,64,64,40,40,40,33,33,35,35,38,38))
#Just the means
fviz_pca_ind(EventsPCA_1hour,
             geom.ind = "none",
             palette = colors,
             col.ind = as.factor(Norm_1hour$migrationEvent),
             addEllipses = F, 
             mean.point = T,
             ellipse.alpha = 0,
             mean.point.size = 4,
             scaleshape = "none",
             title = " "
             ) +
  theme(legend.position = "none", 
        axis.title.x = element_text(margin = margin(t = 30), size = 15),
        axis.title.y = element_text(margin = margin(r = 30), size = 15),
        axis.text = element_text(size = 15)) +
  scale_shape_identity() +
  scale_shape_manual(values = c(0,0,1,1,1,1,1,1,2,2,3,3,4,4,5,5,5,6,6,6,8,8,9,9,9,9, 10,10,11,11,11,11,12,12,12,12,12,13,13,14,14,14,15,15,16,16,17,17,17,17,64,64,64,64,40,40,40,33,33,35,35,38,38))
```

```{r}
EventsPCA_3hour<-PCA(Norm_3hour[,1:4], scale = F, graph = F)
EventsPCA_3hour$var
fviz_screeplot(EventsPCA_3hour, addlabels = TRUE, title = "3-hour") +
  theme(legend.position = "none", 
        axis.title.x = element_text(margin = margin(t = 30), size = 15),
        axis.title.y = element_text(margin = margin(r = 30), size = 15),
        axis.text = element_text(size = 15))
#Loadings plot
fviz_pca_var(EventsPCA_3hour, 
             col.var =  "cos2", arrowsize = 1.5, repel = T,
             title = " ") +
  theme(legend.position = "none", 
        axis.title.x = element_text(margin = margin(t = 30), size = 15),
        axis.title.y = element_text(margin = margin(r = 30), size = 15),
        axis.text = element_text(size = 15)) +
  scale_color_gradient2(low = "red", mid = "blue", high = "green", 
                        midpoint = 0.5, limits = c(0,1))+
  xlim(c(-1,1)) +
  ylim(c(-1,1))
#Contribution of variables to each PC
contrib_3hour<-get_pca_var(EventsPCA_3hour)
colnames(contrib_3hour$contrib)<- c("PC1", "PC2", "PC3", "PC4")
rownames(contrib_3hour$contrib)<- c("Speed", "Abs. Angle", "Turn Speed", "FPT")
corrplot(contrib_3hour$contrib, method = 'color', 
         is.corr = F,
         addgrid.col = "grey",
         col = COL1('YlGn'),
         addCoef.col = 'black',
         tl.col = "black",
         tl.srt = 0,
         tl.offset = 0.5,
         mar = c(1.5, 1.5, 1.5, 1.5),
         cl.pos = 'n',
         cl.length = 5,
         col.lim=c(0,100),
         tl.cex = 3,
         number.cex = 3,
         number.digits = 0
         )
#Score Plot
fviz_pca_ind(EventsPCA_3hour,
             geom.ind = "none",
             palette = colors,
             col.ind = as.factor(Norm_3hour$migrationEvent),
             addEllipses = TRUE, 
             mean.point = T,
             ellipse.alpha = 0,
             mean.point.size = 2,
             scaleshape = "none",
             title = " "
             ) +
  theme(legend.position = "none", 
        axis.title.x = element_text(margin = margin(t = 30), size = 15),
        axis.title.y = element_text(margin = margin(r = 30), size = 15),
        axis.text = element_text(size = 15)) +
  scale_shape_identity() +
  scale_shape_manual(values = c(0,0,1,1,1,1,1,1,2,2,3,3,4,4,5,5,5,6,6,6,8,8,9,9,9,9, 10,10,11,11,11,11,12,12,12,12,12,13,13,14,14,14,15,15,16,16,17,17,17,17,64,64,64,64,40,40,40,33,33,35,35,38,38))
```

```{r}
#1-day
EventsPCA_1day<-PCA(Norm_1day[,1:4], scale = F, graph = F)
EventsPCA_1day$var
fviz_screeplot(EventsPCA_1hour, addlabels = TRUE, title = "1-day") +
  theme(legend.position = "none", 
        axis.title.x = element_text(margin = margin(t = 30), size = 15),
        axis.title.y = element_text(margin = margin(r = 30), size = 15),
        axis.text = element_text(size = 15))
#Loadings plot
fviz_pca_var(EventsPCA_1day,
             col.var =  "cos2", arrowsize = 1.5, repel = T,
             title = " ") +
  theme(legend.position = "none", 
        axis.title.x = element_text(margin = margin(t = 30), size = 15),
        axis.title.y = element_text(margin = margin(r = 30), size = 15),
        axis.text = element_text(size = 15)) +
  scale_color_gradient2(low = "red", mid = "blue", high = "green", 
                        midpoint = 0.5, limits = c(0,1))+
  xlim(c(-1,1)) +
  ylim(c(-1,1))
#Contribution of variables to each PC
contrib_1day<-get_pca_var(EventsPCA_1day)
colnames(contrib_1day$contrib)<- c("PC1", "PC2", "PC3", "PC4")
rownames(contrib_1day$contrib)<- c("Speed", "Abs. Angle", "Turn Speed", "FPT")
corrplot(contrib_1day$contrib, method = 'color', 
         is.corr = F,
         addgrid.col = "grey",
         col = COL1('YlGn'),
         addCoef.col = 'black',
         tl.col = "black",
         tl.srt = 0,
         tl.offset = 0.5,
         mar = c(1.5, 1.5, 1.5, 1.5),
         cl.pos = 'n',
         cl.length = 5,
         col.lim=c(0,100),
         tl.cex = 3,
         number.cex = 3,
         number.digits = 0
         )
#Score Plot
fviz_pca_ind(EventsPCA_1day,
             geom.ind = "none",
             palette = colors,
             col.ind = as.factor(Norm_1day$migrationEvent),
             addEllipses = TRUE, 
             mean.point = T,
             ellipse.alpha = 0,
             mean.point.size = 2,
             scaleshape = "none",
             title = " "
             ) +
  theme(legend.position = "none", 
        axis.title.x = element_text(margin = margin(t = 30), size = 15),
        axis.title.y = element_text(margin = margin(r = 30), size = 15),
        axis.text = element_text(size = 15)) +
  scale_shape_identity() +
  scale_shape_manual(values = c(0,0,1,1,1,1,1,1,2,2,3,3,4,4,5,5,5,6,6,6,8,8,9,9,9,9, 10,10,11,11,11,11,12,12,12,12,12,13,13,14,14,14,15,15,16,16,17,17,17,17,64,64,64,64,40,40,40,33,33,35,35,38,38))
```

>PCA - subset 2

```{r}
#1-hour
EventsPCA2_1hour<-PCA(Norm2_1hour[,1:4], scale = F, graph = F)
EventsPCA2_1hour$var
fviz_screeplot(EventsPCA2_1hour, addlabels = TRUE, title = "1-hour") +
  theme(legend.position = "none", 
        axis.title.x = element_text(margin = margin(t = 30), size = 15),
        axis.title.y = element_text(margin = margin(r = 30), size = 15),
        axis.text = element_text(size = 15))
#Loadings plot
fviz_pca_var(EventsPCA2_1hour,
             col.var =  "cos2", arrowsize = 1.5, repel = T,
             title = " ") +
  theme(legend.position = "none", 
        axis.title.x = element_text(margin = margin(t = 30), size = 15),
        axis.title.y = element_text(margin = margin(r = 30), size = 15),
        axis.text = element_text(size = 15)) +
  scale_color_gradient2(low = "red", mid = "blue", high = "green", 
                        midpoint = 0.5, limits = c(0,1))+
  xlim(c(-1,1)) +
  ylim(c(-1,1))
#Contribution of variables to each PC
contrib2_1hour<-get_pca_var(EventsPCA2_1hour)
colnames(contrib2_1hour$contrib)<- c("PC1", "PC2", "PC3", "PC4")
rownames(contrib2_1hour$contrib)<- c("Speed", "Abs. Angle", "Turn Speed", "FPT")
corrplot(contrib2_1hour$contrib, method = 'color', 
         is.corr = F,
         addgrid.col = "grey",
         col = COL1('YlGn'),
         addCoef.col = 'black',
         tl.col = "black",
         tl.srt = 0,
         tl.offset = 0.5,
         mar = c(1.5, 1.5, 1.5, 1.5),
         cl.pos = 'n',
         cl.length = 5,
         col.lim=c(0,100),
         tl.cex = 3,
         number.cex = 3,
         number.digits = 0
         )
#Score Plot
fviz_pca_ind(EventsPCA2_1hour,
             geom.ind = "none",
             palette = c("red", "green", "blue"),
             col.ind = as.factor(Norm2_1hour$Region),
             addEllipses = TRUE, 
             mean.point = T,
             ellipse.alpha = .05,
             mean.point.size = 4,
             scaleshape = "none",
             title = " "
             ) +
  theme(legend.position = "none", 
        axis.title.x = element_text(margin = margin(t = 30), size = 15),
        axis.title.y = element_text(margin = margin(r = 30), size = 15),
        axis.text = element_text(size = 15))
```

```{r}
EventsPCA2_3hour<-PCA(Norm2_3hour[,1:4], scale = F, graph = F)
EventsPCA2_3hour$var
fviz_screeplot(EventsPCA2_3hour, addlabels = TRUE, title = "3-hour") +
  theme(legend.position = "none", 
        axis.title.x = element_text(margin = margin(t = 30), size = 15),
        axis.title.y = element_text(margin = margin(r = 30), size = 15),
        axis.text = element_text(size = 15))
#Loadings plot
fviz_pca_var(EventsPCA2_3hour, 
             col.var =  "cos2", arrowsize = 1.5, repel = T,
             title = " ") +
  theme(legend.position = "none", 
        axis.title.x = element_text(margin = margin(t = 30), size = 15),
        axis.title.y = element_text(margin = margin(r = 30), size = 15),
        axis.text = element_text(size = 15)) +
  scale_color_gradient2(low = "red", mid = "blue", high = "green", 
                        midpoint = 0.5, limits = c(0,1))+
  xlim(c(-1,1)) +
  ylim(c(-1,1))
#Contribution of variables to each PC
contrib2_3hour<-get_pca_var(EventsPCA2_3hour)
colnames(contrib2_3hour$contrib)<- c("PC1", "PC2", "PC3", "PC4")
rownames(contrib2_3hour$contrib)<- c("Speed", "Abs. Angle", "Turn Speed", "FPT")
corrplot(contrib2_3hour$contrib, method = 'color', 
         is.corr = F,
         addgrid.col = "grey",
         col = COL1('YlGn'),
         addCoef.col = 'black',
         tl.col = "black",
         tl.srt = 0,
         tl.offset = 0.5,
         mar = c(1.5, 1.5, 1.5, 1.5),
         cl.pos = 'n',
         cl.length = 5,
         col.lim=c(0,100),
         tl.cex = 3,
         number.cex = 3,
         number.digits = 0
         )
#Score Plot
fviz_pca_ind(EventsPCA2_3hour,
             geom.ind = "none",
             palette = c("Red", "Green", "Blue"),
             col.ind = as.factor(Norm2_3hour$Region),
             addEllipses = T, 
             mean.point = T,
             ellipse.alpha = 0.05,
             mean.point.size = 4,
             scaleshape = "none",
             title = " "
             ) +
  theme(legend.position = "none", 
        axis.title.x = element_text(margin = margin(t = 30), size = 15),
        axis.title.y = element_text(margin = margin(r = 30), size = 15),
        axis.text = element_text(size = 15))
```

```{r}
#1-day
EventsPCA2_1day<-PCA(Norm2_1day[,1:4], scale = F, graph = F)
EventsPCA2_1day$var
fviz_screeplot(EventsPCA2_1day, addlabels = TRUE, title = "1-day") +
  theme(legend.position = "none", 
        axis.title.x = element_text(margin = margin(t = 30), size = 15),
        axis.title.y = element_text(margin = margin(r = 30), size = 15),
        axis.text = element_text(size = 15))
#Loadings plot
fviz_pca_var(EventsPCA2_1day,
             col.var =  "cos2", arrowsize = 1.5, repel = T,
             title = " ") +
  theme(legend.position = "none", 
        axis.title.x = element_text(margin = margin(t = 30), size = 15),
        axis.title.y = element_text(margin = margin(r = 30), size = 15),
        axis.text = element_text(size = 15)) +
  scale_color_gradient2(low = "red", mid = "blue", high = "green", 
                        midpoint = 0.5, limits = c(0,1))+
  xlim(c(-1,1)) +
  ylim(c(-1,1))
#Contribution of variables to each PC
contrib2_1day<-get_pca_var(EventsPCA2_1day)
colnames(contrib2_1day$contrib)<- c("PC1", "PC2", "PC3", "PC4")
rownames(contrib2_1day$contrib)<- c("Speed", "Abs. Angle", "Turn Speed", "FPT")
corrplot(contrib2_1day$contrib, method = 'color', 
         is.corr = F,
         addgrid.col = "grey",
         col = COL1('YlGn'),
         addCoef.col = 'black',
         tl.col = "black",
         tl.srt = 0,
         tl.offset = 0.5,
         mar = c(1.5, 1.5, 1.5, 1.5),
         cl.pos = 'n',
         cl.length = 5,
         col.lim=c(0,100),
         tl.cex = 3,
         number.cex = 3,
         number.digits = 0
         )
#Score Plot
fviz_pca_ind(EventsPCA2_1day,
             geom.ind = "none",
             palette = c("Red", "Green", "Blue"),
             col.ind = as.factor(Norm2_1day$Region),
             addEllipses = TRUE, 
             mean.point = T,
             ellipse.alpha = 0.05,
             mean.point.size = 4,
             scaleshape = "none",
             title = " "
             ) +
  theme(axis.title.x = element_text(margin = margin(t = 30), size = 15),
        axis.title.y = element_text(margin = margin(r = 30), size = 15),
        axis.text = element_text(size = 15))
```

#### PERMANOVA and PERMDISP

> Many variables do not seem normally distributed. To avoid issues with normal distribution assumptions, I will use a non-parametric test. Since I am comparing groups that each have multiple variables, I will use a PERMANOVA. To start, you will first need to see if you data meets the PERMANOVA dispersion assumptions. Interpretation of p-values will be heavily influenced based on whether the assumptions of PERMANOVA are met.

> Subset 1

```{r}
set.seed(2016)
#determine if the test will be unbalanced. If there are hundreds of entries, an unbalanced design should not affect the results.  
#Create a distance matrix
dis <- dist(Norm_1hour[,1:4])
#create a dispersion object
mod <- betadisper(dis, Norm_1hour$migrationEvent, type = "centroid")
#Test the homogeneity of dispersion between groups. If the p-value is > 0.05, then the assumptions of PERMANOVA are met: the null hypothesis is that overall dispersion between groups is not different.
permutest(mod)
#post-hoc for dispersions
dispersions<-TukeyHSD(mod)
dispersions_m<-tukey_to_matrix(dispersions$group) #make into a matrix for corrplot
dispersions_m[is.na(dispersions_m)]<- 1
dispersions_m_t<-t(dispersions_m)
dispersions_m_t[is.na(dispersions_m_t)]<- 1
dispersions1<-dispersions_m*dispersions_m_t
#Make heatmap
Heatmap(dispersions1, name = "p-value", 
        row_names_gp = gpar(fontsize = 10),
        column_names_gp= gpar(fontsize = 10),
        cluster_rows = F,
        cluster_columns = F,
        col = colorRamp2(breaks = c(0,0.5,0.51, 1), 
                    colors = c("red", "red","cadetblue3", "cadetblue3")),
        color_space = c(0,1),
        rect_gp = gpar(col = "grey80", lwd = 0.5),
        heatmap_legend_param = list(legend_height = unit(12, "cm"))) #export size is 1500 x 1160

#permanova
perm<-adonis2(dis ~ migrationEvent, data = Norm_1hour, method = "euclidean") #At least one group mean is different hypothesis - the groups are different from each other.
perm
adonis_OmegaSq(perm, partial = T) #Add omega R-squared value (effect size)
#post-hoc test
pairperm<-pairwise.adonis2(dis ~ migrationEvent, data = Norm_1hour)
pairperm<-pairperm[-1] #remove the first entry
pairperm_DF<-data.frame()
for (i in names(pairperm)) {
  x<-as.data.frame(pairperm[[i]])
  name<-i
  R2<-x$R2[1]
  P.Val<-x$`Pr(>F)`[1]
  val<-cbind(name, R2, P.Val)
  pairperm_DF<-rbind(val, pairperm_DF)
}
pairperm_DF$R2<-as.numeric(pairperm_DF$R2)
summary(pairperm_DF)
colnames(pairperm_DF)<-c("Comparison","R2", "p.adj")
pairperm_DF<-separate(data = pairperm_DF, col = Comparison, into = c("left", "right"), sep = "\\_vs_")
pairperm_DF<-select(pairperm_DF, "right", "left", "p.adj")
pairperm_DF<-pairperm_DF[, c("right", "left", "p.adj")]
Artoo_2013<-filter(pairperm_DF, left == "Artoo_2013")
Artoo_2013<-select(Artoo_2013, "left", "right", "p.adj")
Woody_2014<-filter(pairperm_DF, right == "Woody_2014")
Woody_2014<-select(Woody_2014, "left", "right", "p.adj")
Missing<-rbind(Woody_2014, Artoo_2013)
colnames(Missing)<-c("right", "left", "p.adj")
pairperm_DF_c<-rbind(Missing, pairperm_DF)
pairperm_DF_c$p.adj<-as.numeric(pairperm_DF_c$p.adj)
pairperm_DF_m<-as.matrix(xtabs(pairperm_DF_c[, 3] ~ pairperm_DF_c[,2] + pairperm_DF_c[, 1]))
pairperm_DF_m[pairperm_DF_m ==0]<-1
pairperm_DF_m_t<-t(pairperm_DF_m)
pairperm_DF1<-pairperm_DF_m*pairperm_DF_m_t

#heat map time
Heatmap(pairperm_DF1, name = "p-value", 
        row_names_gp = gpar(fontsize = 10),
        column_names_gp= gpar(fontsize = 10),
        cluster_rows = F,
        cluster_columns = F,
        col = colorRamp2(breaks = c(0,0.5,0.51, 1), 
                    colors = c("red", "red","cadetblue3", "cadetblue3")),
        color_space = c(0,1),
        rect_gp = gpar(col = "grey80", lwd = 0.5),
        heatmap_legend_param = list(legend_height = unit(12, "cm")))
```

```{r}
set.seed(2016)
dis_3 <- dist(Norm_3hour[,1:4])
mod_3 <- betadisper(dis_3, Norm_3hour$migrationEvent, type = "centroid")
permutest(mod_3)

dispersions_3<-TukeyHSD(mod_3)
dispersions_3_m<-tukey_to_matrix(dispersions_3$group) #make into a matrix for corrplot
dispersions_3_m[is.na(dispersions_3_m)]<- 1
dispersions_3_m_t<-t(dispersions_3_m)
dispersions_3_m_t[is.na(dispersions_3_m_t)]<- 1
dispersions3<-dispersions_3_m*dispersions_3_m_t
#Make heatmap
Heatmap(dispersions3, name = "p-value", 
        row_names_gp = gpar(fontsize = 10),
        column_names_gp= gpar(fontsize = 10),
        cluster_rows = F,
        cluster_columns = F,
        col = colorRamp2(breaks = c(0,0.5,0.51, 1), 
                    colors = c("red", "red","cadetblue3", "cadetblue3")),
        color_space = c(0,1),
        rect_gp = gpar(col = "grey80", lwd = 0.5),
        heatmap_legend_param = list(legend_height = unit(12, "cm")))

#permanova
perm_3<-adonis2(dis_3 ~ migrationEvent, data = Norm_3hour, method = "euclidean") #At least one group mean is different hypothesis - the groups are different from each other.
perm_3
adonis_OmegaSq(perm_3, partial = TRUE) #Add omega R-squared value (effect size)
#post-hoc test
pairperm_3<-pairwise.adonis2(dis_3 ~ migrationEvent, data = Norm_3hour)
pairperm_3<-pairperm_3[-1] #remove the first entry
pairperm_3_DF<-data.frame()
for (i in names(pairperm_3)) {
  x<-as.data.frame(pairperm_3[[i]])
  name<-i
  R2<-x$R2[1]
  P.Val<-x$`Pr(>F)`[1]
  val<-cbind(name, R2, P.Val)
  pairperm_3_DF<-rbind(val, pairperm_3_DF)
}
pairperm_3_DF$R2<-as.numeric(pairperm_3_DF$R2)
summary(pairperm_3_DF)
colnames(pairperm_3_DF)<-c("Comparison","R2", "p.adj")
pairperm_3_DF<-separate(data = pairperm_3_DF, col = Comparison, into = c("left", "right"), sep = "\\_vs_")
pairperm_3_DF<-select(pairperm_3_DF, "right", "left", "p.adj")
pairperm_3_DF<-pairperm_3_DF[, c("right", "left", "p.adj")]
Artoo_2013<-filter(pairperm_3_DF, left == "Artoo_2013")
Artoo_2013<-select(Artoo_2013, "left", "right", "p.adj")
Woody_2014<-filter(pairperm_3_DF, right == "Woody_2014")
Woody_2014<-select(Woody_2014, "left", "right", "p.adj")
Missing<-rbind(Woody_2014, Artoo_2013)
colnames(Missing)<-c("right", "left", "p.adj")
pairperm_3_DF_c<-rbind(Missing, pairperm_3_DF)
pairperm_3_DF_c$p.adj<-as.numeric(pairperm_3_DF_c$p.adj)
pairperm_3_DF_m<-as.matrix(xtabs(pairperm_3_DF_c[, 3] ~ pairperm_3_DF_c[,2] + pairperm_3_DF_c[, 1]))
pairperm_3_DF_m[pairperm_3_DF_m ==0]<-1
pairperm_3_DF_m_t<-t(pairperm_3_DF_m)
pairperm_DF3<-pairperm_3_DF_m*pairperm_3_DF_m_t

#heat map time
Heatmap(pairperm_DF3, name = "p-value", 
        row_names_gp = gpar(fontsize = 10),
        column_names_gp= gpar(fontsize = 10),
        cluster_rows = F,
        cluster_columns = F,
        col = colorRamp2(breaks = c(0,0.5,0.51, 1), 
                    colors = c("red", "red","cadetblue3", "cadetblue3")),
        color_space = c(0,1),
        rect_gp = gpar(col = "grey80", lwd = 0.5),
        heatmap_legend_param = list(legend_height = unit(12, "cm")))
```

```{r}
set.seed(2016)
dis_24 <- dist(Norm_1day[,1:4])
mod_24 <- betadisper(dis_24, Norm_1day$migrationEvent, type = "centroid")
permutest(mod_24) 

dispersions_24<-TukeyHSD(mod_24)
dispersions_24_m<-tukey_to_matrix(dispersions_24$group) #make into a matrix for corrplot
dispersions_24_m[is.na(dispersions_24_m)]<- 1
dispersions_24_m_t<-t(dispersions_24_m)
dispersions_24_m_t[is.na(dispersions_24_m_t)]<- 1
dispersions24<-dispersions_24_m*dispersions_24_m_t

#Make heatmap
Heatmap(dispersions24, name = "p-value", 
        row_names_gp = gpar(fontsize = 10),
        column_names_gp= gpar(fontsize = 10),
        cluster_rows = F,
        cluster_columns = F,
        col = colorRamp2(breaks = c(0,0.5,0.51, 1), 
                    colors = c("red", "red","cadetblue3", "cadetblue3")),
        color_space = c(0,1),
        rect_gp = gpar(col = "grey80", lwd = 0.5),
        heatmap_legend_param = list(legend_height = unit(12, "cm")))
#permanova
perm_24<-adonis2(dis_24 ~ migrationEvent, data = Norm_1day, method = "euclidean") #At least one group mean is different hypothesis - the groups are different from each other.
perm_24
adonis_OmegaSq(perm_24, partial = TRUE) #Add omega R-squared value (effect size)
#post-hoc test
pairperm_24<-pairwise.adonis2(dis_24 ~ migrationEvent, data = Norm_1day)
pairperm_24<-pairperm_24[-1] #remove the first entry
pairperm_24_DF<-data.frame()
for (i in names(pairperm_24)) {
  x<-as.data.frame(pairperm_24[[i]])
  name<-i
  R2<-x$R2[1]
  P.Val<-x$`Pr(>F)`[1]
  val<-cbind(name, R2, P.Val)
  pairperm_24_DF<-rbind(val, pairperm_24_DF)
}
pairperm_24_DF$R2<-as.numeric(pairperm_24_DF$R2)
summary(pairperm_24_DF)
colnames(pairperm_24_DF)<-c("Comparison","R2", "p.adj")
pairperm_24_DF<-separate(data = pairperm_24_DF, col = Comparison, into = c("left", "right"), sep = "\\_vs_")
pairperm_24_DF<-select(pairperm_24_DF, "right", "left", "p.adj")
pairperm_24_DF<-pairperm_24_DF[, c("right", "left", "p.adj")]
Artoo_2013<-filter(pairperm_24_DF, left == "Artoo_2013")
Artoo_2013<-select(Artoo_2013, "left", "right", "p.adj")
Woody_2014<-filter(pairperm_24_DF, right == "Woody_2014")
Woody_2014<-select(Woody_2014, "left", "right", "p.adj")
Missing<-rbind(Woody_2014, Artoo_2013)
colnames(Missing)<-c("right", "left", "p.adj")
pairperm_24_DF_c<-rbind(Missing, pairperm_24_DF)
pairperm_24_DF_c$p.adj<-as.numeric(pairperm_24_DF_c$p.adj)
pairperm_24_DF_m<-as.matrix(xtabs(pairperm_24_DF_c[, 3] ~ pairperm_24_DF_c[,2] + pairperm_24_DF_c[, 1]))
pairperm_24_DF_m[pairperm_24_DF_m ==0]<-1
pairperm_24_DF_m_t<-t(pairperm_24_DF_m)
pairperm_DF24<-pairperm_24_DF_m*pairperm_24_DF_m_t

#heat map time
Heatmap(pairperm_DF24, name = "p-value", 
        row_names_gp = gpar(fontsize = 10),
        column_names_gp= gpar(fontsize = 10),
        cluster_rows = F,
        cluster_columns = F,
        col = colorRamp2(breaks = c(0,0.5,0.51, 1), 
                    colors = c("red", "red","cadetblue3", "cadetblue3")),
        color_space = c(0,1),
        rect_gp = gpar(col = "grey80", lwd = 0.5),
        heatmap_legend_param = list(legend_height = unit(12, "cm")))
```

> Subset 2

```{r}
set.seed(2016)
#determine if the test will be unbalanced. If there are hundreds of entries, an unbalanced design should not affect the results.  
#Create a distance matrix
in_dis <- dist(Norm2_1hour[,1:4])
#create a dispersion object
in_mod <- betadisper(in_dis, Norm2_1hour$Region, type = "centroid")
#Test the homogeneity of dispersion between groups. If the p-value is > 0.05, then the assumptions of PERMANOVA are met: the null hypothesis is that overall dispersion between groups is not different.
permutest(in_mod) #test if dispersion from centroids drives differences via PERMDISP. 
in_dispersions<-TukeyHSD(in_mod)
in_dispersions
#permanova
in_perm<-adonis2(in_dis ~ Region, data = Norm2_1hour, method = "euclidean") 
in_perm
adonis_OmegaSq(in_perm, partial = TRUE) 

in_pairperm<-pairwise.adonis2(in_dis ~ Region, data = Norm2_1hour)
in_pairperm
```

```{r}
set.seed(2016)
in_dis_3 <- dist(Norm2_3hour[,1:4])
in_mod_3 <- betadisper(in_dis_3, Norm2_3hour$Region, type = "centroid")
permutest(in_mod_3) 
in_dispersions_3<-TukeyHSD(in_mod_3)
in_dispersions_3
#permanova
in_perm_3<-adonis2(in_dis_3 ~ Region, data = Norm2_3hour, method = "euclidean") 
in_perm_3
adonis_OmegaSq(in_perm_3, partial = TRUE) 

in_pairperm_3<-pairwise.adonis2(in_dis_3 ~ Region, data = Norm2_3hour)
in_pairperm_3
```

```{r}
set.seed(2016)
in_dis_24 <- dist(Norm2_1day[,1:4])
in_mod_24 <- betadisper(in_dis_24, Norm2_1day$Region, type = "centroid")
permutest(in_mod_24) 
in_dispersions_24<-TukeyHSD(in_mod_24)
in_dispersions_24
#permanova
in_perm_24<-adonis2(in_dis_24 ~ Region, data = Norm2_1day, method = "euclidean") 
in_perm_24
adonis_OmegaSq(in_perm_24, partial = TRUE) 

in_pairperm_24<-pairwise.adonis2(in_dis_24 ~ Region, data = Norm2_1day)
in_pairperm_24
```

## References

* Help using the move package to download data from Movebank: <https://cran.r-project.org/web/packages/move/vignettes/browseMovebank.html> 
* Navigating and using data from the move package: <https://cran.r-project.org/web/packages/move/vignettes/move.html>
* Download a CSV file from github: <https://lokraj.me/post/download-github-data/>
* Github and RStudio interface: <https://rfortherestofus.com/2021/02/how-to-use-git-github-with-r/>
* Finding mode in R: <https://www.statology.org/mode-in-r/>
* Projections in R: <https://www.nceas.ucsb.edu/sites/default/files/2020-04/OverviewCoordinateReferenceSystems.pdf>
* adehabitatLT use (not the vignette): <http://www.r-gators.com/2018/01/31/wildlife-tracking-data-in-r/>



