---
title: "Lewis' Thesis Code"
author: "Lewis Hakam"
date: "10/3/2023"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#personal access token if needed: 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
#Data retrieval and manipulation
library(move)
library(readr) #download CSV using URL link from github
library(sp)
library(moveVis)
#organizating, filtering, cleaning data
library(plyr)
library(dplyr) #deleted and re installed rlang
library(stats)
library(devtools) #deleted and re installed htmltools
library(lutz) #time zone lookup
library(solaR) #calculate solar time
library(lubridate)
find_mode <- function(x) {
  u <- unique(x)
  tab <- tabulate(match(x, u))
  u[tab == max(tab)]
} #no built in function for mode in R so I got this one from the web. 
library(data.table)
#Movement models
library(migrateR) #could not build vignettes. Also used force = T and updated some packages.
library(adehabitatLT)
#static map
library(ggplot2)
library(sf)
library(tmap)
wgs84<-CRS("+proj=longlat +datum=WGS84") #GCS in WGS 1984
azim_orign = CRS("+proj=aeqd +lat_0=24.43 +lon_0=-82.74 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m +no_defs") #PCS in custom azimuthal equidistant projection. estiated middle point of the data visually with google maps
data(World) #world data with country polygons needed for mapping
world_sf<-st_transform(World, azim_orign) #projected world data
plotshit<-function(y) {
  ggplot(y) +
    geom_sf(data = world_sf, fill = "white") +
    geom_sf(size = 1, aes(color = year)) +
    coord_sf(datum = st_crs(azim_orign), xlim = c(-4000000, 3000000), ylim = c(3500000, -2500000)) +
    theme(legend.position = "bottom", legend.text = element_text(size = 7)) +
    labs(color = "year") +
    ggtitle(paste0(y$trackId))
} #function for batch plotting in a for loop
```
## Introduction

Hey there! If you are viewing this article, you are interested in how I analyzed osprey tracking data for my final thesis, completed in Spring 2024. I will guide through a step by step process. I have used R Markdown, which may be new for you. This document will not go over the nuances I used to code the Markdown file. For simplicity and privacy, not all code lines are shown in the Rmarkdown output. If you want to see the entire code script, you will need to open the RStudio project, accessible through the Osprey Shared Drive or by contacting me directly. I have done my best to annotate my code for easy interpret ability.


## Packages
Here is a list of packages and functions in the order of their first appearance that I used.You will likely have to use the function install.packages ("Package Name") before you can access the package using the library (Package Name) function. I had a few instances where I had supporting packages downloaded, but they were not up-to-date nor could I force them to update. For these packages I had to go into this PC < documents < R and locally delete the packages that were not updating and then re-install them. I've made notes next to supporting packages for which I did this. 

```{r, eval=FALSE}
library(move) #download, manipulate, and analyze telemetry data
library(dplyr) #Filtering data via the filter() function. Deleted and re installed rlang
library(stats) #find the median and other statistical operations
find_mode <- function(x) {
  u <- unique(x)
  tab <- tabulate(match(x, u))
  u[tab == max(tab)]
} #no built in function for mode in R so I got this one from the web. 
library(ggplot2) #dynamic, mapping and graphing
library(sf) #displaying coordinate data in ggplot
library(rgdal) #creating custom PCS via the CRS() function. 
wgs84<-CRS("+proj=longlat +datum=WGS84") #GCS in WGS 1984, required for a PCS.
azim_orign = CRS("+proj=aeqd +lat_0=25 +lon_0=-90 +x_0=0 +y_0=0 
                 +ellps=WGS84 +datum=WGS84 
                 +units=m +no_defs") #PCS in custom azimuthal equidistant projection
library(tmap) #world polygon data
data(World) #world data with country polygons needed for mapping
world_sf<-st_transform(World, azim_orign) #projected world data
plotshit<-function(y) {
  ggplot(y) +
    geom_sf(data = world_sf, fill = "white") +
    geom_sf(size = 1, aes(color = year)) +
    coord_sf(datum = st_crs(azim_orign), xlim = c(-2000000, 8000000), ylim = c(3000000, -3000000)) +
    theme(legend.position = "bottom", legend.text = element_text(size = 7)) +
    labs(color = "year") +
    ggtitle(paste0(y$trackId))
} #function for batch plotting in a for loop
library(lubridate) #manipulate date/timestamps  the proper 
library(solaR) #calculate solar time for IL osprey data
library(sp) #spatial object needed for an ltraj object and to find center of data
library(adehabitatLT) #create an ltraj object needed to run functions in migrateR
library(devtools) #Needed to install migrateR. Deleted and re installed htmltools
library(migrateR) #Could not build vignettes. Also used force = T and updated some packages.See references for link to vignettes and github code repository with download instructions. 
library(data.table) #data frame list manipulation
library(moveVis) #for converting a dataframe into a move object
```

## Downloading and Filter Tracking Data

#### Downloading and Exploring Data From Movebank
First, If you do not have a Movebank account, you must make a free account before you can access any of the data.

I pulled data from three sources:
1. Bierregaard East Coast Osprey Tracking - open-access download (permission to use in publications)
2. Barb Jensen's Michigan Osprey Tracks - open-access download (permission to use in publications)
3. The Montana osprey Satellite Tracking Program - Permission-access only

To pull data from movebank you will use the 'move' package. Before you can pull data with the move package you will need to create an object called loginStored which contains your login info. I have not included the actual code since I do not want to give out my password. Your code will look something like this:
```{r, eval=FALSE}
loginStored <- movebankLogin(username = "UserName", password = "Password")
```

```{r, echo=FALSE}
loginStored <- movebankLogin(username = "lhaka2", password = "!ShamaJ2016!")
```

Next you will download the Movebank study synopsis and summary tables. The synopsis can provide details on how many individuals were tracked, the time frame tracking occurred, the type of device used, and permission/privacy restrictions. The synopsis along with the Movebank study name can be located on the Movebank website along with a handy interactive map. It is possible to search for studies within R, but it is honestly easier to use the web-based platform. The summary table will provide a summary of each individual tracked per device. This means that some individuals may have multiple entries in a summary table to account for different tracking mechanisms. We can use the summary table info to fill out demographic info and determine how many individuals were tracked using GPS or GPS/GSM devices. 
```{r}
#download Bierregaard data Synopsis
getMovebankStudy(study ="Osprey Bierregaard North and South America", login = loginStored)
#download Bierregaard summary table
Bierregaard_Osprey_summary<-getMovebankReferenceTable(study ="Osprey Bierregaard North and South America", login = loginStored, allAttributes = FALSE)
#Explore sensor attributes and associated codes. We only want a summary of individuals with GPS tracks.
getMovebankSensorsAttributes(study="Osprey Bierregaard North and South America",login=loginStored) #sensor_type_id for GPS that we will need is 653. 7842954 is the GPS engineering data and 82798	is Argos data. Filter the summary table to keep only individuals with the sensor_type_id of 653.
Bierregaard_Osprey_summary<-filter(Bierregaard_Osprey_summary,
                                   Bierregaard_Osprey_summary$sensor_type_id == 653)
#Create a table of sex and life stage for a brief overview of how many adults/juveniles of male and female exist within the dataset.
table(Bierregaard_Osprey_summary$animal_life_stage, Bierregaard_Osprey_summary$animal_sex)
# While this table is helpful at a glance, I found I could fill in life stage by looking at the birth date if include and comparing it with the tracking year. I also consulted the comments for clues to age and sex. I did this by going to the environment pane and clicking on the filtered summary table to open the full table. 
```

```{r}
#Barb Jensen's data
getMovebankStudy(study ="Pandion haliaetus Osprey - SouthEast Michigan", login = loginStored)
Jensen_Osprey_summary<-getMovebankReferenceTable(study ="Pandion haliaetus Osprey - SouthEast Michigan", 
                                                 login = loginStored, allAttributes = FALSE)
Jensen_Osprey_summary<-filter(Jensen_Osprey_summary, Jensen_Osprey_summary$sensor_type_id == 653)
table(Jensen_Osprey_summary$animal_sex) #22 individuals total, 11 females, 8 males, and 3 unknown
```

The summary tables give me an idea of the total number of individuals in each study and the sex and age of each individual. However, it does not tell me the number of fall migrations and whether those migrations were complete or not, does not tell if the individual started migration, and may have missing age data that can be determined from mapping the data. That said, the data sets are really large, so I can initially filter the data to make it easier to work with and plot. I will remove:
* individuals with sampling frequencies courser than 1-hour (using mode and median)
* GPS fixes outside of July-November (7-11)
* erroneous points that don't meet a defined biological/practical threshold.

```{r}
#Download data from Movebank for GPS tracked individuals.This takes some time, but downloads data directly from Movebank. From the summary data, I filtered out only GPS tracked individuals. When downloading movebank data, I can specify which individuals I want in my dataset using animalName = (list of animal local identifiers)
Bierregaard_Osprey<-getMovebankData(study ="Osprey Bierregaard North and South America",
                                    animalName = c(Bierregaard_Osprey_summary$animal_local_identifier),
                                    login = loginStored, removeDuplicatedTimestamps=TRUE) 
#Get the time between consecutive locations using the move package
list_Bierr<-timeLag(Bierregaard_Osprey, units = "hours")
#lapply function applies a function to a list.Each individual within the data set has a median or mode timelag of 1 hour or finer. I used median/mode instead of average since its inevitable that some points were missed (either because the GPS was set to turn off an night, the GPS was turned off when the bird reached a certain location, the data was collected in bursts in the case of GSM, or because the GPS could not get signal)
sapply(list_Bierr, find_mode) 
sapply(list_Bierr, median)
#make into a data frame so I can easily filter, plot, and manipulate the data.
Bierregaard_Osprey_DF<-as.data.frame(Bierregaard_Osprey)
#I don't need to convert timestamp to local time since its already in local time apparently
#Extract month data for filtering
Bierregaard_Osprey_DF$month<-month(ymd_hms(Bierregaard_Osprey_DF$timestamp))
#Extract year data for plotting later
Bierregaard_Osprey_DF$year<-year(ymd_hms(Bierregaard_Osprey_DF$timestamp))
#remove rows with NA values for the next step
Bierregaard_Osprey_DF<-Bierregaard_Osprey_DF[!is.na(Bierregaard_Osprey_DF$heading),]
#filter erroneous points: Erroneous points are those with unrealistically high speed and turning angle (course). For course (sometimes mistakenly referred to as heading) we will look at the 90th percentile of points as suggested by Gupte et al., 2021. Course is measured as the angle from magnetic north (clockwise).A course of 0 indicates no movement. For speed I used Kerlinger 1989 to determine that the maximum recorded flight speed of an osprey was 33.4 meters/second.
quantile(Bierregaard_Osprey_DF$heading, probs = 0.9) #got 228
Bierregaard_Osprey_DF_Filter<-filter(Bierregaard_Osprey_DF, ground_speed <= 33.4 & heading < 239)
#Filter data - removed timestamps from December through June (keep months 7-11)
Bierregaard_Osprey_Data <- Bierregaard_Osprey_DF_Filter %>%
  filter(sensor_type_id == 653 & month %in% c(6:12))
```

```{r}
#All individuals are tracked via GPS so no need to specify which individuals to download. 
Jensen_Osprey<-getMovebankData(study ="Pandion haliaetus Osprey - SouthEast Michigan",
                                    login = loginStored, removeDuplicatedTimestamps=TRUE) 
#explore sampling frequencies using time lag
list_Jens<-timeLag(Jensen_Osprey, units = "hours")
sapply(list_Jens, find_mode)
sapply(list_Jens, median) #X82186.Humphries_Rachel and X82190.Fenton_Aldo have courser frequencies than what I am looking for so I will remove these individuals after mapping. 
Jensen_Osprey_DF<-as.data.frame(Jensen_Osprey)
Jensen_Osprey_DF$month<-month(ymd_hms(Jensen_Osprey_DF$timestamp))
Jensen_Osprey_DF$year<-year(ymd_hms(Jensen_Osprey_DF$timestamp))
Jensen_Osprey_DF<-Jensen_Osprey_DF[!is.na(Jensen_Osprey_DF$heading),]
#Filter data - removed timestamps from December through June (keep months 7-11)
quantile(Jensen_Osprey_DF$heading, probs = 0.9) #got 218
Jensen_Osprey_DF_Filter<-filter(Jensen_Osprey_DF, ground_speed <= 33.4 & heading < 227)
Jensen_Osprey_Data <- Jensen_Osprey_DF_Filter %>%
  filter(sensor_type_id == 653 & month %in% c(6:12))
#remove the two individuals that were tracked at courser sampling frequencies than needed.
Jensen_Osprey_Data$trackId<-as.character(Jensen_Osprey_Data$trackId)
remove<-c("X82190.Fenton_Aldo", "X82186.Humphries_Rachel")
Jensen_Osprey_Data<-Jensen_Osprey_Data[!(Jensen_Osprey_Data$trackId %in% remove),]
unique(Jensen_Osprey_Data$trackId)
```

To map the data from each study, I will use the 'ggplot', 'sf', 'rgdal', and base R functions. This includes a custom function I created to help batch map all of my data. Data on the background world countries will be obtained using 'tmap' and data will be projected using a custom aziumthal equidistant projection for now. 

```{r, fig.show="hold", out.width="10%"}
#Spatially reference the Bierregaard data
Bierregaard_Osprey_Data_sf<-st_as_sf(x=Bierregaard_Osprey_Data, 
                                     coords = c("location_long.1", "location_lat.1"), crs = wgs84)
Bierregaard_Osprey_Data_sf<-st_transform(Bierregaard_Osprey_Data_sf, azim_orign)
#to color code year, ggplot needs year to be a factor data type. 
Bierregaard_Osprey_Data_sf$year<-as.factor(Bierregaard_Osprey_Data_sf$year) 
#batch map the GPS points for each individual using ggplot
names<-unique(Bierregaard_Osprey_Data_sf$trackId)

for (i in names) {
  y = filter(Bierregaard_Osprey_Data_sf, trackId == i)
  pws<-plotshit(y)
  print(pws)
}
#I will remove individuals that did not start migration or were tracked in 2007 (the tracks from this year are very thin: "Pearl 2013", "Rafael 2009","Luke 2007", "Aster 2017", "Chester 2014", "Felix 2007", "Claws 2007", "Claws 2008" "Mackenzie 2013", "Trepassey 2016", "Tucker 2011", "Jocelyn 2016", 

#I will also remove tracks from specific years for some individuals since those migrations were not started or skipped (juveniles spend 18 months in wintering grounds during their first winter): "Rammie 2013", "Snowy 2012", "Thatcher 2011", "Penelope 2009", "Buck 2010", "Belle 2011", "Flow 2015", "Art 2013", "Artoo 2014", "Crabby 2015", "Borealis 2018", "Daphne 2017", "Weber 2014", "Nick 2017", "Ron 2015", "Woody 2015"
```

```{r, fig.show="hold", out.width="10%"}
#Give Jensen data spatial reference
Jensen_Osprey_Data_sf<-st_as_sf(x=Jensen_Osprey_Data, 
                                coords = c("location_long.1", "location_lat.1"), crs = wgs84)
Jensen_Osprey_Data_sf<-st_transform(Jensen_Osprey_Data_sf, azim_orign)
Jensen_Osprey_Data_sf$year<-as.factor(Jensen_Osprey_Data_sf$year)
#batch map the GPS points for each individual using ggplot
names<-unique(Jensen_Osprey_Data_sf$trackId)
names
for (i in names) {
  y = filter(Jensen_Osprey_Data_sf, trackId == i)
  pws<-plotshit(y)
  print(pws)
}

#Remove individuals that did not migrate: "X586.Brighton_Barb 2015", "X934.Kensington.Pebbles 2017"

#Remove tracks for years where migration did not happend: "X423.Humphries_DTE_Ozzie 2015", "X139005.Monroe.Julie 2017"
```

The plots along with the summary data allow me to create an excel file with all the demographic info for each each migration event including the individual associated with each migration event. Migration events will be analyzed separately from each other. From the plots, I can also tell if some individuals were juveniles during their first migration and remove those that did not start migration.

For now this is as much as I can filter the Movebank data. I will have to filter out migration events that did not occur later and GPS points that fall outside of the migration time period. We will get to this later once all the datasets are combined. 

#### Downloading and Exploring Data From the Illinois Osprey Program
I won't have as much filtering to do with the Illinois osprey data since I have explored this data in the past and I know which individuals I should keep. I also know all individuals are GPS tracked and that only GPS points are included in the data set. I will still need to filter by month and take out erroneous points, but that is about all I have to do. I have plotted the data on a single map for reference. 

Load in data and keep only individuals that started migration.
```{r}
#The dataset is on github, so you can directly import the dataset to R. No special code required. 
IL_OspreyData<-as.data.frame(PTT2014to2022Compiled)
#I explored this data earlier in the year, and determined these were the individuals that started migration.
keep<-c("2016_35D_14700", "2017_47D_171072", "2019_69D_180325", "2022_27R_220453", "2022_26R_234626", "2020_06R_202512", "2017_44D_171073", "2014_14D_139465", "2020_73D_202511")
IL_OspreyData<-IL_OspreyData[IL_OspreyData$ID %in% keep, ] 
```

Clean and filter the data further. 
```{r}
#Rename lat/lon columns cause the names currently will cause issues
names(IL_OspreyData) [names(IL_OspreyData) == "Longitude(E)"] <- "location_long"
names(IL_OspreyData) [names(IL_OspreyData) == "Latitude(N)"] <- "location_lat"
#Remove NAs from lat/lon
IL_OspreyData<-filter(IL_OspreyData, location_lat !=0 | location_long != 0)
#Create timestamp
IL_OspreyData$TimeStamp<-as.POSIXct(IL_OspreyData$TimeStamp, format = "%m/%d/%Y %H:%M", tz = "UTC")
#Calculate Solar Time
IL_OspreyData$timestamp<-local2Solar(IL_OspreyData$TimeStamp, IL_OspreyData$location_long)
IL_OspreyData$timestamp<-as.POSIXct(IL_OspreyData$TimeStamp, format = "%m/%d/%Y %H:%M")
#Filter data further
IL_OspreyData <- IL_OspreyData %>%
  filter(Month %in% c(6:12))
quantile(IL_OspreyData$Course, probs = 0.9) #got 252
IL_OspreyData<-filter(IL_OspreyData, Speed <= 33.4 & Course < 252)
#Plot and look for visual outliers
IL_OspreyData_sf<-st_as_sf(x=IL_OspreyData, coords = c("location_long", "location_lat"), crs = wgs84)
IL_OspreyData_sf<-st_transform(IL_OspreyData_sf, azim_orign)
ggplot() +
  geom_sf(data = world_sf, fill = "white") +
  geom_sf(data = IL_OspreyData_sf, size = 1, aes(color = ID)) +
  coord_sf(datum = st_crs(azim_orign), xlim = c(-3000000, 1000000), ylim = c(3000000, -2000000)) +
  theme(legend.position = "bottom", legend.text = element_text(size = 7)) +
  labs(color = "ID") #there is one erroneous point that must be removed at a high lat/long

#remove obvious visual outlier. The position of this outlier within the data table may change. To find it open the table view and filter to show only 69D. Then sort the latitude column by the largest value. 
IL_OspreyData<-IL_OspreyData[-4208, ]
IL_OspreyData_sf<-IL_OspreyData_sf[-4208, ]
ggplot() +
  geom_sf(data = world_sf, fill = "white") +
  geom_sf(data = IL_OspreyData_sf, size = 1, aes(color = ID)) +
  coord_sf(datum = st_crs(azim_orign), xlim = c(-3000000, 1000000), ylim = c(3000000, -2000000)) +
  theme(legend.position = "bottom", legend.text = element_text(size = 7)) +
  labs(color = "ID") 
```

#### Combine Dataframes Into A Aingle Dataset And Create Migration Events IDs
```{r}
#keep only columns needed for analysis: time stamp, individual ID, year, month, lat, lon, columns for each data set. Make sure the names of each column are the same between all data sets. Use the str() function to look at column names
IL_OspreyData<-IL_OspreyData[ ,c("ID", "Month", "Year", "timestamp", "location_lat", "location_long")] 
#rename to match movebank data
names(IL_OspreyData) [names(IL_OspreyData) == "Year"] <- "year"
names(IL_OspreyData) [names(IL_OspreyData) == "Month"] <- "month"
names(IL_OspreyData) [names(IL_OspreyData) == "ID"] <- "trackId"

Bierregaard_Osprey_Data2<-Bierregaard_Osprey_Data[ ,c("location_lat", "location_long", "timestamp", "trackId", "month", "year")]

Jensen_Osprey_Data2<-Jensen_Osprey_Data[ ,c("location_lat", "location_long", "timestamp", "trackId", "month", "year")]

#combine data sets
Osprey_Data<-rbind(Bierregaard_Osprey_Data2, Jensen_Osprey_Data2, IL_OspreyData)

# assign each migration event a unique ID that is the name of the name of the [animal Name]_[Migration Year]
Osprey_Data$migrationEvent<-paste(Osprey_Data$trackId, Osprey_Data$year)
length(unique(Osprey_Data$migrationEvent))

#Remove migration events where migration did not occur (or are from 2007)
remove<-c("Pearl 2013", "Rafael 2009","Luke 2007", "Aster 2017", "Chester 2014", "Felix 2007", "Claws 2007", "Claws 2008", "Mackenzie 2013", "Trepassey 2016", "Tucker 2011", "Jocelyn 2016", "Rammie 2013", "Snowy 2012", "Thatcher 2011", "Penelope 2009", "Buck 2010", "Belle 2011", "Flow 2015", "Art 2013", "Artoo 2014", "Crabby 2015", "Borealis 2018", "Daphne 2017", "Weber 2014", "Nick 2017", "Ron 2015", "Woody 2015", "X586.Brighton_Barb 2015", "X934.Kensington.Pebbles 2017", "X423.Humphries_DTE_Ozzie 2015", "X139005.Monroe.Julie 2017")

# factor levels will be set to 0 in the data frame, but won't actually be removed. I have to change to character data format to remove rows.
Osprey_Data$migrationEvent<-as.character(Osprey_Data$migrationEvent)
Osprey_Data<-Osprey_Data[!(Osprey_Data$migrationEvent %in% remove), ]
length(unique(Osprey_Data$migrationEvent))
```

#### Filter Start And End Dates

Project the data and calculate NSD
```{r}
#Project data using custom azimuthal equidistant projection
Osprey_Data_sf<-st_as_sf(x=Osprey_Data, coords = c("location_long", "location_lat"), crs = wgs84)
Osprey_Data_sf<-st_transform(Osprey_Data_sf, azim_orign)

#Extract false easting and northing from our projected data which are in meters. 
False<-st_coordinates(Osprey_Data_sf) 
False<-as.data.frame(False)
Osprey_Data$X<-False$X/1000 #/1000 to equal kilometers. 
Osprey_Data$Y<-False$Y/1000
#Make a spatial data frame using the package 'sp'. 'adehabitatLR' requires an sp object rather than an sf object.  
Osprey_trajr<-Osprey_Data
coordinates(Osprey_trajr) <- c("X", "Y")
proj4string(Osprey_trajr) <- azim_orign

#Create an ltraj object using the 'adehabitat' package. ltraj objects automatically calculate NSD.
Osprey_trajr <- as.ltraj(coordinates(Osprey_trajr),
                          date=Osprey_trajr$timestamp,
                          id=Osprey_trajr$migrationEvent, typeII=TRUE)
```

Run the dispersal model and compared start/end dates with final recorded date
```{r}
#Run the migrateR dispersal model
Osprey_nsd<-mvmtClass(Osprey_trajr)
Osprey_dispersal<-mvmt2dt(Osprey_nsd, mod = "disperser") #Note that some individuals did not fit dispersal patterns
length(Osprey_dispersal) #Some trajectories did not fit the dispersal model. These birds did not get far enough into migration for start and end dates to be determined. These 6 individuals will need to be removed: "Caleb 2013", "Layla 2017", "Rock 2017", "X131008.Lux.Arbor.In.Flight 2017", "X401.Houghton_Harvest 2014", "X610.Monroe_DTE_Clawdia 2015"

#extract the start and end dates for each migration event and create a dataframe for each migration event with start and end date as columns.
Osprey_StartEnd<-rbindlist(Osprey_dispersal, idcol = "migrationEvent")
Start_End<-rep(c("Start", "End"), times = nrow(Osprey_StartEnd)/2) #divide by two since this will be the number of EACH character that will appear. 
Osprey_StartEnd$Start_End<-Start_End
Osprey_StartEnd<-reshape(Osprey_StartEnd, idvar = "migrationEvent", 
                         timevar = "Start_End", direction = "wide")

#Remove individuals that did not have model fits:
remove2<-c("Caleb 2013", "Layla 2017", "Rock 2017", "X131008.Lux.Arbor.In.Flight 2017", "X401.Houghton_Harvist 2014", "X610.Monroe_DTE_Clawdia 2015")
Osprey_Data<-Osprey_Data[!(Osprey_Data$migrationEvent %in% remove2), ] #left with 142 migration events.

# compare the end timestamp to the final timestamp of each migration event. If the end date = final timestamp then the migration was not completed. Mark this in the excel file. 
Osprey_Data_FinalTrackDay<-Osprey_Data %>%
  group_by(migrationEvent) %>%
  slice_max(timestamp)
Osprey_Data_FinalTrackDay$FinalDay<-Osprey_Data_FinalTrackDay$timestamp

Osprey_StartEnd<-merge(Osprey_StartEnd, Osprey_Data_FinalTrackDay, by = "migrationEvent")
Osprey_StartEnd<-Osprey_StartEnd[, c("migrationEvent", "date.Start", "date.End", "FinalDay", "month", "year", "location_lat", "location_long")]
```

Now that we have start and end date we can filter out non-migration points. It should be noted that migration is defined by this model. The model is fairly simple and cannot take into account the actual complexity of migration events. For example, 26R spent 1 month at a location outside of his natal home range before continuing on migration without going back to his home range. Leaving his natal home range could be considered the start of his migration, or the directional flight to his wintering grounds from this pre-migratory stopover could be considered the start of migration. The model provides the later interpretation. 

Filter GPS points by start/end dates and create 4 different datasets with different sampling frequencies. 
```{r}
#Keep only GPS fixes that fall within the start-end day range for each migration event. We could go to seconds/minutes ect., however we will use duration in days, so may as well keep all timestamps within the start/end day. 
events<-unique(Osprey_Data$migrationEvent)
df<-data.frame() #create an empty dataframe to store results

for (i in events) {
  y = filter(Osprey_Data, migrationEvent == i)
  x = filter(Osprey_StartEnd, migrationEvent == i)
  z = filter(y, timestamp <= as.Date(x$date.End) & timestamp >= as.Date(x$date.Start)) #We find the start/end day
  df = rbind(df, z) #binds all iteration outputs into a single dataframe
}

length(unique(df$migrationEvent)) #142 migration events and 115 individuals
```


```{r}
#Convert to a move object. To do this I had to use the moveVis package since the move function does not work well with actual dataframes. The move package has a number of very convinient functions that allow me to calculate a number of different movement characteristics and prep the data from analysis.
OSPR<-df2move(df,
        proj = wgs84, 
        x = "location_long", y = "location_lat", 
        time = "timestamp", track_id = "migrationEvent")

#regularize the time series through the move package. To do this we will have to apply the regularization to each move object within the movestack (annoying, I know). The interpolate time function fills in positional points along a standard time series. We will use 1-hour to do this. 
Stacked<-list() #empty list for output

for (i in 1:142) {
  x<-interpolateTime(OSPR[[i]], time=as.difftime(1, units="hours"), spaceMethod='rhumbline')
  print(plot(OSPR[[i]], col="red",pch=20, main="By time interval"))
  print(points(x))
  print(lines(OSPR[[i]], col="red"))
  print(legend("bottomleft", c("True locations", "Interpolated locations"), 
               col=c("red", "black"), pch=c(20,1))) #all points have a black outline which is annoying
  Stacked[[i]] = x
}

#convert output list of move objects into a move stack.
OSPR_1hour<-moveStack(Stacked) 
str(OSPR_1hour@trackId) #check that all migration events are accounted for
```

#### Thin the trajectory to different sampling frequencies
```{r}
#3-hour sampling frequency
Stacked_3hr<-list()
  
for (i in 1:142) {
  x<-thinTrackTime(OSPR_1hour[[i]], interval = as.difftime(3, units="hours"),
                          tolerance = as.difftime(15, units="mins"), criteria = "closest")
  split<-move::split(x)
  Stacked_3hr[[i]] = split$selected
}

OSPR_3hour<-moveStack(Stacked_3hr) 
str(OSPR_3hour@trackId)

#1-day sampling frequency
Stacked_1day<-list()
  
for (i in 1:142) {
  x<-thinTrackTime(OSPR_1hour[[i]], interval = as.difftime(1, units="days"),
                          tolerance = as.difftime(15, units="mins"), criteria = "closest")
  split<-move::split(x)
  Stacked_1day[[i]] = split$selected
}

OSPR_1day<-moveStack(Stacked_1day) 
str(OSPR_1day@trackId)

#3-day sampling frequency
Stacked_3day<-list()
  
for (i in 1:142) {
  x<-thinTrackTime(OSPR_1hour[[i]], interval = as.difftime(3, units="days"),
                          tolerance = as.difftime(15, units="mins"), criteria = "closest")
  split<-move::split(x)
  Stacked_3day[[i]] = split$selected
}

OSPR_3day<-moveStack(Stacked_3day) 
str(OSPR_3day@trackId)

#plot one of the migration events for each time period
plot(OSPR_1hour[[1]])
plot(OSPR_3hour[[1]])
plot(OSPR_1day[[1]])
plot(OSPR_3day[[1]])
```

The data set is now filtered and ready for analysis!

## Data Analysis
Data analysis starts with summarizing the data for Hierarchical Clustering Analysis (HCA). I will then move onto exploratory data analysis, looking at the distribution of the data to determine normalcy. This will inform the test I use later on. I can also explore clustering potential which will inform my decision to reduce dimensionality using a PCA before HCA.  I will repeat all steps for each of the different sampling frequencies. 

#### Summarizing data
First lets calculate primary characteristics with the move package and some secondary characteristics
```{r}
#Project data with custom azimuthal equidistant projection
OSPR_1hour_azim <- spTransform(OSPR_1hour, CRSobj=azim_orign)
OSPR_3hour_azim <- spTransform(OSPR_3hour, CRSobj=azim_orign)
OSPR_1day_azim <- spTransform(OSPR_1day, CRSobj=azim_orign)
OSPR_3day_azim <- spTransform(OSPR_3day, CRSobj=azim_orign)

#calculating distance between consecutive GPS points (meters)
OSPR_1hour_azim$distance<-unlist(lapply(distance(OSPR_1hour_azim), c, NA))
OSPR_3hour_azim$distance<-unlist(lapply(distance(OSPR_3hour_azim), c, NA))
OSPR_1day_azim$distance<-unlist(lapply(distance(OSPR_1day_azim), c, NA))
OSPR_3day_azim$distance<-unlist(lapply(distance(OSPR_3day_azim), c, NA))

#calculating speed/velocity between consecutive GPS points (m/s)
OSPR_1hour_azim$speed<-unlist(lapply(speed(OSPR_1hour_azim), c, NA))
OSPR_3hour_azim$speed<-unlist(lapply(speed(OSPR_3hour_azim), c, NA))
OSPR_1day_azim$speed<-unlist(lapply(speed(OSPR_1day_azim), c, NA))
OSPR_3day_azim$speed<-unlist(lapply(speed(OSPR_3day_azim), c, NA))

#Project data with custom conformal projection: the Mercator projection is a conformal cylindrical projection
mercator = CRS("+init=epsg:4326") #this projection covers the world so no need to custom define projection attributes
OSPR_1hour_mercator <- spTransform(OSPR_1hour, CRSobj=mercator)
OSPR_3hour_mercator <- spTransform(OSPR_3hour, CRSobj=mercator)
OSPR_1day_mercator <- spTransform(OSPR_1day, CRSobj=mercator)
OSPR_3day_mercator <- spTransform(OSPR_3day, CRSobj=mercator)

#calculating absolute angle
OSPR_1hour_mercator$abs_angle<-unlist(lapply(angle(OSPR_1hour_mercator), c, NA))
OSPR_3hour_mercator$abs_angle<-unlist(lapply(angle(OSPR_3hour_mercator), c, NA))
OSPR_1day_mercator$abs_angle<-unlist(lapply(angle(OSPR_1day_mercator), c, NA))
OSPR_3day_mercator$abs_angle<-unlist(lapply(angle(OSPR_3day_mercator), c, NA))

#calculate Turning Angle
OSPR_1hour_mercator$rel_angle<-unlist(lapply(turnAngleGc
                                             (OSPR_1hour_mercator), 
                                             function(x) c(NA, x, NA)))
OSPR_3hour_mercator$rel_angle<-unlist(lapply(turnAngleGc
                                             (OSPR_3hour_mercator), 
                                             function(x) c(NA, x, NA)))
OSPR_1day_mercator$rel_angle<-unlist(lapply(turnAngleGc
                                             (OSPR_1day_mercator), 
                                             function(x) c(NA, x, NA)))
OSPR_3day_mercator$rel_angle<-unlist(lapply(turnAngleGc
                                             (OSPR_3day_mercator), 
                                             function(x) c(NA, x, NA)))

#calculating time measurements
OSPR_1hour_azim$lag<-unlist(lapply(timeLag(OSPR_1hour_azim, 
                                           units = "hours"), c, NA))
OSPR_3hour_azim$lag<-unlist(lapply(timeLag(OSPR_3hour_azim, 
                                           units = "hours"), c, NA))
OSPR_1day_azim$lag<-unlist(lapply(timeLag(OSPR_1day_azim, 
                                           units = "hours"), c, NA))
OSPR_3day_azim$lag<-unlist(lapply(timeLag(OSPR_3day_azim, 
                                           units = "hours"), c, NA))
```

Next I will transform my move objects into data frames and extract the characteristics plus coordinate and migration Event information. I will append the angular info from the Mercator projected data frames to the azimuthal dataframes then calculate angular velocity. 
```{r}
#1-hour
OSPR_1hour_azim_DF<-as.data.frame(OSPR_1hour_azim)
OSPR_1hour_mercator_DF<-as.data.frame(OSPR_1hour_mercator)
OSPR_1hour_azim_DF$rel_angle<-OSPR_1hour_mercator_DF$rel_angle
OSPR_1hour_azim_DF$abs_angle<-OSPR_1hour_mercator_DF$abs_angle
OSPR_1hour_azim_DF$turn_speed<-
  OSPR_1hour_azim_DF$speed * cos(OSPR_1hour_azim_DF$rel_angle)

#3-hour
OSPR_3hour_azim_DF<-as.data.frame(OSPR_3hour_azim)
OSPR_3hour_mercator_DF<-as.data.frame(OSPR_3hour_mercator)
OSPR_3hour_azim_DF$rel_angle<-OSPR_3hour_mercator_DF$rel_angle
OSPR_3hour_azim_DF$abs_angle<-OSPR_3hour_mercator_DF$abs_angle
OSPR_3hour_azim_DF$turn_speed<-
  OSPR_3hour_azim_DF$speed * cos(OSPR_3hour_azim_DF$rel_angle)

#1-day
OSPR_1day_azim_DF<-as.data.frame(OSPR_1day_azim)
OSPR_1day_mercator_DF<-as.data.frame(OSPR_1day_mercator)
OSPR_1day_azim_DF$rel_angle<-OSPR_1day_mercator_DF$rel_angle
OSPR_1day_azim_DF$abs_angle<-OSPR_1day_mercator_DF$abs_angle
OSPR_1day_azim_DF$turn_speed<-
  OSPR_1day_azim_DF$speed * cos(OSPR_1day_azim_DF$rel_angle)

#3-day
OSPR_3day_azim_DF<-as.data.frame(OSPR_3day_azim)
OSPR_3day_mercator_DF<-as.data.frame(OSPR_3day_mercator)
OSPR_3day_azim_DF$rel_angle<-OSPR_3day_mercator_DF$rel_angle
OSPR_3day_azim_DF$abs_angle<-OSPR_3day_mercator_DF$abs_angle
OSPR_3day_azim_DF$turn_speed<-
  OSPR_3day_azim_DF$speed * cos(OSPR_3day_azim_DF$rel_angle)
```

Finally, I will transform my dataframe into an ltraj object with the azimuthal equidistant projection so I can calculate FPT and displacement.
```{r}

```

Lastly, I will summarize my data so that there is one observation of each variable per individual. 
```{r}

```


#### Exploratory data analysis

First I will look at the distribution of data for each sampling frequency
```{r}
#normality of data

#clustering potential of data

#PCA???

#Extracting principle components

```

#### Hiearchical Clustering Analysis
```{r}
# 1-hour
```

```{r}
# 3-hour
```

```{r}
# 1-Day
```

```{r}
# 3-Day
```

#### Optimal clusters
```{r}
# 1-hour
```

```{r}
# 3-hour
```

```{r}
# 1-Day
```

```{r}
# 3-Day
```

#### PERMANOVA
```{r}
# 1-hour
```

```{r}
# 3-hour
```

```{r}
# 1-Day
```

```{r}
# 3-Day
```








```{r}
#check to see if the NSD dispersal model works
Bierregaard_Osprey_Data_Artoo<-filter(Bierregaard_Osprey_Data_sf, trackId == "Artoo")
Bierregaard_Osprey_Data_Artoo$month<-as.factor(Bierregaard_Osprey_Data_Artoo$month)
ggplot() +
    geom_sf(data = world_sf, fill = "white") +
    geom_sf(data = Bierregaard_Osprey_Data_Artoo, size = 1, aes(color = month)) +
    coord_sf(datum = st_crs(azim_orign), xlim = c(-2000000, 8000000), ylim = c(3000000, -3000000)) +
    theme(legend.position = "bottom", legend.text = element_text(size = 7)) +
    labs(color = "month") +
    facet_wrap(~year)#looks good!
```


## References

* Help using the move package to download data from Movebank: <https://cran.r-project.org/web/packages/move/vignettes/browseMovebank.html> 
* Navigating and using data from the move package: <https://cran.r-project.org/web/packages/move/vignettes/move.html>
* Download a CSV file from github: <https://lokraj.me/post/download-github-data/>
* Github and RStudio interface: <https://rfortherestofus.com/2021/02/how-to-use-git-github-with-r/>
* Finding mode in R: <https://www.statology.org/mode-in-r/>
* migrateR Vignette: <https://github.com/dbspitz/migrateR/blob/master/migrateR%20vignette.pdf>
* Projections in R: <https://www.nceas.ucsb.edu/sites/default/files/2020-04/OverviewCoordinateReferenceSystems.pdf>